{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkobyQHnHmfr"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1736530922442,
     "user": {
      "displayName": "Daniel Berenson",
      "userId": "01641759045451756668"
     },
     "user_tz": 300
    },
    "id": "aEvYxZF4wEa5"
   },
   "outputs": [],
   "source": [
    "# prompt: Import libraries and open CSV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import modules.testing as testing\n",
    "import modules.data_cleaning_utils as dcu\n",
    "from importlib import reload\n",
    "\n",
    "# my_computer_fpath = \"C:\\\\Users\\\\dfber\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\"\n",
    "my_computer_fpath = \"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from October Merlin pull\n",
    "file_path = my_computer_fpath + \"e26f9ccc-68a4-42b4-9d0d-508a83026a1c.csv\"\n",
    "delivery_datetime_is_incorrect = True\n",
    "procedure_datetime_is_incorrect = True\n",
    "procedure_starttime_is_incorrect = True\n",
    "\n",
    "# # Data from January Merin pull\n",
    "# file_path = my_computer_fpath + \"befa2320-c0e0-476c-b66c-7fd2fb90179e.csv\"\n",
    "# delivery_datetime_is_incorrect = False\n",
    "# procedure_datetime_is_incorrect = True\n",
    "# procedure_starttime_is_incorrect = True\n",
    "\n",
    "merlin_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dcu)\n",
    "df = merlin_df.copy()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delivery_site_2188'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLFyVswb3COk"
   },
   "source": [
    "# Initial Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anes_procedure_cols: list = [\n",
    "    'anes_procedure_type_2253', \n",
    "    'anes_procedure_start_dts_2254', \n",
    "    'anes_procedure_anesthesiologist_2255', \n",
    "    'anes_procedure_resident_2256', \n",
    "    'anes_procedure_pt_position_2257', \n",
    "    'anes_procedure_approach_2258', \n",
    "    'anes_procedure_location_2259', \n",
    "    'anes_procedure_note_id_2260', \n",
    "    'anes_procedure_dos_dts_2261', \n",
    "    'anes_procedure_dpe_2262', \n",
    "    'anes_procedure_epidural_needle_2263', \n",
    "    'anes_procedure_epidural_needle_gauge_2264', \n",
    "    'anes_procedure_lor_depth_2265', \n",
    "    'anes_procedure_catheter_depth_2266', \n",
    "    'anes_procedure_spinal_needle_type_2267', \n",
    "    'anes_procedure_spinal_needle_gauge_2268', \n",
    "    'anes_procedure_spinal_needle_length_2269', \n",
    "    'anes_procedure_paresthesias_2270', \n",
    "    'anes_procedure_note_text_2271',\n",
    "    'anes_procedure_encounter_id_2273'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode |-separated notes\n",
    "df = dcu.explode_separated_procedure_notes(df, anes_procedure_cols=anes_procedure_cols, delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of procedures by type\n",
    "# Note that other procedure types, including Blood Patch but also A-lines,\n",
    "# nerve blocks, and POCUS orders, are currently parsed by Merlin to NaN\n",
    "df['anes_procedure_type_2253'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in RAW info\n",
    "# This is needed at the moment to get the NotePurposeDSC (to help eliminate near-duplicate notes)\n",
    "# and also to RegEx the Number of Attempts\n",
    "\n",
    "raw_info_fpath = my_computer_fpath + \"Full Identified raw anesthesia_procedure_notes.csv\"\n",
    "raw_df = pd.read_csv(raw_info_fpath)\n",
    "df = dcu.add_raw_info(df, raw_info_fpath, processed_note_id_col = 'anes_procedure_note_id_2260', raw_info_cols = ['NotePurposeDSC','NoteTXT'])\n",
    "df = dcu.regex_note_text(df, desired_col = 'number_of_neuraxial_attempts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ISNBfVtIQUV"
   },
   "source": [
    "## Handle datetime issues\n",
    "\n",
    "Bug: Merlin is bringing anes_procedure_dos_dts_2261 as Eastern times when in fact they are UTC. I resolve this by editing the raw strings before conversion to datetime objects.\n",
    "\n",
    "Bug: The same WAS true for delivery_time before it was corrected in Merlin in January. I resolve this by editing the raw strings before conversion to datetime objects.\n",
    "\n",
    "Bug: Because delivery_date is stored separately from delivery_time, if the UTC change causes the time to go to the next day, this is NOT reflected in the updated delivery_date. This was also fixed in Merlin in January.\n",
    "\n",
    "Bug: Merlin ignores AM/PM in anes_procedure_start_dts_2254 and assumes all entries are AM. I resolve this (for now) by ignoring these written start times and just using dos_dts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validated times:\n",
    "https://partnershealthcare-my.sharepoint.com/:x:/r/personal/dberenson_bwh_harvard_edu/_layouts/15/Doc.aspx?sourcedoc=%7BD674A3E1-815E-46B8-9AA4-16558C09411A%7D&file=Manually%20Verified%20Catheters.xlsx&action=default&mobileredirect=true&wdOrigin=OUTLOOK-METAOS.FILEBROWSER.FILES-FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['anes_procedure_note_id_2260'] == '2981389717',['delivery_date','delivery_time','anes_procedure_start_dts_2254','anes_procedure_dos_dts_2261']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delivery_datetime_is_incorrect:\n",
    "    df = dcu.fix_delivery_datetime(df)\n",
    "else:\n",
    "    df = dcu.add_delivery_datetime(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['maternal_dob'] = pd.to_datetime(df['maternal_dob_2043'],utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if procedure_datetime_is_incorrect:\n",
    "    df = dcu.fix_procedure_dos_datetime(df)\n",
    "else:\n",
    "    df['dos_dts'] = pd.to_datetime(df['anes_procedure_dos_dts_2261'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1736530972209,
     "user": {
      "displayName": "Daniel Berenson",
      "userId": "01641759045451756668"
     },
     "user_tz": 300
    },
    "id": "59dC3WB-ZH8-"
   },
   "outputs": [],
   "source": [
    "df['start_dts'] = pd.to_datetime(df['anes_procedure_start_dts_2254'],format='mixed',utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1736530972393,
     "user": {
      "displayName": "Daniel Berenson",
      "userId": "01641759045451756668"
     },
     "user_tz": 300
    },
    "id": "giGm_VZXdG5i",
    "outputId": "eec08e3c-c2d3-494f-ccb7-301cd698e376"
   },
   "outputs": [],
   "source": [
    "# Extract the time part of the 'start_dts' column to check whether it covers all 24 h or only 12 h due to AM/PM bug\n",
    "df[df['start_dts'].notna()]['start_dts'].dt.time.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1736530972394,
     "user": {
      "displayName": "Daniel Berenson",
      "userId": "01641759045451756668"
     },
     "user_tz": 300
    },
    "id": "ohVRiD_RRohx"
   },
   "outputs": [],
   "source": [
    "# This code has been changed to avoid the AM/PM bug\n",
    "if procedure_starttime_is_incorrect:\n",
    "    df['best_timestamp'] = df['dos_dts']\n",
    "else:\n",
    "    df['best_timestamp'] = df['start_dts'].fillna(df['dos_dts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "anes_procedure_cols.extend(['best_timestamp', 'dos_dts', 'start_dts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JPfIBsGQjpd"
   },
   "source": [
    "## Handle near-duplicate notes\n",
    "\n",
    "There is also a column \"NotePurposeDSC\" in the raw EDW data that can be \"ADDENDUM\" or \"NORMAL\" or blank. When there are duplicate notes, the first one will be blank and subsequent ones will be ADDENDUM. I use this fact upstream and delete all the ones that are blank.\n",
    "\n",
    "Then, I go through and delete other notes that appear to be duplicates. The majority of these are apparently due to TWINS, where a single NoteID appears twice in the dataset due to how Merlin generates it birthwise rather than momwise. However, it CANNOT be done by just eliminating non-unique NoteIDs, as \"double-notes\" (which appear in Epic as one note that has two procedure descriptions concatenated together) also have the same NoteID. Instead, I drop rows where both the NoteID and the ProcedureType match.\n",
    "\n",
    " Instead, I look within each encounter and check if there are two notes that are the same procedure type and within a short minute_offset of each other. If so, I delete the less-complete note.\n",
    "\n",
    "IMPORTANT: It turns out to be the case that there are sometimes, genuinely in Epic, two procedures done within only a few mins of each other. I AM CURRENTLY WORKING ON FIGURING OUT WHAT IS GOING ON WITH THESE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop near-duplicate notes with blank NotePurposeDSC\n",
    "df = df.dropna(subset=['NotePurposeDSC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcu.print_encounter(df,'3128029077') # double note\n",
    "dcu.print_encounter(df,'3451276171') # known near-duplicate note (that is genuinely duplicated (actually, triplicated) in Epic)\n",
    "dcu.print_encounter(df,'3188356337') # known near-duplicate note (that is duplicated due to twins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop near-duplicate notes with identical Procedure Type and NoteID (i.e., duplicated twins)\n",
    "df = df.drop_duplicates(subset=['anes_procedure_type_2253','anes_procedure_note_id_2260'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcu.print_encounter(df,'3128029077') # double note\n",
    "dcu.print_encounter(df,'3451276171') # known near-duplicate note (that is genuinely duplicated (actually, triplicated) in Epic)\n",
    "dcu.print_encounter(df,'3188356337') # known near-duplicate note (that is duplicated due to twins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dcu)\n",
    "df = saved_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.label_and_drop_worse_versions_of_duplicates(df, anes_procedure_cols, minute_offset=10, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When minute_offset = 60, there are 564 near-duplicates identified.\\\n",
    "When minute_offset = 30, there are 310 near-duplicates identified.\\\n",
    "When minute_offset = 10, there are 175 near-duplicates identified.\\\n",
    "When minute_offset = 2, there are 97 near-duplicates identified.\\\n",
    "When minute_offset = 1, there are 63 near-duplicates identified.\\\n",
    "When minute_offset = 0, there are 12 near-duplicates identified.\\\n",
    "\n",
    "I manually evaluated about twenty. If the minute_offset is 0-10, there are a mix of duplicate notes vs replacements/multiple attempts. If the minute_offset > 10, I found only true replacements (commonly due to positive test dose). Therefore I will use minute_offset = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcu.print_encounter(df,'3128029077') # double note\n",
    "dcu.print_encounter(df,'3451276171') # known near-duplicate note (that is genuinely duplicated (actually, triplicated) in Epic)\n",
    "dcu.print_encounter(df,'3188356337') # known near-duplicate note (that is duplicated due to twins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtFesSAK1wsT"
   },
   "source": [
    "## Address cases where an epidural note followed by a spinal note is actually a planned CSE, not a failed catheter. Also address what 'epidural/intrathecal' really means.\n",
    "\n",
    "Secret CSEs are spinal and epidural within 5 mins\n",
    "\n",
    "Epidural/intrathecal notes are declared epidural unless ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dcu)\n",
    "df = saved_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.process_secret_CSEs(df, minute_offset=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dcu)\n",
    "df = saved_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.classify_true_procedure_type(df, intelligent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlc5uOLquoP_"
   },
   "source": [
    "# Classify failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dcu)\n",
    "df = saved_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.label_failed_catheters(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPBvuAV6NRli"
   },
   "source": [
    "# Additional Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count prior failed neuraxials in this encounter and failed and total across all encounters\n",
    "\n",
    "Takes ~8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.count_prior_catheters(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "757vQkZucKbN"
   },
   "source": [
    "## Handle timeseries data (e.g., pain scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.handle_pain_scores(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3n9mBjG2mKn"
   },
   "source": [
    "## Clean DPE and LOR_Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.handle_dpe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EALlbR6-OUaQ"
   },
   "outputs": [],
   "source": [
    "df = dcu.handle_lor_depth(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjq02Q_U2ucI"
   },
   "source": [
    "## Make numerical columns numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.numerify_columns(df, columns_to_convert = ['gestational_age_2052','bmi_end_pregnancy_2044', 'maternal_weight_end_pregnancy_2045', 'maternal_height_2046', 'gravidity_2047', 'parity_2048','baby_weight_2196','bmi_before_pregnancy_2161','secs_rom_thru_delivery_2197'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and plausibilify elapsed times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.handle_elapsed_times(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include other limits on plausible data for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rdMZOcI2xZH"
   },
   "source": [
    "## Handle proceduralist names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.handle_anesthesiologists(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqbNsz2U26MN"
   },
   "source": [
    "## Feature engineering on categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.engineer_categorical_variables(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new unique identifier based on epic_pmrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_unique_id(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5mjv9mmfl2r"
   },
   "source": [
    "# Save processed data prior to analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "LAF1Cqkc04aE"
   },
   "outputs": [],
   "source": [
    "complete_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "SWdKTtPTflV9"
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a pickle file\n",
    "complete_data.to_pickle(my_computer_fpath + \"processed_merlin_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "3AIT25V7f_vj"
   },
   "outputs": [],
   "source": [
    "# prompt: Import libraries and open CSV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pxsgBzbfzOz"
   },
   "outputs": [],
   "source": [
    "# Load the pickled DataFrame\n",
    "complete_data = pd.read_pickle(my_computer_fpath + \"processed_merlin_data.pkl\")\n",
    "\n",
    "# Now you can work with the DataFrame\n",
    "complete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "zwIzc7441Cot"
   },
   "outputs": [],
   "source": [
    "df = complete_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KddNYcoUiEWD"
   },
   "source": [
    "# Reduce Table to Chosen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wouftr6X4Z57"
   },
   "outputs": [],
   "source": [
    "# prompt: print all columns as a list and make it easy to read over multiple lines\n",
    "\n",
    "# Assuming 'df' is your DataFrame (as defined in the provided code)\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Print the list of columns, formatted for readability\n",
    "print(\"Columns of the DataFrame:\")\n",
    "for i, col in enumerate(all_columns):\n",
    "    print(f\"{i+1}. {col} ||| {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "EFrTx2-X-Jyx"
   },
   "outputs": [],
   "source": [
    "chosen_features = [\n",
    "#    \"id\",\n",
    "    \"unique_pt_id\",\n",
    "    \"anes_procedure_encounter_id_2273\",\n",
    "    \"gestational_age_weeks\",\n",
    "    \"delivery_site\",\n",
    "    \"delivery_site_bwh\",\n",
    "    \"baby_weight_2196\",\n",
    "    \"rom_thru_delivery_hours\",\n",
    "    \"fetal_presentation_category_2243\",\n",
    "    \"fetal_presentation_position_2247\",\n",
    "    \"bmi_end_pregnancy_2044\",\n",
    "    \"maternal_weight_end_pregnancy_2045\",\n",
    "    \"bmi_before_pregnancy_2161\",\n",
    "#    \"zipcode_2185\",\n",
    "    \"gravidity_2047\",\n",
    "    \"parity_2048\",\n",
    "#    \"anes_procedure_note_text_2271\",\n",
    "#    \"best_timestamp\",\n",
    "#    \"true_procedure_type\",\n",
    "    \"is_neuraxial_catheter\",\n",
    "    \"failed_catheter\",\n",
    "    \"has_subsequent_neuraxial_catheter\",\n",
    "    \"has_subsequent_spinal\",\n",
    "    \"has_subsequent_airway\",\n",
    "#    \"dpe\",\n",
    "    \"lor_depth\",\n",
    "    \"current_resident_catheter_count\",\n",
    "    \"highly_experienced_anesthesiologist\",\n",
    "    \"highly_experienced_resident\",\n",
    "    \"current_anesthesiologist_catheter_count\",\n",
    "    \"moderately_experienced_anesthesiologist\",\n",
    "    \"has_scoliosis\",\n",
    "    \"has_dorsalgia\",\n",
    "    \"has_back_problems\",\n",
    "    \"maternal_race\",\n",
    "    \"maternal_ethnicity\",\n",
    " #   \"prior_pain_scores\",\n",
    "    \"prior_pain_scores_max\",\n",
    "    \"composite_psychosocial_problems\",\n",
    "    \"only_private_insurance\",\n",
    "    \"maternal_language_english\",\n",
    "    \"marital_status_married_or_partner\",\n",
    "    \"country_of_origin_USA\",\n",
    "    \"employment_status_fulltime\",\n",
    "    \"composite_SES_advantage\",\n",
    "    \"epidural_needle_type\",\n",
    "    \"paresthesias_present\",\n",
    "    \"number_of_neuraxial_attempts\",\n",
    "    \"prior_failed_catheters_this_enc\",\n",
    "    \"prior_failed_catheters_prev_enc\",\n",
    "    \"prior_all_catheters_all_enc\",\n",
    "    \"true_procedure_type_incl_dpe\",\n",
    "    \"maternal_age_years\",\n",
    "    \"placement_to_delivery_hours\",\n",
    "    \"labor_induction\",\n",
    "    \"position_posterior_or_transverse\",\n",
    "    \"presentation_cephalic\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Nr0wa_Zq-RfA"
   },
   "outputs": [],
   "source": [
    "df = df[chosen_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIr75B_tGNEV"
   },
   "outputs": [],
   "source": [
    "# prompt: print all columns as a list and make it easy to read over multiple lines\n",
    "\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Print the list of columns, formatted for readability\n",
    "print(\"Columns of the DataFrame:\")\n",
    "for i, col in enumerate(all_columns):\n",
    "    print(f\"{i+1}. {col} ||| {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(my_computer_fpath + 'processed_merlin_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNKPV6NaW7+ZRxbDtLw/frD",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
