{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkobyQHnHmfr"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1736530922442,
     "user": {
      "displayName": "Daniel Berenson",
      "userId": "01641759045451756668"
     },
     "user_tz": 300
    },
    "id": "aEvYxZF4wEa5"
   },
   "outputs": [],
   "source": [
    "# prompt: Import libraries and open CSV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import modules.testing as testing\n",
    "import modules.data_cleaning_utils as dcu\n",
    "from importlib import reload\n",
    "\n",
    "# my_computer_fpath = \"C:\\\\Users\\\\dfber\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\"\n",
    "my_computer_fpath = \"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data from October Merlin pull\n",
    "# file_path = my_computer_fpath + \"e26f9ccc-68a4-42b4-9d0d-508a83026a1c.csv\"\n",
    "# delivery_datetime_is_incorrect = True\n",
    "# procedure_datetime_is_incorrect = True\n",
    "# procedure_starttime_is_incorrect = True\n",
    "\n",
    "# # Data from January Merin pull\n",
    "# file_path = my_computer_fpath + \"ccfaad4e-0523-4fe8-bc87-150370deef90.csv\"\n",
    "# delivery_datetime_is_incorrect = False\n",
    "# procedure_datetime_is_incorrect = True\n",
    "# procedure_starttime_is_incorrect = True\n",
    "\n",
    "# Data from March Merlin pull\n",
    "file_path = my_computer_fpath + \"55670a61-a439-48a2-9f1e-acf1e4156730.csv\"\n",
    "delivery_datetime_is_incorrect = False\n",
    "procedure_datetime_is_incorrect = True\n",
    "procedure_starttime_is_incorrect = True\n",
    "\n",
    "\n",
    "merlin_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dcu)\n",
    "df = merlin_df.copy()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delivery_site_2188'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLFyVswb3COk"
   },
   "source": [
    "# Initial Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anes_procedure_cols: list = [\n",
    "    'anes_procedure_type_2253', \n",
    "    'anes_procedure_start_dts_2254', \n",
    "    'anes_procedure_anesthesiologist_2255', \n",
    "    'anes_procedure_resident_2256', \n",
    "    'anes_procedure_pt_position_2257', \n",
    "    'anes_procedure_approach_2258', \n",
    "    'anes_procedure_location_2259', \n",
    "    'anes_procedure_note_id_2260', \n",
    "    'anes_procedure_dos_dts_2261', \n",
    "    'anes_procedure_dpe_2262', \n",
    "    'anes_procedure_epidural_needle_2263', \n",
    "    'anes_procedure_epidural_needle_gauge_2264', \n",
    "    'anes_procedure_lor_depth_2265', \n",
    "    'anes_procedure_catheter_depth_2266', \n",
    "    'anes_procedure_spinal_needle_type_2267', \n",
    "    'anes_procedure_spinal_needle_gauge_2268', \n",
    "    'anes_procedure_spinal_needle_length_2269', \n",
    "    'anes_procedure_paresthesias_2270', \n",
    "    'anes_procedure_note_text_2271',\n",
    "    'anes_procedure_encounter_id_2273'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode |-separated notes\n",
    "df = dcu.explode_separated_procedure_notes(df, anes_procedure_cols=anes_procedure_cols, delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of procedures by type\n",
    "# Note that other procedure types, including Blood Patch but also A-lines,\n",
    "# nerve blocks, and POCUS orders, are currently parsed by Merlin to NaN\n",
    "df['anes_procedure_type_2253'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in RAW info\n",
    "# This is needed at the moment to get the NotePurposeDSC (to help eliminate near-duplicate notes)\n",
    "# and also to RegEx the Number of Attempts\n",
    "\n",
    "raw_info_fpath = my_computer_fpath + \"Full Identified raw anesthesia_procedure_notes.csv\"\n",
    "raw_df = pd.read_csv(raw_info_fpath)\n",
    "df = dcu.add_raw_info(df, raw_info_fpath, processed_note_id_col = 'anes_procedure_note_id_2260', raw_info_cols = ['NotePurposeDSC','NoteTXT'])\n",
    "df = dcu.regex_note_text(df, desired_col = 'number_of_neuraxial_attempts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ISNBfVtIQUV"
   },
   "source": [
    "## Handle datetime issues\n",
    "\n",
    "Bug: Merlin is bringing anes_procedure_dos_dts_2261 as Eastern times when in fact they are UTC. I resolve this by editing the raw strings before conversion to datetime objects.\n",
    "\n",
    "Bug: The same WAS true for delivery_time before it was corrected in Merlin in January. I resolve this by editing the raw strings before conversion to datetime objects.\n",
    "\n",
    "Bug: Because delivery_date is stored separately from delivery_time, if the UTC change causes the time to go to the next day, this is NOT reflected in the updated delivery_date. This was also fixed in Merlin in January.\n",
    "\n",
    "Bug: Merlin ignores AM/PM in anes_procedure_start_dts_2254 and assumes all entries are AM. I resolve this (for now) by ignoring these written start times and just using dos_dts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validated times:\n",
    "https://partnershealthcare-my.sharepoint.com/:x:/r/personal/dberenson_bwh_harvard_edu/_layouts/15/Doc.aspx?sourcedoc=%7BD674A3E1-815E-46B8-9AA4-16558C09411A%7D&file=Manually%20Verified%20Catheters.xlsx&action=default&mobileredirect=true&wdOrigin=OUTLOOK-METAOS.FILEBROWSER.FILES-FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['anes_procedure_note_id_2260'] == '2981389717',['delivery_date','delivery_time','anes_procedure_start_dts_2254','anes_procedure_dos_dts_2261']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delivery_datetime_is_incorrect:\n",
    "    df = dcu.fix_delivery_datetime(df)\n",
    "else:\n",
    "    df = dcu.add_delivery_datetime(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['maternal_dob'] = pd.to_datetime(df['maternal_dob_2043'],utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if procedure_datetime_is_incorrect:\n",
    "    df = dcu.fix_procedure_dos_datetime(df)\n",
    "else:\n",
    "    df['dos_dts'] = pd.to_datetime(df['anes_procedure_dos_dts_2261'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1736530972209,
     "user": {
      "displayName": "Daniel Berenson",
      "userId": "01641759045451756668"
     },
     "user_tz": 300
    },
    "id": "59dC3WB-ZH8-"
   },
   "outputs": [],
   "source": [
    "df['start_dts'] = pd.to_datetime(df['anes_procedure_start_dts_2254'],format='mixed',utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1736530972393,
     "user": {
      "displayName": "Daniel Berenson",
      "userId": "01641759045451756668"
     },
     "user_tz": 300
    },
    "id": "giGm_VZXdG5i",
    "outputId": "eec08e3c-c2d3-494f-ccb7-301cd698e376"
   },
   "outputs": [],
   "source": [
    "# Extract the time part of the 'start_dts' column to check whether it covers all 24 h or only 12 h due to AM/PM bug\n",
    "df[df['start_dts'].notna()]['start_dts'].dt.time.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1736530972394,
     "user": {
      "displayName": "Daniel Berenson",
      "userId": "01641759045451756668"
     },
     "user_tz": 300
    },
    "id": "ohVRiD_RRohx"
   },
   "outputs": [],
   "source": [
    "# This code has been changed to avoid the AM/PM bug\n",
    "if procedure_starttime_is_incorrect:\n",
    "    df['best_timestamp'] = df['dos_dts']\n",
    "else:\n",
    "    df['best_timestamp'] = df['start_dts'].fillna(df['dos_dts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anes_procedure_cols.extend(['best_timestamp', 'dos_dts', 'start_dts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be great to look at the title of the anesthesia encounter and eliminate ones other than Labor Epidural or CS, rather than relying on the time narrowing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.calculate_and_narrow_time_from_placement_to_delivery(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JPfIBsGQjpd"
   },
   "source": [
    "## Handle near-duplicate notes\n",
    "\n",
    "There is also a column \"NotePurposeDSC\" in the raw EDW data that can be \"ADDENDUM\" or \"NORMAL\" or blank. When there are duplicate notes, the first one will be blank and subsequent ones will be ADDENDUM. I use this fact upstream and delete all the ones that are blank.\n",
    "\n",
    "Then, I go through and delete other notes that appear to be duplicates. The majority of these are apparently due to TWINS, where a single NoteID appears twice in the dataset due to how Merlin generates it birthwise rather than momwise. However, it CANNOT be done by just eliminating non-unique NoteIDs, as \"double-notes\" (which appear in Epic as one note that has two procedure descriptions concatenated together) also have the same NoteID. Instead, I drop rows where both the NoteID and the ProcedureType match.\n",
    "\n",
    " Instead, I look within each encounter and check if there are two notes that are the same procedure type and within a short minute_offset of each other. If so, I delete the less-complete note.\n",
    "\n",
    "IMPORTANT: It turns out to be the case that there are sometimes, genuinely in Epic, two procedures done within only a few mins of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop near-duplicate notes with blank NotePurposeDSC\n",
    "df = df.dropna(subset=['NotePurposeDSC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcu.print_encounter(df,'3128029077') # double note\n",
    "dcu.print_encounter(df,'3451276171') # known near-duplicate note (that is genuinely duplicated (actually, triplicated) in Epic)\n",
    "dcu.print_encounter(df,'3188356337') # known near-duplicate note (that is duplicated due to twins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop near-duplicate notes with identical Procedure Type and NoteID (i.e., duplicated twins)\n",
    "df = df.drop_duplicates(subset=['anes_procedure_type_2253','anes_procedure_note_id_2260'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcu.print_encounter(df,'3128029077') # double note\n",
    "dcu.print_encounter(df,'3451276171') # known near-duplicate note (that is genuinely duplicated (actually, triplicated) in Epic)\n",
    "dcu.print_encounter(df,'3188356337') # known near-duplicate note (that is duplicated due to twins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dcu)\n",
    "df = saved_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.label_and_drop_worse_versions_of_duplicates(df, anes_procedure_cols, minute_offset=10, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When minute_offset = 60, there are 564 near-duplicates identified.\\\n",
    "When minute_offset = 30, there are 310 near-duplicates identified.\\\n",
    "When minute_offset = 10, there are 175 near-duplicates identified.\\\n",
    "When minute_offset = 2, there are 97 near-duplicates identified.\\\n",
    "When minute_offset = 1, there are 63 near-duplicates identified.\\\n",
    "When minute_offset = 0, there are 12 near-duplicates identified.\\\n",
    "\n",
    "I manually evaluated about twenty. If the minute_offset is 0-10, there are a mix of duplicate notes vs replacements/multiple attempts. If the minute_offset > 10, I found only true replacements (commonly due to positive test dose). Therefore I will use minute_offset = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcu.print_encounter(df,'3128029077') # double note\n",
    "dcu.print_encounter(df,'3451276171') # known near-duplicate note (that is genuinely duplicated (actually, triplicated) in Epic)\n",
    "dcu.print_encounter(df,'3188356337') # known near-duplicate note (that is duplicated due to twins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtFesSAK1wsT"
   },
   "source": [
    "## Address cases where an epidural note followed by a spinal note is actually a planned CSE, not a failed catheter. Also address what 'epidural/intrathecal' really means.\n",
    "\n",
    "Secret CSEs are spinal and epidural within 5 mins\n",
    "\n",
    "Epidural/intrathecal notes are declared epidural unless ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.process_secret_CSEs(df, minute_offset=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.classify_true_procedure_type(df, intelligent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlc5uOLquoP_"
   },
   "source": [
    "# Classify failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.label_failed_catheters(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPBvuAV6NRli"
   },
   "source": [
    "# Additional Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count prior failed neuraxials in this encounter and failed and total across all encounters\n",
    "\n",
    "Takes ~8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.count_prior_catheters(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "757vQkZucKbN"
   },
   "source": [
    "## Handle timeseries data (e.g., pain scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.handle_pain_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prior_pain_scores_max'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dcu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.handle_cmi_scores(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3n9mBjG2mKn"
   },
   "source": [
    "## Clean DPE and LOR_Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.handle_dpe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EALlbR6-OUaQ"
   },
   "outputs": [],
   "source": [
    "df = dcu.handle_lor_depth(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjq02Q_U2ucI"
   },
   "source": [
    "## Make numerical columns numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.numerify_columns(df, columns_to_convert = ['gestational_age_2052','bmi_end_pregnancy_2044', 'maternal_weight_end_pregnancy_2045', 'maternal_height_2046', 'gravidity_2047', 'parity_2048','baby_weight_2196','bmi_before_pregnancy_2161','secs_rom_thru_delivery_2197'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer unexpected_delta_LOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.engineer_unexpected_delta_LOR(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and plausibilify elapsed times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['secs_rom_thru_delivery_2197']/3600).describe(percentiles=[0.05,0.25,0.5,0.75,0.95,0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.convert_elapsed_times(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rdMZOcI2xZH"
   },
   "source": [
    "## Handle proceduralist names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.handle_anesthesiologists(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqbNsz2U26MN"
   },
   "source": [
    "## Feature engineering on categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.engineer_categorical_variables(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new unique identifier based on epic_pmrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dcu.create_unique_id(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5mjv9mmfl2r"
   },
   "source": [
    "# Save processed data prior to analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAF1Cqkc04aE"
   },
   "outputs": [],
   "source": [
    "complete_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWdKTtPTflV9"
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a pickle file\n",
    "complete_data.to_pickle(my_computer_fpath + \"processed_merlin_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AIT25V7f_vj"
   },
   "outputs": [],
   "source": [
    "# prompt: Import libraries and open CSV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import modules.testing as testing\n",
    "import modules.data_cleaning_utils as dcu\n",
    "import modules.data_table_utils as dtu\n",
    "from importlib import reload\n",
    "\n",
    "# my_computer_fpath = \"C:\\\\Users\\\\dfber\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\"\n",
    "my_computer_fpath = \"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pxsgBzbfzOz"
   },
   "outputs": [],
   "source": [
    "# Load the pickled DataFrame\n",
    "complete_data = pd.read_pickle(my_computer_fpath + \"processed_merlin_data.pkl\")\n",
    "\n",
    "# Now you can work with the DataFrame\n",
    "complete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwIzc7441Cot"
   },
   "outputs": [],
   "source": [
    "df = complete_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KddNYcoUiEWD"
   },
   "source": [
    "# Reduce Table to Chosen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wouftr6X4Z57"
   },
   "outputs": [],
   "source": [
    "# prompt: print all columns as a list and make it easy to read over multiple lines\n",
    "\n",
    "# Assuming 'df' is your DataFrame (as defined in the provided code)\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Print the list of columns, formatted for readability\n",
    "print(\"Columns of the DataFrame:\")\n",
    "for i, col in enumerate(all_columns):\n",
    "    print(f\"{i+1}. {col} ||| {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFrTx2-X-Jyx"
   },
   "outputs": [],
   "source": [
    "chosen_features = [\n",
    "    #    \"id\",\n",
    "    \"unique_pt_id\",\n",
    "    \"anes_procedure_encounter_id_2273\",\n",
    "    \"is_neuraxial_catheter\",\n",
    "    \"failed_catheter\",\n",
    "    \"has_subsequent_neuraxial_catheter\",\n",
    "    \"has_subsequent_spinal\",\n",
    "    \"has_subsequent_airway\",\n",
    "    #    \"best_timestamp\",\n",
    "    \"placement_to_delivery_hours\",\n",
    "    \"rom_thru_delivery_hours\",\n",
    "    \"rom_to_placement_hours\",\n",
    "    \"maternal_age_years\",\n",
    "    \"gravidity_2047\",\n",
    "    \"parity_2048\",\n",
    "    \"multiple_gestation\",\n",
    "    \"labor_induction\",\n",
    "    \"gestational_age_weeks\",\n",
    "    \"baby_weight_2196\",\n",
    "    \"fetal_position_is_posterior_or_transverse\",\n",
    "    \"fetal_position\",\n",
    "    \"fetal_presentation_is_cephalic\",\n",
    "    \"fetal_presentation\",\n",
    "    \"bmi_end_pregnancy_2044\",\n",
    "    \"bmi_greater_than_40\",\n",
    "    \"maternal_weight_end_pregnancy_2045\",\n",
    "    \"bmi_before_pregnancy_2161\",\n",
    "    \"delivery_site_is_bwh\",\n",
    "    \"delivery_site\",\n",
    "    \"has_resident\",\n",
    "    \"has_anesthesiologist\",\n",
    "    \"current_anesthesiologist_catheter_count\",\n",
    "    \"current_resident_catheter_count\", \n",
    "    \"total_team_catheter_count\",\n",
    "    \"anesthesiologist_experience_category\",\n",
    "    \"resident_experience_category\",\n",
    "    \"high_bmi_and_highly_experienced_resident\",\n",
    "    \"high_bmi_and_lowly_experienced_resident\",\n",
    "    \"high_bmi_and_no_resident\",\n",
    "    \"high_bmi_and_highly_experienced_anesthesiologist\",\n",
    "    \"scoliosis_and_highly_experienced_resident\",\n",
    "    \"scoliosis_and_lowly_experienced_resident\",\n",
    "    \"scoliosis_and_no_resident\",\n",
    "    \"scoliosis_and_highly_experienced_anesthesiologist\",\n",
    "    \"high_bmi_and_scoliosis\",\n",
    "    \"has_scoliosis\",\n",
    "    \"has_dorsalgia\",\n",
    "    \"has_back_problems\",\n",
    "    \"maternal_race\",\n",
    "    \"maternal_ethnicity\",\n",
    "    \"prior_ob_cmi_scores_max\",\n",
    "    \"CS_hx\",\n",
    "    \"high_risk_current_pregnancy\",\n",
    "    \"high_risk_hx\",\n",
    "    \"iufd\",\n",
    "    \"composite_psychosocial_problems\",\n",
    "    \"only_private_insurance\",\n",
    "    \"maternal_language_english\",\n",
    "    \"marital_status_married_or_partner\",\n",
    "    \"country_of_origin_USA\",\n",
    "    \"employment_status_fulltime\",\n",
    "    \"composite_SES_advantage\",\n",
    "    #    \"anes_procedure_note_text_2271\",\n",
    "    #    \"true_procedure_type\",\n",
    "    #    \"dpe\",\n",
    "    \"true_procedure_type_incl_dpe\",\n",
    "    \"lor_depth\",\n",
    "    \"predicted_lor_depth\",\n",
    "    \"unexpected_delta_lor\",\n",
    "    \"unexpected_delta_lor_squared\",\n",
    "    \"epidural_needle_type\",\n",
    "    \"prior_pain_scores_max\",\n",
    "    \"paresthesias_present\",\n",
    "    \"number_of_neuraxial_attempts\",\n",
    "    \"prior_failed_catheters_this_enc\",\n",
    "    \"prior_failed_catheters_prev_enc\",\n",
    "    \"prior_all_catheters_all_enc\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Excluded Columns:\")\n",
    "for i, col in enumerate(col for col in all_columns if col not in chosen_features):\n",
    "    print(f\"{i+1}. {col} ||| {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nr0wa_Zq-RfA"
   },
   "outputs": [],
   "source": [
    "df = df[chosen_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIr75B_tGNEV"
   },
   "outputs": [],
   "source": [
    "# prompt: print all columns as a list and make it easy to read over multiple lines\n",
    "\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Print the list of columns, formatted for readability\n",
    "print(\"Columns of the DataFrame:\")\n",
    "for i, col in enumerate(all_columns):\n",
    "    print(f\"{i+1}. {col} ||| {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(my_computer_fpath + 'processed_merlin_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit to neuraxial catheters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['is_neuraxial_catheter'] == 1,:]\n",
    "df.drop(columns=['is_neuraxial_catheter'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Table One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtu.describe_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_table, num_table = dtu.describe_as_tables(df)\n",
    "table_one = dtu.create_table_one(cat_table, num_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_cat_table,failures_num_table = dtu.describe_as_tables(df[df['failed_catheter'] == 1])\n",
    "successes_cat_table,succeses_num_table = dtu.describe_as_tables(df[df['failed_catheter'] == 0])\n",
    "failures_table_one = dtu.create_table_one(failures_cat_table, failures_num_table)\n",
    "successes_table_one = dtu.create_table_one(successes_cat_table, succeses_num_table)\n",
    "table_one_by_failure_status = successes_table_one.merge(failures_table_one, on='Variable', suffixes=('_success', '_failure'))\n",
    "table_one_by_failure_status = (\n",
    "    table_one\n",
    "    .merge(failures_table_one, on='Variable', suffixes=('', '_failures'), how='left')\n",
    "    .merge(successes_table_one, on='Variable', suffixes=('', '_successes'), how='left')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noteworthy columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifier columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_cols = ['anes_procedure_encounter_id_2273','unique_pt_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_col = 'failed_catheter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highly correlated columns\n",
    "\n",
    "Depending on the algorithm used, it may be wise to drop these prior to regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_cols = [\n",
    "    'current_anesthesiologist_catheter_count', # correlated with categorical experience variables\n",
    "    'current_resident_catheter_count', # correlated with categorical experience variables\n",
    "    'gravidity_2047', # correlated with parity\n",
    "    'bmi_before_pregnancy_2161', # correlated with BMI end pregnancy\n",
    "    'maternal_weight_end_pregnancy_2045', # correlated with BMI end pregnancy\n",
    "    \"only_private_insurance\", # correlated with composite_SES_advantage\n",
    "    \"maternal_language_english\", # correlated with composite_SES_advantage\n",
    "    \"marital_status_married_or_partner\", # correlated with composite_SES_advantage\n",
    "    \"country_of_origin_USA\", # correlated with composite_SES_advantage\n",
    "    \"employment_status_fulltime\", # correlated with composite_SES_advantage\n",
    "    'epidural_needle_type', # correlated with delivery location\n",
    "    'maternal_ethnicity', # correlated with race\n",
    "    \"delivery_site\", # correlated with delivery_site_bwh,\n",
    "    \"fetal_presentation_position_2247\", # correlated with position_posterior_or_transverse\n",
    "    \"fetal_presentation_category_2243\" # correlated with presentation_cephalic\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-predictive columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_predictive_columns = ['maternal_race','has_scoliosis','composite_SES_advantage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data leakage columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_leakage_columns = ['rom_thru_delivery_hours','placement_to_delivery_hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For numeric columns, impute with the median\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# Fit and transform the numeric columns\n",
    "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# For categorical columns, impute with the most frequent value\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Identify categorical columns (adjust if your dtypes are different)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Note that Boolean columns, which are converted to int64, are not imputed because they have no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Imputed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(my_computer_fpath + 'processed_and_imputed_merlin_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNKPV6NaW7+ZRxbDtLw/frD",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
