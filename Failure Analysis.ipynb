{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Referencing Guide\n",
    "\n",
    "Merlin now gives me MRN, PMRN, and Encounter. The MRN is correct in Epic but the PMRN is not.\n",
    " \n",
    "Vesela gave me a file with PMRN and Encounter. This Encounter matches Merlin but the PMRN does not match Merlin, instead the PMRN matches Epic.\n",
    " \n",
    "My colleague Ayumi gave me a file with MRN and Encounter. The MRN is correct in Epic but the Encounter does not match the other Encounters.\n",
    "\n",
    "To achieve my end goal of matching Merlin's Encounters with Ayumi's Encounters, I need to do the following:\n",
    " \n",
    "1) Start with Ayumi's Encounter\n",
    "2) Find Ayumi's corresponding MRN\n",
    "3) Match this MRN to Merlin\n",
    "4) Find Merlin's corresponding Encounter\n",
    " \n",
    "And then if I want to go from Ayumi's Encounter to a patient's true PMRN, I need to add the following steps:\n",
    "\n",
    "5) Start with Merlin's Encounter\n",
    "6) Match this Encounter in Vesela's raw data file\n",
    "7) Find the corresponding PMRN in the raw data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_computer_fpath = \"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\"\n",
    "merlin_df = merlin_df = pd.read_csv(my_computer_fpath + \"3a1615c2-2350-46d8-adf9-1415ddad370e.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled DataFrame\n",
    "# complete_data = pd.read_pickle(\"C:\\\\Users\\\\dfber\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\processed_merlin_data.pkl\")\n",
    "complete_data = pd.read_pickle(\"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\processed_merlin_data.pkl\")\n",
    "\n",
    "# Now you can work with the DataFrame\n",
    "complete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = complete_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayumi_df = pd.read_excel(\"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\ayumi_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_identified_data = pd.read_csv(\"C:\\\\Users\\\\User\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\Full Identified raw anesthesia_procedure_notes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for separate vs concatenated notes at other hospitals\n",
    "\n",
    "We can see here that before the end of 2016, concatenated notes stop appearing. The few that appear afterwards are not true concatenations but times when someone free-text commented on airway during an epidural note or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter NoteTXT for \"epidural\" and \"airway\" (case-insensitive) - this looks for double-notes but also will find single notes that contain both terms\n",
    "mask = (\n",
    "    raw_identified_data['NoteTXT'].str.contains('epidural', case=False, na=False) &\n",
    "    raw_identified_data['NoteTXT'].str.contains('airway', case=False, na=False)\n",
    ")\n",
    "filtered_notes = raw_identified_data[mask][['PatientEncounterID', 'epic_pmrn']]\n",
    "\n",
    "# Step 2: Split and explode anes_procedure_encounter_id column\n",
    "merlin_df_exploded = merlin_df.copy()\n",
    "merlin_df_exploded['anes_procedure_encounter_id_2273'] = merlin_df_exploded[\n",
    "    'anes_procedure_encounter_id_2273'\n",
    "].astype(str).str.split('|')\n",
    "merlin_df_exploded = merlin_df_exploded.explode('anes_procedure_encounter_id_2273')\n",
    "\n",
    "# Optional: strip whitespace if needed\n",
    "merlin_df_exploded['anes_procedure_encounter_id_2273'] = merlin_df_exploded[\n",
    "    'anes_procedure_encounter_id_2273'\n",
    "].str.strip()\n",
    "\n",
    "# Step 3: Merge with filtered notes\n",
    "merged = pd.merge(\n",
    "    filtered_notes,\n",
    "    merlin_df_exploded.drop('epic_pmrn', axis=1),  # Drop epic_pmrn to avoid duplication\n",
    "    left_on='PatientEncounterID',\n",
    "    right_on='anes_procedure_encounter_id_2273',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# # Step 4: Filter for delivery_site == 'nwh'\n",
    "# merged = merged[merged['delivery_site_2188'].str.lower() == 'nwh']\n",
    "\n",
    "# Step 5: Sort by delivery_date\n",
    "merged = merged.sort_values(by='delivery_date')\n",
    "\n",
    "# Step 6: Print final desired columns\n",
    "print(merged[['PatientEncounterID', 'delivery_date', 'epic_pmrn']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[['PatientEncounterID', 'delivery_date', 'epic_pmrn','delivery_site_2188']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare my data to Ayumi's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayumi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly compare Merlin and Ayumi to see how many catheters are in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure strings\n",
    "ayumi_df['MRN'] = ayumi_df['MRN'].astype(str)\n",
    "merlin_df['epidural_bwh_mrns_2354'] = merlin_df['epidural_bwh_mrns_2354'].astype(str)\n",
    "\n",
    "# Collect matching rows manually\n",
    "matches = []\n",
    "\n",
    "for _, row in ayumi_df.iterrows():\n",
    "    mrn = row['MRN']\n",
    "    matched_merlin = merlin_df[merlin_df['epidural_bwh_mrns_2354'].str.contains(mrn, na=False)]\n",
    "    \n",
    "    # For each match, combine with the ayumi row\n",
    "    for _, merlin_row in matched_merlin.iterrows():\n",
    "        combined = pd.concat([row, merlin_row])\n",
    "        matches.append(combined)\n",
    "\n",
    "# Convert to DataFrame\n",
    "merged = pd.DataFrame(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ayumi_df['MRN'].unique()), len(merlin_df['epidural_bwh_mrns_2354'].unique()), len(merged['MRN'].unique()), len(merged['epidural_bwh_mrns_2354'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare to my processed data set\n",
    "\n",
    "Note that PMRNs from here do not translate to Epic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure strings\n",
    "ayumi_df['MRN'] = ayumi_df['MRN'].astype(str)\n",
    "df['epidural_bwh_mrns_2354'] = df['epidural_bwh_mrns_2354'].astype(str)\n",
    "\n",
    "# Collect matching rows manually\n",
    "matches = []\n",
    "\n",
    "for _, row in ayumi_df.iterrows():\n",
    "    mrn = row['MRN']\n",
    "    matched_rows = df[df['epidural_bwh_mrns_2354'].str.contains(mrn, na=False)]\n",
    "    \n",
    "    # For each match, combine with the ayumi row\n",
    "    for _, match_row in matched_rows.iterrows():\n",
    "        combined = pd.concat([row, match_row])\n",
    "        matches.append(combined)\n",
    "\n",
    "# Convert to DataFrame\n",
    "merged = pd.DataFrame(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ayumi_df['MRN'].unique()), len(df['epidural_bwh_mrns_2354'].unique()), len(merged['MRN'].unique()), len(merged['epidural_bwh_mrns_2354'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove pre-2017 as these are no longer in my data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayumi_df['Time'] = pd.to_datetime(ayumi_df['Time'], errors='coerce')\n",
    "ayumi_df_post_2017 = ayumi_df[ayumi_df['Time'] > pd.Timestamp('2017-01-01')]\n",
    "# Ensure strings\n",
    "ayumi_df_post_2017['MRN'] = ayumi_df_post_2017['MRN'].astype(str)\n",
    "df['epidural_bwh_mrns_2354'] = df['epidural_bwh_mrns_2354'].astype(str)\n",
    "\n",
    "# Collect matching rows manually\n",
    "matches = []\n",
    "\n",
    "for _, row in ayumi_df_post_2017.iterrows():\n",
    "    mrn = row['MRN']\n",
    "    matched_rows = df[df['epidural_bwh_mrns_2354'].str.contains(mrn, na=False)]\n",
    "    \n",
    "    # For each match, combine with the ayumi row\n",
    "    for _, match_row in matched_rows.iterrows():\n",
    "        combined = pd.concat([row, match_row])\n",
    "        matches.append(combined)\n",
    "\n",
    "# Convert to DataFrame\n",
    "merged = pd.DataFrame(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ayumi_df_post_2017['MRN'].unique()), len(df['epidural_bwh_mrns_2354'].unique()), len(merged['MRN'].unique()), len(merged['epidural_bwh_mrns_2354'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayumi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayumi_df_post_2017.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collapse unique MRNs; if any failure then the collapsed MRN has `failed` is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct types\n",
    "merged['failed_catheter'] = merged['failed_catheter'].astype(float)\n",
    "\n",
    "# Convert Time and delivery_date to string (and drop NaT safely)\n",
    "merged['Time'] = merged['Time'].astype(str)\n",
    "merged['delivery_date'] = merged['delivery_date'].astype(str)\n",
    "\n",
    "# Collapse to one row per MRN with failed + concatenated dates\n",
    "collapsed = (\n",
    "    merged.groupby('MRN')\n",
    "    .agg({\n",
    "        'failed_catheter': lambda x: (x == 1).any(),  # True if any failed\n",
    "        'Time': lambda x: '|'.join(sorted(set(x))),  # concat unique Times\n",
    "        'delivery_date': lambda x: '|'.join(sorted(set(x))),  # concat unique delivery_dates\n",
    "        'epic_pmrn': 'first'  # just pick first one (or use a join here too if needed)\n",
    "    })\n",
    "    .rename(columns={'failed_catheter': 'failed'})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed['failed'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed[collapsed['failed'] == False] # Catheters that are marked as failed by Ayumi but marked as not-failed by me:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catheters that are marked as failed by Ayumi but marked as not-failed by me:\n",
    "1) The anesthesia encounter was terminated and a new one was made for the repeat epidural 10080675744\n",
    "2) The repeat catheter was within 10 minutes of the first one 10040815687\n",
    "3) Epidural note not captured in Merlin 10052565972\n",
    "4) Not actually a failure! 10091378998\n",
    "5) Repeat catheter placed within 10 minutes of the first one 10131236735\n",
    "6) Epidural note not captured in Merlin 10097511113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter delivery date\n",
    "df['delivery_datetime'] = pd.to_datetime(df['delivery_datetime'], errors='coerce')\n",
    "df_filtered = df[df['best_timestamp'] < pd.Timestamp('2021-01-01',tz='America/New_York')]\n",
    "df_filtered = df_filtered[df_filtered['delivery_site'] == 'bwh']\n",
    "\n",
    "# Step 2: Keep only failed_catheter == 1\n",
    "df_filtered = df_filtered[df_filtered['failed_catheter'] == 1]\n",
    "\n",
    "# Step 3: Prepare ayumi MRNs as a set for fast lookup\n",
    "ayumi_mrns = set(ayumi_df['MRN'].astype(str))\n",
    "\n",
    "# Step 4: Check for match in any |-delimited MRN\n",
    "def get_match_info(epi_str):\n",
    "    if pd.isna(epi_str):\n",
    "        return False, None\n",
    "    epidural_mrns = epi_str.split('|')\n",
    "    for mrn in epidural_mrns:\n",
    "        if mrn in ayumi_mrns:\n",
    "            return True, mrn\n",
    "    return False, None\n",
    "\n",
    "\n",
    "df_filtered[['has_ayumi_match', 'matching_ayumi_mrn']] = df_filtered['epidural_bwh_mrns_2354'] \\\n",
    "    .astype(str) \\\n",
    "    .apply(lambda x: pd.Series(get_match_info(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[df_filtered['has_ayumi_match']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[~df_filtered['has_ayumi_match']][['epidural_bwh_mrns_2354', 'delivery_datetime', 'best_timestamp']] # Catheters that are marked as failed by Ayumi but marked as not-failed by me:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem generally to be true failures that were missed by Ayumi for whatever reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually analyze some successes and failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_identified_data.loc[raw_identified_data['NoteID'] == '2362576456']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of 'NoteID' to 'epic_pmrn' from raw_identified_data\n",
    "note_to_mrn = raw_identified_data.set_index('NoteID')['epic_pmrn'].to_dict()\n",
    "note_to_purpose = raw_identified_data.set_index('NoteID')['NotePurposeDSC'].to_dict()\n",
    "\n",
    "# Use the mapping to create the new 'mrn' column in df\n",
    "df['epic_pmrn'] = df['anes_procedure_note_id_2260'].map(note_to_mrn)\n",
    "df['NotePurposeDSC'] = df['anes_procedure_note_id_2260'].map(note_to_purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Choose 10 random failed_catheters and 10 random non-failed_catheters\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it contains a column 'failed_catheter'\n",
    "failed_catheters = df[df['failed_catheter'] == 1]\n",
    "non_failed_catheters = df[df['failed_catheter'] == 0]\n",
    "\n",
    "# Randomly choose 10 failed catheters\n",
    "random_failed_catheters = failed_catheters.sample(n=10, random_state=42)  # random_state for reproducibility\n",
    "chosen_failed_catheter_encounter_ids = ['3324914343','3272008150','3234765502','3305371022','3216449190','3186345033','3493903332','3285273066','3320528828','3191160118']\n",
    "chosen_failed_catheters = df[df['anes_procedure_encounter_id_2273'].isin(chosen_failed_catheter_encounter_ids)]\n",
    "\n",
    "# Randomly choose 10 non-failed catheters\n",
    "random_non_failed_catheters = non_failed_catheters.sample(n=10, random_state=42) # random_state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    'epic_pmrn',\n",
    "    \"best_timestamp\",\n",
    "    \"failed_catheter\",\n",
    "    \"true_procedure_type\",\n",
    "    \"NotePurposeDSC\",\n",
    "    \"Regulated_Anesthesiologist_Name\",\n",
    "    \"Regulated_Resident_Name\",\n",
    "    \"anes_procedure_encounter_id_2273\",\n",
    "    \"anes_procedure_note_id_2260\",\n",
    "    \"subsequent_proof_of_failure_note_id\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_procedure_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_failed_catheters[column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_failed_catheters[column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_non_failed_catheters[column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['anes_procedure_encounter_id_2273'] == '3191160118'][column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for failed catheters and delivery location 'mgh'\n",
    "random_failed_catheters_mgh = df[(df['failed_catheter'] == True) & (df['delivery_site_2188'] == 'mgh')].sample(n=10, random_state=42)\n",
    "\n",
    "# Display the chosen sample\n",
    "random_failed_catheters_mgh[column_names]\n",
    "chosen_failed_catheters_mgh_encounter_ids = [\"3268447806\", \"3396191507\", \"3258959083\", \"3581696894\", \"3271964781\", \"3583787789\", \"3402989492\", \"3476124055\", \"3304131417\", \"3522418740\"]\n",
    "chosen_failed_catheter_mgh_note_ids = ['2903598031', '6426160113', '2535157730', '11282242570',\n",
    "       '3002237621', '11340428769', '6612736939', '8559605944',\n",
    "       '3947063203', '9788012155']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['anes_procedure_encounter_id_2273'].isin(chosen_failed_catheters_mgh_encounter_ids)][column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['anes_procedure_note_id_2260'].isin(chosen_failed_catheter_mgh_note_ids)][column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_failed_catheters_mgh[column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at cases where a failure is replaced by the same anesthesia team\n",
    "\n",
    "Encounter_ID 3607123568 is an example where the same attending/resident team did the index procedure and the replacement (in this case because the epidural migrated out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_replacement_has_same_anesthesia_team(row, df):\n",
    "    if row['is_neuraxial_catheter'] and row['failed_catheter']:\n",
    "        this_anesthesiologist = row['Regulated_Anesthesiologist_Name']\n",
    "        this_resident = row['Regulated_Resident_Name']\n",
    "        subsequent_procedures = row['subsequent_proof_of_failure_note_id'].replace('\\'','').replace('[','').replace(']','').split(',')\n",
    "        for note_id in subsequent_procedures:\n",
    "            replacing_anesthesiologist = df[(df['anes_procedure_note_id_2260'] == note_id) & (df['is_neuraxial_catheter'] == True)]['Regulated_Anesthesiologist_Name']\n",
    "            replacing_resident = df[(df['anes_procedure_note_id_2260'] == note_id) & (df['is_neuraxial_catheter'] == True)]['Regulated_Resident_Name']\n",
    "            if (this_anesthesiologist == replacing_anesthesiologist).any() and (this_resident == replacing_resident).any():\n",
    "                # print()\n",
    "                # print('new note')\n",
    "                # print(this_anesthesiologist)\n",
    "                # print(this_resident)\n",
    "                # print(replacing_anesthesiologist)\n",
    "                # print(replacing_resident)\n",
    "                # print(this_anesthesiologist == replacing_anesthesiologist)\n",
    "                # print(this_resident == replacing_resident)\n",
    "                row['replaced_by_same_team'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['failed_catheter']==1].apply(lambda x: check_if_replacement_has_same_anesthesia_team(x, df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
