{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import modules.data_analysis_utils as dau\n",
    "import modules.data_visualization_utils as dvu\n",
    "from importlib import reload\n",
    "\n",
    "# my_computer_fpath = \"C:\\\\Users\\\\dfber\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\"\n",
    "my_computer_fpath = \"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(my_computer_fpath + 'processed_and_imputed_merlin_data.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepend '#' for better dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_procedure_type_incl_dpe'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_default_categories = {\n",
    "    'true_procedure_type_incl_dpe': 'epidural',\n",
    "    'anesthesiologist_experience_category': 'moderate',\n",
    "    'resident_experience_category': 'no_resident',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dau.prepend_char(df, '#', chosen_default_categories=chosen_default_categories, cols_to_ignore=['anes_procedure_encounter_id_2273','unique_pt_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuraxial_catheter_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some individually interesting regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = neuraxial_catheter_df.dropna(subset=['lor_depth', 'number_of_neuraxial_attempts'])\n",
    "\n",
    "# Fit the model using the formula\n",
    "model = smf.ols('number_of_neuraxial_attempts ~ lor_depth', data=df_corr).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Do univariate logistic regression separately using number of attempts and loss of resistance depth to predict failure\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Prepare the data for logistic regression with number of attempts as the predictor\n",
    "df_logreg_attempts = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'failed_catheter'])\n",
    "# Fit the logistic regression model\n",
    "model_attempts = smf.logit('failed_catheter ~ number_of_neuraxial_attempts', data=df_logreg_attempts).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_attempts.summary())\n",
    "\n",
    "\n",
    "# Prepare the data for logistic regression with loss of resistance depth as the predictor\n",
    "df_logreg_lor = neuraxial_catheter_df.dropna(subset=['lor_depth', 'failed_catheter'])\n",
    "# Fit the logistic regression model\n",
    "model_lor = smf.logit('failed_catheter ~ lor_depth', data=df_logreg_lor).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_lor.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Now do multivariate analysis using the same two predictors\n",
    "\n",
    "# Prepare the data for logistic regression with both predictors\n",
    "df_logreg_multi = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'lor_depth', 'failed_catheter'])\n",
    "\n",
    "# Fit the logistic regression model with both predictors\n",
    "model_multi = smf.logit('failed_catheter ~ number_of_neuraxial_attempts + lor_depth', data=df_logreg_multi).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_multi.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for logistic regression with prior_failed_catheters_this_enc as the predictor\n",
    "df_logreg_prior_failed = neuraxial_catheter_df.dropna(subset=['prior_failed_catheters_this_enc', 'failed_catheter'])\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model_attempts = smf.logit('failed_catheter ~ prior_failed_catheters_this_enc', data=df_logreg_prior_failed).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_attempts.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All univariate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = dau.all_regressions_each_dummy(neuraxial_catheter_df, 'failed_catheter')\n",
    "# This returns a DataFrame with columns: [column, param_name, coef, pval].\n",
    "# Each level of a categorical predictor will appear as a separate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['category_variable'] = results_df['param_name'].apply(dau.parse_param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['column','category_variable','coef','pval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_variables = results_df.shape[0]\n",
    "alpha = 0.05 / total_num_variables\n",
    "significant_variables = results_df[results_df['pval'] < alpha].shape[0]\n",
    "text = f'Of {total_num_variables} total variables, {significant_variables} were significant at a \\nBonferroni-corrected alpha = {alpha}'\n",
    "dau.show_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_coef_vs_pval(results_df.loc[results_df['column']!='placement_to_delivery_hours',:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['bmi_end_pregnancy_2044', 'baby_weight_2196', 'gestational_age_weeks', 'maternal_age_years']:\n",
    "    neuraxial_catheter_df[col] = neuraxial_catheter_df[col] - neuraxial_catheter_df[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuraxial_catheter_df['gestational_age_weeks'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce colinear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvu.plot_correlation_heatmap_with_related_groups(neuraxial_catheter_df, drop_columns=['anes_procedure_encounter_id_2273','unique_pt_id'],additional_groups='preset',draw_group_boxes=True,draw_group_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuraxial_catheter_df_reduced = dau.reduce_cols(neuraxial_catheter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvu.plot_correlation_heatmap_with_related_groups(neuraxial_catheter_df_reduced, drop_columns=['anes_procedure_encounter_id_2273','unique_pt_id'],additional_groups='preset',draw_group_boxes=True,draw_group_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = neuraxial_catheter_df_reduced.copy()\n",
    "failure_rate = test_dataset['failed_catheter'].mean()\n",
    "test_dataset['failed_catheter'] = np.random.binomial(n=1, p=failure_rate, size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = dau.preprocess_data(data=test_dataset)\n",
    "dau.do_logistic_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real LOGIT regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = dau.preprocess_data_for_statsmodels(data=neuraxial_catheter_df_reduced.copy())\n",
    "# If I try to use the general preprocess_data function, the variable names get screwed up\n",
    "# and so does the results_summ\n",
    "# So I use this custom one instead that uses pd.get_dummies instead of OneHotEncoder\n",
    "# I don't know exactly why it needs to be this way\n",
    "y_pred_prob, result_summ, evaluation_metrics, evaluation_metrics_by_threshold = dau.do_logistic_regression_with_statsmodels(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_by_threshold = pd.DataFrame(evaluation_metrics_by_threshold)\n",
    "evaluation_metrics_by_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvu.plot_roc_curve(y_test,y_pred_prob, evaluation_metrics['roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_result_table, logit_predictor_table = dau.prepare_logit_tables(result_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df, procedural_df = dau.divide_and_rename_patient_and_procedural_factors(logit_predictor_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvu.show_forest_plots(patient_df, procedural_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = dau.preprocess_data(data=neuraxial_catheter_df_reduced.copy())\n",
    "logistic_model = dau.do_logistic_regression(X_train, X_test, y_train, y_test)\n",
    "dau.print_sklearn_coefficients(feature_names=X_train.columns, coefficients=logistic_model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring for DPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df['dpe'] = (df['true_procedure_type_incl_dpe'] == 'dpe').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'dpe'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring for CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df['cse'] = (df['true_procedure_type_incl_dpe'] == 'cse').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'cse'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity scoring for DPE vs CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df = df[df['true_procedure_type_incl_dpe'].isin(['cse', 'dpe'])]\n",
    "df['cse_not_dpe'] = (df['true_procedure_type_incl_dpe'] == 'cse').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'cse_not_dpe'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n",
    "\n",
    "del df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
