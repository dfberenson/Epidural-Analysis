{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import modules.data_analysis_utils as dau\n",
    "from importlib import reload\n",
    "\n",
    "# my_computer_fpath = \"C:\\\\Users\\\\dfber\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\"\n",
    "my_computer_fpath = \"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(my_computer_fpath + 'processed_and_imputed_merlin_data.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepend '#' for better dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dau.prepend_char_to_most_common(df, '#', cols_to_ignore=['anes_procedure_encounter_id_2273','unique_pt_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuraxial_catheter_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete unknowable columns to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_leakage_columns = ['rom_thru_delivery_hours','placement_to_delivery_hours']\n",
    "neuraxial_catheter_df = neuraxial_catheter_df.drop(data_leakage_columns, axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some individually interesting regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = neuraxial_catheter_df.dropna(subset=['lor_depth', 'number_of_neuraxial_attempts'])\n",
    "\n",
    "# Fit the model using the formula\n",
    "model = smf.ols('number_of_neuraxial_attempts ~ lor_depth', data=df_corr).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Do univariate logistic regression separately using number of attempts and loss of resistance depth to predict failure\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Prepare the data for logistic regression with number of attempts as the predictor\n",
    "df_logreg_attempts = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'failed_catheter'])\n",
    "# Fit the logistic regression model\n",
    "model_attempts = smf.logit('failed_catheter ~ number_of_neuraxial_attempts', data=df_logreg_attempts).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_attempts.summary())\n",
    "\n",
    "\n",
    "# Prepare the data for logistic regression with loss of resistance depth as the predictor\n",
    "df_logreg_lor = neuraxial_catheter_df.dropna(subset=['lor_depth', 'failed_catheter'])\n",
    "# Fit the logistic regression model\n",
    "model_lor = smf.logit('failed_catheter ~ lor_depth', data=df_logreg_lor).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_lor.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Now do multivariate analysis using the same two predictors\n",
    "\n",
    "# Prepare the data for logistic regression with both predictors\n",
    "df_logreg_multi = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'lor_depth', 'failed_catheter'])\n",
    "\n",
    "# Fit the logistic regression model with both predictors\n",
    "model_multi = smf.logit('failed_catheter ~ number_of_neuraxial_attempts + lor_depth', data=df_logreg_multi).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_multi.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for logistic regression with prior_failed_catheters_this_enc as the predictor\n",
    "df_logreg_prior_failed = neuraxial_catheter_df.dropna(subset=['prior_failed_catheters_this_enc', 'failed_catheter'])\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model_attempts = smf.logit('failed_catheter ~ prior_failed_catheters_this_enc', data=df_logreg_prior_failed).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_attempts.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All univariate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = dau.all_regressions_each_dummy(neuraxial_catheter_df, 'failed_catheter')\n",
    "# This returns a DataFrame with columns: [column, param_name, coef, pval].\n",
    "# Each level of a categorical predictor will appear as a separate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['category_variable'] = results_df['param_name'].apply(dau.parse_param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['column','category_variable','coef','pval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['pval'] < 0.05 / results_df.shape[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot: coefficient vs -log10(p-value)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "offset = 1e-300  # so we don't take log10(0)\n",
    "x_vals = results_df[results_df['pval'] < 0.9]['coef']\n",
    "y_vals = -np.log10(results_df[results_df['pval'] < 0.9]['pval'] + offset)\n",
    "\n",
    "sc = ax.scatter(x_vals, y_vals, color='blue')\n",
    "\n",
    "# Annotate each point\n",
    "for i, row in results_df[results_df['pval'] < 0.9].iterrows():\n",
    "    ax.text(\n",
    "        row['coef'],\n",
    "        -np.log10(row['pval'] + offset),\n",
    "        dau.remove_nums(str(row['column'] + ('__' + str(row['category_variable'])) if row['category_variable'] != '' else row['column'])),\n",
    "        fontsize=8,\n",
    "        ha='left',\n",
    "        va='bottom'\n",
    "    )\n",
    "\n",
    "# Add a reference line for p=0.05\n",
    "ax.axhline(-np.log10(0.05), color='red', linestyle='--', label='p=0.05')\n",
    "\n",
    "ax.set_xlabel('Coefficient')\n",
    "ax.set_ylabel('-log10(p-value)')\n",
    "ax.set_title(f'Logistic Regressions for Catheter_Failure ~ Each Predictor (All Dummies)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['bmi_end_pregnancy_2044', 'baby_weight_2196', 'gestational_age_weeks', 'maternal_age_years']:\n",
    "    neuraxial_catheter_df[col] = neuraxial_catheter_df[col] - neuraxial_catheter_df[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuraxial_catheter_df['gestational_age_weeks'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random data for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = neuraxial_catheter_df.copy()\n",
    "failure_rate = test_dataset['failed_catheter'].mean()\n",
    "test_dataset['failed_catheter'] = np.random.binomial(n=1, p=failure_rate, size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = test_dataset\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "# If you do NOT want to use `anes_procedure_encounter_id_2273` as a feature:\n",
    "X = X.drop(columns=[\"unique_pt_id\"])  \n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train logistic regression with class weights\n",
    "logistic_model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', n_jobs=1)\n",
    "logistic_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = logistic_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"TEST RANDOM Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nTEST RANDOM Classification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real LOGIT regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Load and clean data\n",
    "data = neuraxial_catheter_df.copy()\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "# 2. Group-aware split\n",
    "groups = X['unique_pt_id']\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "\n",
    "# 3. Identify numeric vs. categorical\n",
    "numeric_features = X_train.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# 4. Impute numeric\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "X_train[numeric_features] = num_imputer.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = num_imputer.transform(X_test[numeric_features])\n",
    "\n",
    "# 5. Scale numeric\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# 6. Impute categorical\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_features] = cat_imputer.fit_transform(X_train[categorical_features])\n",
    "X_test[categorical_features] = cat_imputer.transform(X_test[categorical_features])\n",
    "\n",
    "# 7. One-hot encode categorical\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features, drop_first=True,dtype=int).astype(float)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_features, drop_first=True,dtype=int).astype(float)\n",
    "\n",
    "X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "# 8. Fit logistic regression with Statsmodels\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit(disp=0)\n",
    "\n",
    "# 9. Predict\n",
    "X_test_const = sm.add_constant(X_test, has_constant='add')\n",
    "y_pred_prob = result.predict(X_test_const)\n",
    "\n",
    "evaluation_metrics_by_threshold = []\n",
    "\n",
    "for i in range(0, 21):\n",
    "    prediction_threshold = i / 20\n",
    "    y_pred = (y_pred_prob >= prediction_threshold).astype(int)\n",
    "\n",
    "    # 10. Evaluate\n",
    "    evaluation_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred,zero_division=np.nan),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "        # \"classification_report\": classification_report(y_test, y_pred)\n",
    "    }\n",
    "    evaluation_metrics_by_threshold.append(evaluation_metrics)\n",
    "\n",
    "result_summ = result.summary(alpha = 0.001)\n",
    "\n",
    "# print(\"Model Evaluation:\")\n",
    "# for metric, value in evaluation_metrics.items():\n",
    "#     if metric == \"classification_report\":\n",
    "#         print(\"\\nClassification Report:\\n\", value)\n",
    "#     else:\n",
    "#         print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_by_threshold = pd.DataFrame(evaluation_metrics_by_threshold)\n",
    "evaluation_metrics_by_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_result_table = result_summ.tables[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table = pd.DataFrame(result_summ.tables[1].data)\n",
    "# 1. Set the first row as the new column headers.\n",
    "logit_predictor_table.columns = logit_predictor_table.iloc[0]\n",
    "\n",
    "# 2. Remove the first row (since it's now serving as header).\n",
    "logit_predictor_table = logit_predictor_table[1:]\n",
    "\n",
    "# 3. Set the first column (after the column headers update) as the index.\n",
    "# Here, `df.columns[0]` represents the name of the first column.\n",
    "logit_predictor_table = logit_predictor_table.set_index(logit_predictor_table.columns[0])\n",
    "\n",
    "# 4. Sort by the 'P>|z|' column in ascending order.\n",
    "logit_predictor_table = logit_predictor_table.sort_values('P>|z|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the odds ratio (OR) and the 95% confidence intervals\n",
    "logit_predictor_table['OR'] = np.exp(logit_predictor_table['coef'].astype(float))\n",
    "logit_predictor_table['OR_lower'] = np.exp(logit_predictor_table['[0.0005'].astype(float))\n",
    "logit_predictor_table['OR_upper'] = np.exp(logit_predictor_table['0.9995]'].astype(float))\n",
    "\n",
    "# Create the 'OR (95% CI)' column\n",
    "logit_predictor_table['OR (99.9% CI)'] = logit_predictor_table.apply(\n",
    "    lambda row: f\"{row['OR']:.2f} ({row['OR_lower']:.2f} - {row['OR_upper']:.2f})\", axis=1)\n",
    "\n",
    "logit_predictor_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_factors = ['delivery_site_bwh',\n",
    "       'parity_2048', 'gestational_age_weeks',\n",
    "       'presentation_cephalic',\n",
    "       'has_dorsalgia',\n",
    "       'maternal_age_years', 'has_back_problems',\n",
    "       'prior_failed_catheters_prev_enc',\n",
    "       'baby_weight_2196',\n",
    "       'composite_psychosocial_problems', 'prior_failed_catheters_this_enc',\n",
    "       'position_posterior_or_transverse', 'prior_pain_scores_max',\n",
    "       'bmi_end_pregnancy_2044', 'labor_induction']\n",
    "\n",
    "procedural_factors = ['true_procedure_type_incl_dpe_cse', \n",
    "       'true_procedure_type_incl_dpe_dpe',\n",
    "       'highly_experienced_anesthesiologist_none', 'lor_depth', 'number_of_neuraxial_attempts',\n",
    "       'paresthesias_present',\n",
    "       'highly_experienced_resident_no',\n",
    "       'highly_experienced_anesthesiologist_yes',\n",
    "       'highly_experienced_resident_yes',\n",
    "       'true_procedure_type_incl_dpe_intrathecal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = logit_predictor_table.loc[patient_factors].copy()\n",
    "procedural_df = logit_predictor_table.loc[procedural_factors].copy()\n",
    "\n",
    "rename_map = {\n",
    "    'gestational_age_weeks': 'Gestational Age (per week)',\n",
    "    'delivery_site_bwh': 'Delivery at our obstetric teaching hospital (vs other)',\n",
    "    'baby_weight_2196': 'Baby Weight (per kg)',\n",
    "    'bmi_end_pregnancy_2044': 'BMI (per kg/m^2)',\n",
    "    'parity_2048': 'Parity (per birth)',\n",
    "    'has_dorsalgia': 'Back pain (vs none)',\n",
    "    'has_back_problems': 'Scoliosis or other back problems (vs none)',\n",
    "    'prior_pain_scores_max': 'Max pain score prior to placement (per unit 0-10)',\n",
    "    'composite_psychosocial_problems': 'Psychosocial risk factors (vs none)',\n",
    "    'prior_failed_catheters_this_enc': 'Prior failed catheters in this encounter (per failure)',\n",
    "    'prior_failed_catheters_prev_enc': 'Prior failed catheters in prior encounters (per failure)',\n",
    "    'maternal_age_years': 'Maternal age (per year)',\n",
    "    'labor_induction': 'Induced labor (vs not)',\n",
    "    'position_posterior_or_transverse': 'Posterior or transverse fetal position (vs other)',\n",
    "    'presentation_cephalic': 'Cephalic fetal presentation (vs other)',\n",
    "    # procedural factors below\n",
    "    'lor_depth': 'Depth to loss of resistance (per cm)',\n",
    "    'highly_experienced_anesthesiologist_yes': 'Highly experienced attending anesthesiologist (vs less experienced)',\n",
    "    'highly_experienced_anesthesiologist_none': 'No attending anesthesiologist (vs less experienced attending)',\n",
    "    'highly_experienced_resident_yes': 'Highly experienced resident (vs no resident)',\n",
    "    'highly_experienced_resident_no': 'Less experienced resident (vs no resident)',\n",
    "    'paresthesias_present': 'Paresthesias present during placement (vs none)',\n",
    "    'number_of_neuraxial_attempts': 'Number of placement attempts (per attempt)',\n",
    "    'true_procedure_type_incl_dpe_intrathecal': 'Intrathecal catheter (vs conventional epidural)',\n",
    "    'true_procedure_type_incl_dpe_dpe': 'Dural puncture epidural (vs conventional epidural)',\n",
    "    'true_procedure_type_incl_dpe_cse': 'Combined spinal-epidural (vs conventional epidural)'\n",
    "}\n",
    "\n",
    "patient_df = patient_df.rename(index=rename_map)\n",
    "procedural_df = procedural_df.rename(index=rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "def plot_forest_colored_with_markers(\n",
    "    ax, \n",
    "    df, \n",
    "    title='Forest Plot', \n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5, \n",
    "    x_max=1.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a forest chart on 'ax' given a DataFrame 'df' with columns:\n",
    "      - 'OR'\n",
    "      - 'OR_lower'\n",
    "      - 'OR_upper'\n",
    "    \n",
    "    X-axis is restricted to [x_min, x_max].\n",
    "    \n",
    "    Rules:\n",
    "      - If the center OR is outside [x_min, x_max], skip plotting its dot.\n",
    "      - If the OR or any part of its CI is beyond [x_min, x_max], place '<' or '>' at that boundary.\n",
    "      - Print \"OR X.XX (L.LL, U.UU)\" above each data point in the same color.\n",
    "      - Color each factor's name on the y-axis to match that factor's color.\n",
    "    \n",
    "    Color scheme for significance:\n",
    "      - red if entire CI > 1\n",
    "      - blue if entire CI < 1\n",
    "      - black otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by OR if you want smaller/larger ORs in order\n",
    "    df = df.sort_values('OR')\n",
    "\n",
    "    # We'll manually set the y-ticks, one row per factor\n",
    "    y_positions = np.arange(len(df))\n",
    "\n",
    "    # We won't set the yticklabels yet; we'll do them manually to color each label.\n",
    "    ax.set_yticks(y_positions)\n",
    "    # Temporarily set them all to blank\n",
    "    ax.set_yticklabels([\"\"] * len(df))\n",
    "\n",
    "    # We'll collect the color for each row, so we can color the labels afterward\n",
    "    factor_colors = []\n",
    "\n",
    "    # Plot each factor individually\n",
    "    for y_pos, (idx, row) in zip(y_positions, df.iterrows()):\n",
    "        or_val = row['OR']\n",
    "        ci_low = row['OR_lower']\n",
    "        ci_high = row['OR_upper']\n",
    "\n",
    "        # Decide the color based on significance\n",
    "        if ci_low > 1:\n",
    "            c = 'red'    # entire CI above 1 => significant risk\n",
    "        elif ci_high < 1:\n",
    "            c = 'blue'   # entire CI below 1 => significant protective\n",
    "        else:\n",
    "            c = 'black'  # not significant\n",
    "\n",
    "        factor_colors.append(c)\n",
    "\n",
    "        # Check if OR or CI extends beyond the plot range\n",
    "        outside_left = (or_val < x_min) or (ci_low < x_min)\n",
    "        outside_right = (or_val > x_max) or (ci_high > x_max)\n",
    "\n",
    "        # If the center OR is out of range, skip the dot\n",
    "        center_outside = (or_val < x_min) or (or_val > x_max)\n",
    "        dot_fmt = 'none' if center_outside else 'o'\n",
    "\n",
    "        # Calculate the full error bar from the center\n",
    "        left_err = or_val - ci_low\n",
    "        right_err = ci_high - or_val\n",
    "\n",
    "        # Plot the error bar (may or may not include the dot)\n",
    "        ax.errorbar(\n",
    "            or_val,\n",
    "            y_pos,\n",
    "            xerr=[[left_err], [right_err]],\n",
    "            fmt=dot_fmt,   # skip the dot if center is outside\n",
    "            color=c,\n",
    "            ecolor=c,\n",
    "            capsize=4\n",
    "        )\n",
    "\n",
    "        # Place boundary markers if the OR or any part of CI is outside\n",
    "        if outside_left:\n",
    "            ax.text(\n",
    "                x_min, y_pos, '<', \n",
    "                va='center', ha='right', color=c, fontsize=14\n",
    "            )\n",
    "        if outside_right:\n",
    "            ax.text(\n",
    "                x_max, y_pos, '>', \n",
    "                va='center', ha='left', color=c, fontsize=14\n",
    "            )\n",
    "\n",
    "        # Prepare the label \"OR X.XX (L.LL - U.UU)\"\n",
    "        label_str = f\"OR {or_val:.2f} ({ci_low:.2f} - {ci_high:.2f})\"\n",
    "\n",
    "        # Place the label just above the data point (or boundary)\n",
    "        # We'll define a small offset in Y to shift text \"above\" the marker\n",
    "        label_offset = 0.2  # Adjust as needed\n",
    "        label_y = y_pos - label_offset  # axis is inverted => subtract to go \"up\"\n",
    "        ax.text(\n",
    "            1.06, label_y,\n",
    "            label_str,\n",
    "            va='bottom',   # text rises from the point\n",
    "            ha='center',\n",
    "            color=c,\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "    # Now color each factor name using the same color\n",
    "    # We already set blank y-ticklabels, so let's manually place them:\n",
    "    for y_pos, (idx, c) in zip(y_positions, zip(df.index, factor_colors)):\n",
    "        # We'll place the text a bit left of x_min so it doesn't collide with the plot\n",
    "        ax.text(\n",
    "            x_min - 0.05, y_pos,\n",
    "            idx,\n",
    "            va='center', ha='right',\n",
    "            color=c,\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "    # Draw a vertical line at OR=1\n",
    "    ax.axvline(x=1, color='gray', linestyle='--')\n",
    "\n",
    "    # Invert y-axis so the top row is at the top\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Limit the x-axis\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_xlabel(x_label, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Example usage with TWO subplots\n",
    "# ==========================\n",
    "\n",
    "# Suppose you have:\n",
    "#   patient_df\n",
    "#   procedural_df\n",
    "# Each with columns: ['OR', 'OR_lower', 'OR_upper']\n",
    "# and index = factor names.\n",
    "\n",
    "# Create the figure with two columns\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 8))\n",
    "\n",
    "plot_forest_colored_with_markers(\n",
    "    ax=ax1,\n",
    "    df=patient_df,\n",
    "    title='Patient Factors',\n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5,\n",
    "    x_max=1.5\n",
    ")\n",
    "\n",
    "plot_forest_colored_with_markers(\n",
    "    ax=ax2,\n",
    "    df=procedural_df,\n",
    "    title='Procedural Factors',\n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5,\n",
    "    x_max=1.5\n",
    ")\n",
    "\n",
    "# Create a manual legend for color interpretation:\n",
    "protect_marker = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
    "                               label='Significant protective factor')\n",
    "ns_marker = mlines.Line2D([], [], color='black', marker='o', linestyle='None',\n",
    "                          label='Not significant')\n",
    "risk_marker = mlines.Line2D([], [], color='red', marker='o', linestyle='None',\n",
    "                            label='Significant risk factor')\n",
    "\n",
    "fig.legend(\n",
    "    handles=[protect_marker, ns_marker, risk_marker],\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.05),\n",
    "    ncol=3,\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = neuraxial_catheter_df\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "\n",
    "## Drop delivery_site\n",
    "# X = X.drop(columns=[\"delivery_site\"],errors='ignore')\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "# If you do NOT want to use `anes_procedure_encounter_id_2273` as a feature:\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])  \n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train logistic regression with class weights\n",
    "logistic_model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', n_jobs=1)\n",
    "logistic_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = logistic_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nClassification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the feature names produced by the ColumnTransformer\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 2. Get the coefficients from the trained logistic model\n",
    "#    For binary classification, .coef_ is an array of shape (1, n_features)\n",
    "coefficients = logistic_model.coef_[0]\n",
    "\n",
    "\n",
    "# 3. Combine feature names with coefficients into a list of tuples\n",
    "coef_pairs = list(zip(feature_names, coefficients))\n",
    "\n",
    "# 4. Print the coefficients sorted by absolute magnitude\n",
    "print(\"Coefficients for each feature (sorted by absolute magnitude):\")\n",
    "for name, coef in sorted(coef_pairs, key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "# 5. Print the coefficients sorted alphabetically\n",
    "print('---------------------------------------------')\n",
    "print(\"Coefficients for each feature (sorted alphabetically):\")\n",
    "for name, coef in sorted(coef_pairs):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "# 6. Print the intercept\n",
    "print(f\"Intercept: {logistic_model.intercept_[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Imports\n",
    "##############################################################################\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Import XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "##############################################################################\n",
    "# 0. Load and prepare the dataset\n",
    "##############################################################################\n",
    "data = neuraxial_catheter_df\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "# # Drop delivery_site\n",
    "# X = X.drop(columns=[\"delivery_site\"],errors='ignore')\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])  # remove ID column from features\n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "##############################################################################\n",
    "# 3. Identify numeric and categorical columns\n",
    "##############################################################################\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "##############################################################################\n",
    "# 4. Create preprocessing pipelines\n",
    "##############################################################################\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "##############################################################################\n",
    "# 5. Preprocess the data\n",
    "##############################################################################\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "##############################################################################\n",
    "# 6. Train XGBoost Classifier\n",
    "##############################################################################\n",
    "# Instantiate the XGBClassifier\n",
    "# Note: You can tune parameters such as 'scale_pos_weight' if your data is imbalanced.\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=(y_train.shape[0] - y_train.sum()) / y_train.sum()\n",
    ")\n",
    "xgb_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "##############################################################################\n",
    "# 7. Make predictions\n",
    "##############################################################################\n",
    "y_pred = xgb_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = xgb_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "##############################################################################\n",
    "# 8. Evaluate the model\n",
    "##############################################################################\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "##############################################################################\n",
    "# 9. Print the evaluation metrics\n",
    "##############################################################################\n",
    "print(\"Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nClassification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring for DPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df['dpe'] = (df['true_procedure_type_incl_dpe'] == 'dpe').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'dpe'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring for CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df['cse'] = (df['true_procedure_type_incl_dpe'] == 'cse').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'cse'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity scoring for DPE vs CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df = df[df['true_procedure_type_incl_dpe'].isin(['cse', 'dpe'])]\n",
    "df['cse_not_dpe'] = (df['true_procedure_type_incl_dpe'] == 'cse').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'cse_not_dpe'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "XGBoost - hyperparameter tuning with Optuna\n",
    "\n",
    "Shapley values for interpretability\n",
    "\n",
    "Eliminate features that are both poorly predictive and have lots of missing data\n",
    "\n",
    "Abstract functions and separate them into different files\n",
    "\n",
    "Fewer features will improve interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
