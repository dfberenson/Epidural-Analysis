{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# my_computer_fpath = \"C:\\\\Users\\\\dfber\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\"\n",
    "my_computer_fpath = \"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(my_computer_fpath + 'minimal_merlin_data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend '#' to the most common entry in each categorical column so this will be the one dropped first alphabetically when dummies are made\n",
    "for column in df.select_dtypes(include='object').columns:\n",
    "    if column == 'anes_procedure_encounter_id_2273' or column == 'unique_pt_id':\n",
    "        continue\n",
    "    most_common = df[column].mode()[0]  # Find the most common entry\n",
    "    df[column] = df[column].apply(lambda x: f'#{x}' if x == most_common else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns to make 'failed_catheter' the first column\n",
    "cols = ['failed_catheter'] + [col for col in df.columns if col != 'failed_catheter']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only neuraxial catheter (ie, epidural + CSE + intrathecal) or epidural-only catheter procedures\n",
    "neuraxial_catheter_df = df[df['is_neuraxial_catheter'] == 1].drop('is_neuraxial_catheter',axis=1)\n",
    "epidural_df = df[(df['true_procedure_type_incl_dpe'] == '#epidural') | (df['true_procedure_type_incl_dpe'] == 'dpe')].drop('is_neuraxial_catheter',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume neuraxial_catheter_df is already defined.\n",
    "# For example:\n",
    "# neuraxial_catheter_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Identify categorical columns (assuming columns with dtype 'object' or 'category')\n",
    "categorical_cols = neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Create dummy variables for all identified categorical columns\n",
    "neuraxial_catheter_dummies = pd.get_dummies(neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1), columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Compute the correlation matrix using Pearson correlation by default\n",
    "correlation_matrix = neuraxial_catheter_dummies.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for neuraxial_catheter_df (with dummies)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete boring highly colinear columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_colinear_columns = ['prior_all_catheters_all_enc', # correlated with prior failed catheters\n",
    "                             'bmi_before_pregnancy_2161', # correlated with BMI end pregnancy\n",
    "                             ]\n",
    "\n",
    "neuraxial_catheter_df = neuraxial_catheter_df.drop(list_of_colinear_columns, axis=1,errors='ignore')\n",
    "epidural_df = epidural_df.drop(list_of_colinear_columns, axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume neuraxial_catheter_df is already defined.\n",
    "# For example:\n",
    "# neuraxial_catheter_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Identify categorical columns (assuming columns with dtype 'object' or 'category')\n",
    "categorical_cols = neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Create dummy variables for all identified categorical columns\n",
    "neuraxial_catheter_dummies = pd.get_dummies(neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1), columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Compute the correlation matrix using Pearson correlation by default\n",
    "correlation_matrix = neuraxial_catheter_dummies.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for neuraxial_catheter_df (with dummies)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe Dataframe\n",
    "\n",
    "There are 134997 total rows, of which a fraction have NaN true_procedure_type.\n",
    "\n",
    "Every row receives a value for all Boolean variables: thus if no value is present, they become False. Furthermore, NaN procedures become False is_neuraxial_catheter and failed_catheter.\n",
    "\n",
    "is_neuraxial_catheter includes epidurals + CSEs + intrathecals\n",
    "\n",
    "failed_catheter is applied to BOTH neuraxial_catheters (which may be coded True or False for failure) and also to all procedures that are not neuraxial_catheters (will always be coded False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuraxial_catheter_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataframe(df):\n",
    "    \"\"\"\n",
    "    For each column in df:\n",
    "      - If dtype is object or int64 or bool, list each unique value and its counts.\n",
    "      - If dtype is float64, display min, Q1, median, Q3, and max.\n",
    "      - Otherwise, handle accordingly (datetime, etc.).\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\"  Data Type: {col_type}\")\n",
    "\n",
    "        if col == \"anes_procedure_encounter_id_2273\" or col == \"unique_pt_id\":\n",
    "            print(f\"  Number unique: {len(df[col].unique())}\")\n",
    "\n",
    "        elif col_type == 'object' or col_type == 'int64' or col_type == 'bool':\n",
    "            # Show unique values and their counts\n",
    "            value_counts = df[col].value_counts(dropna=False)\n",
    "            print(\"  Value counts:\")\n",
    "            for val, count in value_counts.items():\n",
    "                print(f\"    {val}: {count}\")\n",
    "\n",
    "        elif col_type == 'float64':\n",
    "            # Show min, Q1 (25%), median (50%), Q3 (75%), and max\n",
    "            desc = df[col].describe(percentiles=[0.25, 0.5, 0.75])\n",
    "            na_count = df[col].isna().sum()\n",
    "            print(\"  Summary stats:\")\n",
    "            print(f\"    NaN:    {na_count}\")\n",
    "            print(f\"    Min:    {desc['min']}\")\n",
    "            print(f\"    Q1:     {desc['25%']}\")\n",
    "            print(f\"    Median: {desc['50%']}\")\n",
    "            print(f\"    Q3:     {desc['75%']}\")\n",
    "            print(f\"    Max:    {desc['max']}\")\n",
    "\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            # Example handling for datetime columns\n",
    "            print(\"  (Datetime column – no numeric summary or value counts shown.)\")\n",
    "\n",
    "        else:\n",
    "            # Handle any other data types as needed\n",
    "            print(\"  (No specific handling implemented for this data type.)\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "describe_dataframe(neuraxial_catheter_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe as tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_as_tables(df):\n",
    "    # Separate columns by dtype\n",
    "    categorical_cols = []\n",
    "    numeric_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col == \"anes_procedure_encounter_id_2273\" or col == \"unique_pt_id\":\n",
    "            pass\n",
    "        elif df[col].dtype == 'object' or df[col].dtype == 'int64' or df[col].dtype == 'bool':\n",
    "            categorical_cols.append(col)\n",
    "        elif df[col].dtype == 'float64':\n",
    "            numeric_cols.append(col)\n",
    "        else:\n",
    "            # skip or handle datetime, etc. if desired\n",
    "            pass\n",
    "\n",
    "    # --- Build table for categorical variables ---\n",
    "    cat_data = {}\n",
    "    for col in categorical_cols:\n",
    "        # Get value counts (including NaN as a separate category)\n",
    "        vc = df[col].value_counts(dropna=False)\n",
    "        # Convert value counts to a dict, or a formatted string\n",
    "        vc_str = \", \".join(f\"{val}: {count}\" for val, count in vc.items())\n",
    "        cat_data[col] = {\n",
    "            'value_counts': vc_str\n",
    "        }\n",
    "    cat_df = pd.DataFrame(cat_data).T  # Transpose so rows = columns, col = 'value_counts'\n",
    "\n",
    "    # --- Build table for numeric variables ---\n",
    "    num_data = {}\n",
    "    for col in numeric_cols:\n",
    "        desc = df[col].describe(percentiles=[0.25, 0.5, 0.75])\n",
    "        na_count = df[col].isna().sum()\n",
    "        num_data[col] = {\n",
    "            'count': desc['count'],\n",
    "            'count_nan': na_count,\n",
    "            'min': desc['min'],\n",
    "            'Q1': desc['25%'],\n",
    "            'median': desc['50%'],\n",
    "            'Q3': desc['75%'],\n",
    "            'max': desc['max']\n",
    "        }\n",
    "    num_df = pd.DataFrame(num_data).T  # Transpose so rows = columns\n",
    "\n",
    "    return cat_df, num_df\n",
    "\n",
    "cat_table, num_table = describe_as_tables(neuraxial_catheter_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_value_counts_str(value_counts_str):\n",
    "    \"\"\"\n",
    "    Convert a string like:\n",
    "       \"bwh: 44730, mgh: 26549, nwh: 22476, slm: 5680...\"\n",
    "    into a dict, e.g.:\n",
    "       {\"bwh\": 44730, \"mgh\": 26549, \"nwh\": 22476, \"slm\": 5680}\n",
    "    It will ignore trailing '...' and attempt to parse each value as float.\n",
    "    \"\"\"\n",
    "    # Strip and remove trailing ellipsis (if present)\n",
    "    value_counts_str = value_counts_str.strip()\n",
    "    if value_counts_str.endswith(\"...\"):\n",
    "        value_counts_str = value_counts_str[:-3].strip()\n",
    "    \n",
    "    out_dict = {}\n",
    "    # Split by commas\n",
    "    items = [s.strip() for s in value_counts_str.split(\",\") if s.strip()]\n",
    "    for item in items:\n",
    "        # Split on the first colon only\n",
    "        parts = item.split(\":\", 1)\n",
    "        if len(parts) != 2:\n",
    "            # If we can't split into exactly \"key: value\", skip\n",
    "            continue\n",
    "        key = parts[0].strip()\n",
    "        val_str = parts[1].strip()\n",
    "        # Attempt to parse numeric value\n",
    "        try:\n",
    "            val = float(val_str)\n",
    "        except ValueError:\n",
    "            # If not parseable, store NaN or skip\n",
    "            val = float(\"nan\")\n",
    "        out_dict[key] = val\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def create_table_one(cat_table, num_table):\n",
    "    \"\"\"\n",
    "    cat_table: \n",
    "        index = categorical variable names\n",
    "        column \"value_counts\" = string describing categories & counts (to be parsed)\n",
    "    num_table:\n",
    "        index = numeric variable names\n",
    "        columns include: [\"count\", \"count_nan\", \"min\", \"Q1\", \"median\", \"Q3\", \"max\", ...]\n",
    "    \"\"\"\n",
    "    table_rows = []\n",
    "\n",
    "    # 1) Numeric variables: median [Q1 - Q3]\n",
    "    for var_name in num_table.index:\n",
    "        median_val = num_table.loc[var_name, \"median\"]\n",
    "        q1 = num_table.loc[var_name, \"Q1\"]\n",
    "        q3 = num_table.loc[var_name, \"Q3\"]\n",
    "\n",
    "        summary_str = f\"{median_val:.2f} [{q1:.2f} - {q3:.2f}]\"\n",
    "        table_rows.append([var_name, summary_str])\n",
    "\n",
    "    # 2) Categorical variables\n",
    "    for var_name in cat_table.index:\n",
    "        # 2a) Parse the \"value_counts\" string into a dict\n",
    "        raw_str = cat_table.loc[var_name, \"value_counts\"]\n",
    "        value_counts_dict = parse_value_counts_str(raw_str)\n",
    "\n",
    "        # Compute total (excluding missing if you prefer)\n",
    "        total_n = sum(value_counts_dict.values())\n",
    "\n",
    "        # 2b) Check if binary (i.e., keys == {0,1} after parsing)\n",
    "        keys_set = set(value_counts_dict.keys())\n",
    "        \n",
    "        # Convert keys from string->float->int if needed\n",
    "        # (Because if your raw data had \"1: 106750\", then key might be \"1\" (string), or float(1.0).)\n",
    "        # We can do a quick normalization:\n",
    "        try:\n",
    "            int_keys = {int(float(k)) for k in keys_set}\n",
    "        except:\n",
    "            int_keys = set()  # In case it fails\n",
    "\n",
    "        if int_keys == {0, 1} and len(keys_set) == 2:\n",
    "            # If it's truly binary: single row for the percent of '1'\n",
    "            # (Need to fetch the count for '1' – might be string or float key)\n",
    "            # We'll do a small loop to figure out which key is '1'\n",
    "            n_ones = 0\n",
    "            for k, v in value_counts_dict.items():\n",
    "                try:\n",
    "                    if int(float(k)) == 1:\n",
    "                        n_ones = v\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "            pct_ones = 100.0 * n_ones / total_n if total_n else 0.0\n",
    "            summary_str = f\"{int(n_ones)} ({pct_ones:.2f}%)\"  # cast to int if you prefer\n",
    "            table_rows.append([var_name, summary_str])  # cast to int if you prefer\n",
    "        \n",
    "        else:\n",
    "            # Multi-category: separate row per category\n",
    "            # Sort the keys in some consistent manner\n",
    "            # We'll attempt to sort by the natural ordering of strings\n",
    "            # (Alternatively, sort by numeric if your categories are numeric.)\n",
    "            sorted_keys = sorted(value_counts_dict.keys(), key=str)\n",
    "            \n",
    "            for cat_val in sorted_keys:\n",
    "                n_cat = value_counts_dict[cat_val]\n",
    "                pct_cat = 100.0 * n_cat / total_n if total_n else 0.0\n",
    "                summary_str = f\"{int(n_cat)} ({pct_cat:.2f}%)\"  # cast to int if you prefer\n",
    "                row_label = f\"{var_name} = {cat_val}\"\n",
    "                table_rows.append([row_label, summary_str])\n",
    "\n",
    "    # Build final DataFrame\n",
    "    table_one = pd.DataFrame(table_rows, columns=[\"Variable\", \"Summary\"])\n",
    "    return table_one\n",
    "\n",
    "# Create the table\n",
    "table_one = create_table_one(cat_table, num_table)\n",
    "table_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_cat_table,failures_num_table = describe_as_tables(neuraxial_catheter_df[neuraxial_catheter_df['failed_catheter'] == 1])\n",
    "successes_cat_table,succeses_num_table = describe_as_tables(neuraxial_catheter_df[neuraxial_catheter_df['failed_catheter'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_table_one = create_table_one(failures_cat_table, failures_num_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "successes_table_one = create_table_one(successes_cat_table, succeses_num_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_one_by_failure_status = successes_table_one.merge(failures_table_one, on='Variable', suffixes=('_success', '_failure'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_one_by_failure_status = (\n",
    "    table_one\n",
    "    .merge(failures_table_one, on='Variable', suffixes=('', '_failures'), how='left')\n",
    "    .merge(successes_table_one, on='Variable', suffixes=('', '_successes'), how='left')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_one_by_failure_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: make a histogram of procedure note types using different colors\n",
    "\n",
    "# Assuming 'procedure_type' column exists in your DataFrame 'df'\n",
    "procedure_type_counts = df['true_procedure_type_incl_dpe'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.bar(procedure_type_counts.index, procedure_type_counts.values, color=['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange'])\n",
    "plt.xlabel('Procedure Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Procedure Note Types')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of successes/failures\n",
    "\n",
    "# Group by procedure type and whether it has subsequent anesthesia\n",
    "procedure_counts = pd.crosstab(neuraxial_catheter_df['true_procedure_type_incl_dpe'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Sort the bars in descending order based on the total count of each procedure type\n",
    "procedure_counts = procedure_counts.sort_values(by=0, ascending=False)\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = procedure_counts.plot(kind='bar', stacked=True, figsize=(6\n",
    ", 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "  width = p.get_width()\n",
    "  height = p.get_height()\n",
    "  x, y = p.get_xy()\n",
    "  ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Procedure Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Successful/Failed')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table with the same information\n",
    "print(\"Table of Neuraxial Catheter Procedures by Success/Failure:\")\n",
    "print(procedure_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anesthesiologist Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Create a similar histogram for failure rate vs highly experienced anesthesiologist\n",
    "\n",
    "# Group by 'highly_experienced_anesthesiologist' and 'failed_catheter'\n",
    "experience_failure_counts = pd.crosstab(neuraxial_catheter_df['highly_experienced_anesthesiologist'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = experience_failure_counts.plot(kind='bar', stacked=True, figsize=(6, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Anesthesiologist Experience')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Anesthesiologist Experience')\n",
    "plt.xticks(rotation=0, ha='center', ticks=[0,1,2], labels=['No Anesthesiologist','Not Highly Experienced', 'Highly Experienced'])\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Anesthesiologist Experience:\")\n",
    "experience_failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: create a similar histogram for failure rate vs moderately experienced anesthesiologist\n",
    "\n",
    "# Group by 'moderately_experienced_anesthesiologist' and 'failed_catheter'\n",
    "experience_failure_counts = pd.crosstab(neuraxial_catheter_df['moderately_experienced_anesthesiologist'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = experience_failure_counts.plot(kind='bar', stacked=True, figsize=(6, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Anesthesiologist Experience')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Moderately Experienced Anesthesiologist')\n",
    "plt.xticks(rotation=0, ha='center', ticks=[0,1,2], labels=['No Anesthesiologist','Not Moderately Experienced', 'Moderately Experienced'])\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Moderately Experienced Anesthesiologist:\")\n",
    "experience_failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Create a similar histogram for failure rate vs highly experienced resident\n",
    "\n",
    "# Group by 'highly_experienced_resident' and 'failed_catheter'\n",
    "experience_failure_counts = pd.crosstab(neuraxial_catheter_df['highly_experienced_resident'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = experience_failure_counts.plot(kind='bar', stacked=True, figsize=(6, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Resident Experience')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Resident Experience')\n",
    "plt.xticks(rotation=0, ha='center', ticks=[0,1,2], labels=['No Resident','Not Highly Experienced', 'Highly Experienced'])\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Resident Experience:\")\n",
    "experience_failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Create a similar histogram but look at all combinations of resident and anesthesiologist experience. Make the x-axis labels vertical.\n",
    "\n",
    "# Group by 'highly_experienced_anesthesiologist', 'highly_experienced_resident', and 'failed_catheter'\n",
    "experience_failure_counts = pd.crosstab([neuraxial_catheter_df['highly_experienced_anesthesiologist'], neuraxial_catheter_df['highly_experienced_resident']], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = experience_failure_counts.plot(kind='bar', stacked=True, figsize=(8, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Anesthesiologist and Resident Experience')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Anesthesiologist and Resident Experience')\n",
    "\n",
    "\n",
    "# Customize x-axis labels\n",
    "import itertools\n",
    "anesth_levels = [\"Anes=None\", \"Anes=Not Exp\", \"Anes=Exp\"]\n",
    "resident_levels = [\"Res=None\", \"Res=Not Exp\", \"Res=Exp\"]\n",
    "labels = list(itertools.product(anesth_levels, resident_levels))\n",
    "plt.xticks(rotation=90, ha='center', ticks=range(len(labels)), labels=labels)\n",
    "\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Anesthesiologist and Resident Experience:\")\n",
    "experience_failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: crosstab resident experience by BMI and make violin plots\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'df' is your DataFrame (as defined in the provided code)\n",
    "# and it contains columns 'bmi_end_pregnancy_2044' and 'resident_experience' (or a similar column)\n",
    "\n",
    "# Create the cross-tabulation\n",
    "crosstab_data = pd.crosstab(neuraxial_catheter_df['bmi_end_pregnancy_2044'], neuraxial_catheter_df['highly_experienced_resident'])\n",
    "\n",
    "# Display the cross-tabulation\n",
    "print(\"Crosstab of Resident Experience by BMI:\")\n",
    "print(crosstab_data)\n",
    "\n",
    "# Create violin plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='highly_experienced_resident', y='bmi_end_pregnancy_2044', data=df)\n",
    "plt.xlabel('Resident Experience')  # Customize the x-axis label\n",
    "plt.ylabel('BMI') # Customize the y-axis label\n",
    "plt.title('Violin Plot of BMI by Resident Experience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivery Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: create a similar histogram of delivery_site using crosstab\n",
    "\n",
    "# Create a crosstab for 'delivery_site' and visualize it as a histogram\n",
    "delivery_site_counts = pd.crosstab(neuraxial_catheter_df['delivery_site'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Sort the bars in descending order based on the total count of each delivery site\n",
    "delivery_site_counts = delivery_site_counts.sort_values(by=0, ascending=False)\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = delivery_site_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Delivery Site')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Delivery Site by Success/Failure')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Delivery Site by Success/Failure:\")\n",
    "delivery_site_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: create a pie chart of the fraction of DPE in epidural_df\n",
    "\n",
    "# Count DPE values, treating NaN and '' as \"no\"\n",
    "dpe_counts = epidural_df['true_procedure_type_incl_dpe'].value_counts()\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(dpe_counts, labels=dpe_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Fraction of DPE in Epidural Procedures')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: reproduce the above histogram using crosstab on delivery_site and dpe\n",
    "\n",
    "# Assuming 'df' is your DataFrame (as defined in the provided code)\n",
    "\n",
    "# Create a crosstab for 'delivery_site' and 'dpe' and visualize it as a histogram\n",
    "delivery_site_dpe_counts = pd.crosstab(epidural_df['delivery_site'], (epidural_df['true_procedure_type_incl_dpe'] == 'dpe').astype(int))\n",
    "\n",
    "# Sort the bars in descending order based on the total count of each delivery site\n",
    "delivery_site_dpe_counts = delivery_site_dpe_counts.sort_values(by=1, ascending=False) # Sort by 'no'\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = delivery_site_dpe_counts.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Delivery Site')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Delivery Site by DPE')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['DPE: no', 'DPE: yes']) # Update legend labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Delivery Site by DPE:\")\n",
    "delivery_site_dpe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of successes/failures by DPE status\n",
    "\n",
    "# Group by procedure type and whether it has subsequent anesthesia\n",
    "dpe_crosstab = pd.crosstab(epidural_df['true_procedure_type_incl_dpe'] == 'dpe', epidural_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = dpe_crosstab.plot(kind='bar', stacked=True, figsize=(6\n",
    ", 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "  width = p.get_width()\n",
    "  height = p.get_height()\n",
    "  x, y = p.get_xy()\n",
    "  ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('DPE Status')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Successful/Failed')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do a crosstab histogram of failure versus delivery_site and dpe\n",
    "\n",
    "# Assuming 'df' is your DataFrame (as defined in the provided code)\n",
    "\n",
    "# Create a crosstab for 'delivery_site', 'dpe', and 'failed_catheter'\n",
    "crosstab_df = pd.crosstab([neuraxial_catheter_df['delivery_site'], neuraxial_catheter_df['true_procedure_type_incl_dpe'] == 'dpe'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = crosstab_df.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "\n",
    "# Annotate the bars with percentages\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "\n",
    "plt.xlabel('Delivery Site and DPE')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Crosstab Histogram: Failure vs. Delivery Site and DPE')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the crosstab table\n",
    "print(\"Crosstab Table:\")\n",
    "crosstab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoliosis and back problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: create a histogram of the crosstab of has_scoliosis vs failure_rate\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame (as defined in the provided code)\n",
    "\n",
    "# Group by 'has_scoliosis' and 'failed_catheter'\n",
    "scoliosis_failure_counts = pd.crosstab(neuraxial_catheter_df['has_scoliosis'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = scoliosis_failure_counts.plot(kind='bar', stacked=True, figsize=(6, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Has Scoliosis')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Scoliosis')\n",
    "plt.xticks(rotation=0, ha='center', ticks=[0, 1], labels=['No Scoliosis', 'Scoliosis'])\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Scoliosis:\")\n",
    "scoliosis_failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same but for has_back_problems\n",
    "\n",
    "# Group by 'has_back_problems' and 'failed_catheter'\n",
    "back_problems_failure_counts = pd.crosstab(neuraxial_catheter_df['has_back_problems'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = back_problems_failure_counts.plot(kind='bar', stacked=True, figsize=(6, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Has Back Problems')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Back Problems')\n",
    "plt.xticks(rotation=0, ha='center', ticks=[0, 1], labels=['No Back Problems', 'Back Problems'])\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Back Problems:\")\n",
    "back_problems_failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same but for has_dorsalgia\n",
    "\n",
    "# Group by 'has_dorsalgia' and 'failed_catheter'\n",
    "back_pain_failure_counts = pd.crosstab(neuraxial_catheter_df['has_dorsalgia'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = back_pain_failure_counts.plot(kind='bar', stacked=True, figsize=(6, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Has Back Pain')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Back Pain')\n",
    "plt.xticks(rotation=0, ha='center', ticks=[0, 1], labels=['No Back Pain', 'Back Pain'])\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Back Pain:\")\n",
    "back_pain_failure_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetal Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram, but for fetal_presentation_category vs failure\n",
    "\n",
    "# Group by 'fetal_presentation_category_2243' and 'failed_catheter'\n",
    "fetal_presentation_failure_counts = pd.crosstab(neuraxial_catheter_df['fetal_presentation_category_2243'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = fetal_presentation_failure_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Fetal Presentation Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Fetal Presentation Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Fetal Presentation Category:\")\n",
    "fetal_presentation_failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram, but for fetal_presentation_position vs failure\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame (as defined in the provided code)\n",
    "\n",
    "# Group by 'fetal_presentation_position_2247' and 'failed_catheter'\n",
    "fetal_position_failure_counts = pd.crosstab(neuraxial_catheter_df['fetal_presentation_position_2247'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = fetal_position_failure_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Fetal Presentation Position')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Fetal Presentation Position')\n",
    "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Fetal Presentation Position:\")\n",
    "fetal_position_failure_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race and SES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram, but for maternal_race vs failure\n",
    "\n",
    "# Group by 'maternal_race' and 'failed_catheter'\n",
    "race_failure_counts = pd.crosstab(neuraxial_catheter_df['maternal_race'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = race_failure_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Maternal Race')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Maternal Race')\n",
    "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Maternal Race:\")\n",
    "race_failure_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram, but for each of these:\n",
    "# 32. composite_psychosocial_problems ||| int64\n",
    "# 33. only_private_insurance ||| int64\n",
    "# 34. maternal_language_english ||| int64\n",
    "# 35. marital_status_married_or_partner ||| int64\n",
    "# 36. country_of_origin_USA ||| int64\n",
    "# 37. employment_status_fulltime ||| int64\n",
    "# 38. composite_SES_advantage ||| int64\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame\n",
    "\n",
    "columns_to_analyze = [\n",
    "    'composite_psychosocial_problems',\n",
    "    'only_private_insurance',\n",
    "    'maternal_language_english',\n",
    "    'marital_status_married_or_partner',\n",
    "    'country_of_origin_USA',\n",
    "    'employment_status_fulltime',\n",
    "    'composite_SES_advantage'\n",
    "]\n",
    "\n",
    "for column in columns_to_analyze:\n",
    "  # Group by the current column and 'failed_catheter'\n",
    "  failure_counts = pd.crosstab(neuraxial_catheter_df[column], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "  # Create a stacked bar chart\n",
    "  ax = failure_counts.plot(kind='bar', stacked=True, figsize=(6, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "  # Add percentages within each bar\n",
    "  for p in ax.patches:\n",
    "      width = p.get_width()\n",
    "      height = p.get_height()\n",
    "      x, y = p.get_xy()\n",
    "      ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "  plt.xlabel(column)\n",
    "  plt.ylabel('Count')\n",
    "  plt.title(f'Histogram of Failure Rate vs. {column}')\n",
    "\n",
    "  # Customize x-axis ticks and labels (adjust as needed for each column)\n",
    "  plt.xticks(rotation=0, ha='center')\n",
    "\n",
    "  plt.legend(['Successful', 'Failed'])\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  # Display the table with the same information\n",
    "  print(f\"Table of Failure Rate vs. {column}:\")\n",
    "failure_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram but for prior_pain_scores_max\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame\n",
    "\n",
    "# Group by 'prior_pain_scores_max' and 'failed_catheter'\n",
    "prior_pain_failure_counts = pd.crosstab(neuraxial_catheter_df['prior_pain_scores_max'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = prior_pain_failure_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Prior Pain Scores Max')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Prior Pain Scores Max')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels if needed\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Prior Pain Scores Max:\")\n",
    "prior_pain_failure_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gravidity and Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram but for gravidity_2047 and parity_2048\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame\n",
    "\n",
    "# Group by 'gravidity_2047' and 'failed_catheter'\n",
    "gravidity_failure_counts = pd.crosstab(neuraxial_catheter_df['gravidity_2047'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = gravidity_failure_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Gravidity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Gravidity')\n",
    "plt.xticks(rotation=0)  # Adjust rotation if needed\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table\n",
    "print(\"Table of Failure Rate vs. Gravidity:\")\n",
    "print(gravidity_failure_counts)\n",
    "\n",
    "\n",
    "# Group by 'parity_2048' and 'failed_catheter'\n",
    "parity_failure_counts = pd.crosstab(neuraxial_catheter_df['parity_2048'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = parity_failure_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Parity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Parity')\n",
    "plt.xticks(rotation=0)  # Adjust rotation if needed\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table\n",
    "print(\"Table of Failure Rate vs. Parity:\")\n",
    "parity_failure_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maternal Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'maternal_age_years' and 'failed_catheter' are columns in your DataFrame 'df'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['maternal_age_years', 'failed_catheter'])\n",
    "\n",
    "# Bin the maternal_age_years\n",
    "df_plot['maternal_age_bin'] = (df_plot['maternal_age_years'] // 1).astype(int)\n",
    "\n",
    "# Group by the binned maternal_age_years and calculate the mean and standard error of the mean of failed_catheter\n",
    "failure_by_age = df_plot.groupby('maternal_age_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_age.index, failure_by_age['mean'], marker='o')\n",
    "plt.fill_between(failure_by_age.index,\n",
    "                 failure_by_age['mean'] - failure_by_age['sem'],\n",
    "                 failure_by_age['mean'] + failure_by_age['sem'],\n",
    "                 alpha=0.5) # Add shaded error bars\n",
    "\n",
    "plt.xlabel('Maternal Age (years, binned by 1)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Maternal Age (binned by 1) with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMI / height / weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: plot bmi end pregnancy against failure rate using binning as above.\n",
    "\n",
    "# Assuming 'bmi_end_pregnancy' and 'failed_catheter' are columns in your DataFrame 'df'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['bmi_end_pregnancy_2044', 'failed_catheter'])\n",
    "\n",
    "# Bin the bmi_end_pregnancy\n",
    "df_plot['bmi_end_pregnancy_bin'] = (df_plot['bmi_end_pregnancy_2044'] // 1).astype(int)\n",
    "\n",
    "# Group by the binned bmi_end_pregnancy and calculate the mean and standard error of the mean of failed_catheter\n",
    "failure_by_bmi = df_plot.groupby('bmi_end_pregnancy_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_bmi.index, failure_by_bmi['mean'], marker='o')\n",
    "plt.fill_between(failure_by_bmi.index,\n",
    "                 failure_by_bmi['mean'] - failure_by_bmi['sem'],\n",
    "                 failure_by_bmi['mean'] + failure_by_bmi['sem'],\n",
    "                 alpha=0.5) # Add shaded error bars\n",
    "\n",
    "plt.xlabel('BMI (kg/m^2) at End of Pregnancy (binned by 1)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. BMI at End of Pregnancy (binned by 1) with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: # prompt: plot weight end pregnancy against failure rate using binning as above.\n",
    "\n",
    "# Assuming 'maternal_weight_end_pregnancy_2045' and 'failed_catheter' are columns in your DataFrame 'df'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['maternal_weight_end_pregnancy_2045', 'failed_catheter'])\n",
    "\n",
    "# Bin the maternal weight at the end of pregnancy\n",
    "df_plot['weight_end_pregnancy_bin'] = (df_plot['maternal_weight_end_pregnancy_2045'] // 10).astype(int) * 10\n",
    "\n",
    "# Group by the binned weight and calculate the mean and standard error of the mean of failed_catheter\n",
    "failure_by_weight = df_plot.groupby('weight_end_pregnancy_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_weight.index, failure_by_weight['mean'], marker='o')\n",
    "plt.fill_between(failure_by_weight.index,\n",
    "                 failure_by_weight['mean'] - failure_by_weight['sem'],\n",
    "                 failure_by_weight['mean'] + failure_by_weight['sem'],\n",
    "                 alpha=0.5)  # Add shaded error bars\n",
    "\n",
    "plt.xlabel('Maternal Weight (kg) at End of Pregnancy (binned by 10)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Maternal Weight at End of Pregnancy (binned by 10) with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt: do the same but for height\n",
    "\n",
    "# # Assuming 'height' is a column in your DataFrame 'df'\n",
    "# df_plot = neuraxial_catheter_df.dropna(subset=['maternal_height_2046', 'failed_catheter'])\n",
    "\n",
    "# # Drop heights greater than 250\n",
    "# df_plot = df_plot[df_plot['maternal_height_2046'] <= 250]\n",
    "\n",
    "# # Bin the height\n",
    "# df_plot['height_bin'] = (df_plot['maternal_height_2046'] // 1).astype(int)\n",
    "\n",
    "# # Group by the binned height and calculate the mean and standard error of the mean of failed_catheter\n",
    "# failure_by_height = df_plot.groupby('height_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# # Create the plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(failure_by_height.index, failure_by_height['mean'], marker='o')\n",
    "# plt.fill_between(failure_by_height.index,\n",
    "#                  failure_by_height['mean'] - failure_by_height['sem'],\n",
    "#                  failure_by_height['mean'] + failure_by_height['sem'],\n",
    "#                  alpha=0.5) # Add shaded error bars\n",
    "\n",
    "# plt.xlabel('Height (binned by 1)')\n",
    "# plt.ylabel('Average Failure Rate')\n",
    "# plt.title('Failure Rate vs. Height (binned by 1) with Error Bars')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needle Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram but for epidural_needle_type\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame (as defined in the provided code)\n",
    "\n",
    "# Group by 'epidural_needle_type' and 'failed_catheter'\n",
    "needle_type_failure_counts = pd.crosstab(neuraxial_catheter_df['epidural_needle_type'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = needle_type_failure_counts.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Epidural Needle Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Epidural Needle Type')\n",
    "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Epidural Needle Type:\")\n",
    "needle_type_failure_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paresthesias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram but for paresthesias_present\n",
    "\n",
    "# Group by 'paresthesias_present' and 'failed_catheter'\n",
    "paresthesias_failure_counts = pd.crosstab(neuraxial_catheter_df['paresthesias_present'], neuraxial_catheter_df['failed_catheter'])\n",
    "\n",
    "# Create a stacked bar chart\n",
    "ax = paresthesias_failure_counts.plot(kind='bar', stacked=True, figsize=(6, 6), color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add percentages within each bar\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height/sum([p.get_height() for p in ax.patches if p.get_x() == x]) * 100:.1f}%', (x + width/2, y + height/2), ha='center', va='center')\n",
    "\n",
    "plt.xlabel('Paresthesias Present')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Failure Rate vs. Paresthesias Present')\n",
    "plt.xticks(rotation=0, ha='center', ticks=[0, 1], labels=['No Paresthesias', 'Paresthesias'])\n",
    "plt.legend(['Successful', 'Failed'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the table with the same information\n",
    "print(\"Table of Failure Rate vs. Paresthesias Present:\")\n",
    "paresthesias_failure_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: create a histogram of the number of attempts. Only show integers on the x-axis\n",
    "\n",
    "# Assuming 'number_of_neuraxial_attempts' is a column in your DataFrame 'df'\n",
    "attempts_counts = df['number_of_neuraxial_attempts'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(attempts_counts.index, attempts_counts.values)\n",
    "plt.xlabel('Number of Attempts')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Number of Neuraxial Attempts')\n",
    "plt.xticks(range(int(attempts_counts.index.min()), int(attempts_counts.index.max()) + 1))  # Show only integer ticks on x-axis\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss of Resistance Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: create a histogram of loss of resistance depth. Center the bars over the tick marks and make space between the bars. Bins should be every 0.5\n",
    "\n",
    "# Assuming 'lor_depth' is a column in your DataFrame 'df'\n",
    "lor_depths = neuraxial_catheter_df['lor_depth'].dropna()  # Remove NaN values\n",
    "\n",
    "# Create the histogram with centered bars and spacing\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(lor_depths, bins=np.arange(lor_depths.min(), lor_depths.max() + 0.5, 0.5), rwidth=0.8, align='left')\n",
    "plt.xlabel('Loss of Resistance Depth')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Loss of Resistance Depth')\n",
    "plt.xticks(np.arange(0, lor_depths.max() + 0.5, 1))  # Set x-axis ticks to be at every 1\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Plot number of neuraxial attempts vs LOR depth on the x-axis. Add jiggle to both x and y axes\n",
    "\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'lor_depth'])\n",
    "\n",
    "# Add random jiggle to both x and y axes\n",
    "jiggle_x = np.random.normal(scale = 0.1, size=len(df_plot))\n",
    "jiggle_y = np.random.normal(scale = 0.1, size=len(df_plot))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_plot['lor_depth'] + jiggle_x, df_plot['number_of_neuraxial_attempts'] + jiggle_y, alpha=0.5)\n",
    "plt.xlabel('LOR Depth')\n",
    "plt.ylabel('Number of Neuraxial Attempts')\n",
    "plt.title('Number of Neuraxial Attempts vs. LOR Depth with Jiggle')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the data, dropping NaNs\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['lor_depth', 'number_of_neuraxial_attempts'])\n",
    "\n",
    "# Create a list of unique values in 'number_of_neuraxial_attempts'\n",
    "attempts = [1, 2, 3, 4]\n",
    "\n",
    "# Create histograms for each number_of_neuraxial_attempts\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, attempt in enumerate(attempts, start=1):\n",
    "    # Filter data for each attempt\n",
    "    subset = df_plot[df_plot['number_of_neuraxial_attempts'] == attempt]\n",
    "    \n",
    "    # Plot histogram for 'lor_depth'\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.hist(subset['lor_depth'], bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Histogram of LOR Depth for {attempt} Neuraxial Attempt(s)')\n",
    "    plt.xlabel('LOR Depth')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same but add shaded error bars for +/- standard error of the mean\n",
    "\n",
    "# Assuming 'number_of_neuraxial_attempts' and 'failed_catheter' are columns in your DataFrame 'df'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts'])\n",
    "\n",
    "# Group by number of attempts and calculate the mean and standard error of the mean of failed_catheter\n",
    "failure_by_attempts = df_plot.groupby('number_of_neuraxial_attempts')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_attempts.index, failure_by_attempts['mean'], marker='o')\n",
    "plt.fill_between(failure_by_attempts.index,\n",
    "                 failure_by_attempts['mean'] - failure_by_attempts['sem'],\n",
    "                 failure_by_attempts['mean'] + failure_by_attempts['sem'],\n",
    "                 alpha=0.2) # Add shaded error bars\n",
    "plt.errorbar(failure_by_attempts.index, failure_by_attempts['mean'], yerr=failure_by_attempts['sem'], fmt='o-', capsize=5, elinewidth=1)  # Added error bars\n",
    "plt.xlabel('Number of Neuraxial Attempts')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Number of Neuraxial Attempts with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Plot lor-depth against bmi\n",
    "\n",
    "# Assuming 'lor_depth' and 'bmi_end_pregnancy_2044' are columns in your DataFrame 'df'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['lor_depth', 'bmi_end_pregnancy_2044'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_plot['bmi_end_pregnancy_2044'], df_plot['lor_depth'])\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('LOR Depth')\n",
    "plt.title('LOR Depth vs. BMI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import gaussian_kde\n",
    "\n",
    "# # Extract the data, dropping NaNs\n",
    "# df_plot = neuraxial_catheter_df.dropna(subset=['lor_depth', 'bmi_end_pregnancy_2044'])\n",
    "# x = df_plot['bmi_end_pregnancy_2044'].values\n",
    "# y = df_plot['lor_depth'].values\n",
    "\n",
    "# # Perform kernel density estimation\n",
    "# xy = np.vstack([x, y])\n",
    "# kde = gaussian_kde(xy)\n",
    "\n",
    "# # Define grid over data range\n",
    "# xmin, xmax = x.min() - 1, x.max() + 1\n",
    "# ymin, ymax = y.min() - 1, y.max() + 1\n",
    "# X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "# positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "# Z = np.reshape(kde(positions).T, X.shape)\n",
    "\n",
    "# # Create the contour plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.contourf(X, Y, Z, levels=15, cmap='viridis')\n",
    "# plt.colorbar(label='Density')\n",
    "# plt.xlabel('BMI')\n",
    "# plt.ylabel('LOR Depth')\n",
    "# plt.title('Contour Plot of LOR Depth vs. BMI (KDE)')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same but for failure vs loss of resistance depth. Bin the depth by units of 1\n",
    "\n",
    "# Assuming 'lor_depth' and 'failed_catheter' are columns in your DataFrame 'df'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['lor_depth', 'failed_catheter'])\n",
    "\n",
    "# Bin the LOR depth\n",
    "df_plot['lor_depth_bin'] = (df_plot['lor_depth'] // 1).astype(int)\n",
    "\n",
    "# Group by the binned LOR depth and calculate the mean of failed_catheter\n",
    "failure_by_lor_depth = df_plot.groupby('lor_depth_bin')['failed_catheter'].mean()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_lor_depth.index, failure_by_lor_depth.values, marker='o')\n",
    "plt.xlabel('Loss of Resistance Depth (binned)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Loss of Resistance Depth (binned by 1)')\n",
    "plt.xticks(np.arange(0, df_plot['lor_depth'].max() + 0.5, 1))  # Set x-axis ticks to be at every 1\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Reproduce the same plot, but add shaded error bars for +/- standard error of the mean\n",
    "\n",
    "# Assuming 'lor_depth' and 'failed_catheter' are columns in your DataFrame 'df'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['lor_depth', 'failed_catheter'])\n",
    "\n",
    "# Bin the LOR depth\n",
    "df_plot['lor_depth_bin'] = (df_plot['lor_depth'] // 1).astype(int)\n",
    "\n",
    "# Group by the binned LOR depth and calculate the mean and standard error of the mean of failed_catheter\n",
    "failure_by_lor_depth = df_plot.groupby('lor_depth_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_lor_depth.index, failure_by_lor_depth['mean'], marker='o')\n",
    "plt.fill_between(failure_by_lor_depth.index,\n",
    "                 failure_by_lor_depth['mean'] - failure_by_lor_depth['sem'],\n",
    "                 failure_by_lor_depth['mean'] + failure_by_lor_depth['sem'],\n",
    "                 alpha=0.5) # Add shaded error bars\n",
    "\n",
    "plt.xlabel('Loss of Resistance Depth (binned)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Loss of Resistance Depth (binned by 1) with Error Bars')\n",
    "plt.xticks(np.arange(0, df_plot['lor_depth'].max() + 0.5, 1))  # Set x-axis ticks to be at every 1\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with NaNs\n",
    "df_plot = neuraxial_catheter_df.dropna(\n",
    "    subset=['lor_depth', 'bmi_end_pregnancy_2044', 'failed_catheter']\n",
    ")\n",
    "\n",
    "# Separate the data by failed_catheter category\n",
    "df_0 = df_plot[df_plot['failed_catheter'] == 0]\n",
    "df_1 = df_plot[df_plot['failed_catheter'] == 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot for failed_catheter = 0\n",
    "plt.scatter(\n",
    "    df_0['bmi_end_pregnancy_2044'],\n",
    "    df_0['lor_depth'],\n",
    "    s=10, \n",
    "    alpha=0.7,\n",
    "    color='blue',\n",
    "    label='Failed Catheter = 0'\n",
    ")\n",
    "\n",
    "# Scatter plot for failed_catheter = 1\n",
    "plt.scatter(\n",
    "    df_1['bmi_end_pregnancy_2044'],\n",
    "    df_1['lor_depth'],\n",
    "    s=10, \n",
    "    alpha=0.7,\n",
    "    color='orange',\n",
    "    label='Failed Catheter = 1'\n",
    ")\n",
    "\n",
    "# --- Calculate and plot regression line for failed_catheter = 0 ---\n",
    "p0 = np.polyfit(df_0['bmi_end_pregnancy_2044'], df_0['lor_depth'], deg=1)  # slope, intercept\n",
    "slope_0, intercept_0 = p0\n",
    "print(f\"For failed_catheter=0, slope = {slope_0:.2f}, intercept = {intercept_0:.2f}\")\n",
    "\n",
    "x_vals_0 = np.linspace(df_0['bmi_end_pregnancy_2044'].min(), df_0['bmi_end_pregnancy_2044'].max(), 100)\n",
    "y_vals_0 = np.polyval(p0, x_vals_0)\n",
    "plt.plot(x_vals_0, y_vals_0, color='blue', linewidth=2)\n",
    "\n",
    "# --- Calculate and plot regression line for failed_catheter = 1 ---\n",
    "p1 = np.polyfit(df_1['bmi_end_pregnancy_2044'], df_1['lor_depth'], deg=1)\n",
    "slope_1, intercept_1 = p1\n",
    "print(f\"For failed_catheter=1, slope = {slope_1:.2f}, intercept = {intercept_1:.2f}\")\n",
    "\n",
    "x_vals_1 = np.linspace(df_1['bmi_end_pregnancy_2044'].min(), df_1['bmi_end_pregnancy_2044'].max(), 100)\n",
    "y_vals_1 = np.polyval(p1, x_vals_1)\n",
    "plt.plot(x_vals_1, y_vals_1, color='orange', linewidth=2)\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('LOR Depth')\n",
    "plt.title('LOR Depth vs. BMI')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestational Age and Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same but for gestational age\n",
    "\n",
    "# Histogram of gestational age\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['gestational_age_weeks'].dropna(), bins=20) # Adjust bins as needed\n",
    "plt.xlabel('Gestational Age (days)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Gestational Age')\n",
    "plt.show()\n",
    "\n",
    "# Analyze gestational age in relation to failed catheter\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['gestational_age_weeks', 'failed_catheter'])\n",
    "df_plot['gestational_age_bin'] = (df_plot['gestational_age_weeks'] // 7).astype(int) * 7\n",
    "failure_by_gestational_age = df_plot.groupby('gestational_age_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_gestational_age.index, failure_by_gestational_age['mean'], marker='o')\n",
    "plt.fill_between(failure_by_gestational_age.index,\n",
    "                failure_by_gestational_age['mean'] - failure_by_gestational_age['sem'],\n",
    "                failure_by_gestational_age['mean'] + failure_by_gestational_age['sem'],\n",
    "                alpha=0.5)\n",
    "plt.xlabel('Gestational Age (days) (binned by 7)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Gestational Age (binned by 7) with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same histogram and binned failure rate but for baby_weight_2196\n",
    "\n",
    "# Assuming 'baby_weight_2196' is a column in your DataFrame 'df' or 'neuraxial_catheter_df'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(neuraxial_catheter_df['baby_weight_2196'].dropna(), bins=20)  # Adjust bins as needed\n",
    "plt.xlabel('Baby Weight (kg)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Baby Weight')\n",
    "plt.show()\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame\n",
    "\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['baby_weight_2196', 'failed_catheter'])\n",
    "\n",
    "# Bin the baby weight\n",
    "df_plot['baby_weight_bin'] = (df_plot['baby_weight_2196'] // 0.5) * 0.5\n",
    "\n",
    "# Group by the binned baby weight and calculate the mean and standard error of the mean of failed_catheter\n",
    "failure_by_baby_weight = df_plot.groupby('baby_weight_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_baby_weight.index, failure_by_baby_weight['mean'], marker='o')\n",
    "plt.fill_between(failure_by_baby_weight.index,\n",
    "                 failure_by_baby_weight['mean'] - failure_by_baby_weight['sem'],\n",
    "                 failure_by_baby_weight['mean'] + failure_by_baby_weight['sem'],\n",
    "                 alpha=0.5)  # Add shaded error bars\n",
    "\n",
    "plt.xlabel('Baby Weight (kg) (binned by 0.5)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Baby Weight with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same count histogram but for secs_rom_thru_delivery_2197\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame\n",
    "\n",
    "# Drop NaN values in 'secs_rom_thru_delivery_2197'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['rom_thru_delivery_hours'])\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_plot['rom_thru_delivery_hours'], bins=200)  # Adjust bins as needed\n",
    "plt.xlabel('Hours from ROM to Delivery')\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Hours from ROM to Delivery')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: do the same binned plot for rom_thru_delivery_hours\n",
    "\n",
    "# Assuming 'neuraxial_catheter_df' is your DataFrame\n",
    "\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['rom_thru_delivery_hours', 'failed_catheter'])\n",
    "\n",
    "# Bin the rom_thru_delivery_hours\n",
    "df_plot['rom_thru_delivery_hours_bin'] = (df_plot['rom_thru_delivery_hours'] // 1).astype(int)\n",
    "\n",
    "# Group by the binned rom_thru_delivery_hours and calculate the mean and standard error of the mean of failed_catheter\n",
    "failure_by_rom_delivery = df_plot.groupby('rom_thru_delivery_hours_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_rom_delivery.index, failure_by_rom_delivery['mean'], marker='o')\n",
    "plt.fill_between(failure_by_rom_delivery.index,\n",
    "                 failure_by_rom_delivery['mean'] - failure_by_rom_delivery['sem'],\n",
    "                 failure_by_rom_delivery['mean'] + failure_by_rom_delivery['sem'],\n",
    "                 alpha=0.5)  # Add shaded error bars\n",
    "plt.xlim(0,100)\n",
    "plt.xlabel('Hours from ROM to Delivery (binned)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Hours from ROM to Delivery (binned by 1) with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior failed catheters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'prior_failed_catheters' is a column in your DataFrame 'neuraxial_catheter_df'\n",
    "prior_failed_catheters_counts = neuraxial_catheter_df['prior_failed_catheters_this_enc'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(prior_failed_catheters_counts.index, prior_failed_catheters_counts.values)\n",
    "plt.xlabel('Number of Prior Failed Catheters')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Prior Failed Catheters (This Encounter)')\n",
    "plt.xticks(range(int(prior_failed_catheters_counts.index.min()), int(prior_failed_catheters_counts.index.max()) + 1))  # Show only integer ticks on x-axis\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'prior_failed_catheters' is a column in your DataFrame 'neuraxial_catheter_df'\n",
    "prior_failed_catheters_counts = neuraxial_catheter_df[neuraxial_catheter_df['failed_catheter'] == 1]['prior_failed_catheters_this_enc'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(prior_failed_catheters_counts.index, prior_failed_catheters_counts.values)\n",
    "plt.xlabel('Number of Prior Failed Catheters (This Encounter)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Prior Failed Catheter Among Failed Cases')\n",
    "plt.xticks(range(int(prior_failed_catheters_counts.index.min()), int(prior_failed_catheters_counts.index.max()) + 1))  # Show only integer ticks on x-axis\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with NaN in 'prior_failed_catheters'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['prior_failed_catheters_this_enc'])\n",
    "\n",
    "# Group by 'prior_failed_catheters' and calculate mean + standard error of the mean (sem)\n",
    "failure_by_prior = df_plot.groupby('prior_failed_catheters_this_enc')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_prior.index, failure_by_prior['mean'], marker='o', label='Mean Failure Rate')\n",
    "\n",
    "# Shaded region for +/- SEM\n",
    "plt.fill_between(failure_by_prior.index,\n",
    "                 failure_by_prior['mean'] - failure_by_prior['sem'],\n",
    "                 failure_by_prior['mean'] + failure_by_prior['sem'],\n",
    "                 alpha=0.2, label='+/- SEM')\n",
    "\n",
    "# Error bars for SEM\n",
    "plt.errorbar(failure_by_prior.index,\n",
    "             failure_by_prior['mean'],\n",
    "             yerr=failure_by_prior['sem'],\n",
    "             fmt='o-', capsize=5, elinewidth=1, color='C0') \n",
    "\n",
    "plt.xlabel('Number of Prior Failed Catheters (This Encounter)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Prior Failed Catheters (with SEM)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placement to Delivery Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'placement_to_delivery_hours' and 'failed_catheter' are columns in your DataFrame 'df'\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['placement_to_delivery_hours', 'failed_catheter'])\n",
    "\n",
    "# Bin the placement_to_delivery_hours\n",
    "df_plot['placement_to_delivery_bin'] = (df_plot['placement_to_delivery_hours'] // 1).astype(int)\n",
    "\n",
    "# Group by the binned placement_to_delivery_hours and calculate the mean and standard error of the mean of failed_catheter\n",
    "failure_by_placement_time = df_plot.groupby('placement_to_delivery_bin')['failed_catheter'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(failure_by_placement_time.index, failure_by_placement_time['mean'], marker='o')\n",
    "plt.fill_between(failure_by_placement_time.index,\n",
    "                 failure_by_placement_time['mean'] - failure_by_placement_time['sem'],\n",
    "                 failure_by_placement_time['mean'] + failure_by_placement_time['sem'],\n",
    "                 alpha=0.5) # Add shaded error bars\n",
    "\n",
    "plt.xlabel('Placement to Delivery Time (hours, binned by 1)')\n",
    "plt.ylabel('Average Failure Rate')\n",
    "plt.title('Failure Rate vs. Placement to Delivery Time (binned by 1) with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some individually interesting regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = neuraxial_catheter_df.dropna(subset=['lor_depth', 'number_of_neuraxial_attempts'])\n",
    "\n",
    "# Fit the model using the formula\n",
    "model = smf.ols('number_of_neuraxial_attempts ~ lor_depth', data=df_corr).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical variables like DPE and failed_catheter\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "dpe_crosstab = pd.crosstab((epidural_df['true_procedure_type_incl_dpe'] == 'dpe').astype(int), epidural_df['failed_catheter'])\n",
    "chi2, p, _, _ = chi2_contingency(dpe_crosstab)\n",
    "\n",
    "print(dpe_crosstab.div(dpe_crosstab.sum(axis=1), axis=0) * 100)\n",
    "print(\"Chi-squared statistic:\", chi2)\n",
    "print(\"P-value:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Do univariate logistic regression separately using number of attempts and loss of resistance depth to predict failure\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Prepare the data for logistic regression with number of attempts as the predictor\n",
    "df_logreg_attempts = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'failed_catheter'])\n",
    "# Fit the logistic regression model\n",
    "model_attempts = smf.logit('failed_catheter ~ number_of_neuraxial_attempts', data=df_logreg_attempts).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_attempts.summary())\n",
    "\n",
    "\n",
    "# Prepare the data for logistic regression with loss of resistance depth as the predictor\n",
    "df_logreg_lor = neuraxial_catheter_df.dropna(subset=['lor_depth', 'failed_catheter'])\n",
    "# Fit the logistic regression model\n",
    "model_lor = smf.logit('failed_catheter ~ lor_depth', data=df_logreg_lor).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_lor.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Now do multivariate analysis using the same two predictors\n",
    "\n",
    "# Prepare the data for logistic regression with both predictors\n",
    "df_logreg_multi = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'lor_depth', 'failed_catheter'])\n",
    "\n",
    "# Fit the logistic regression model with both predictors\n",
    "model_multi = smf.logit('failed_catheter ~ number_of_neuraxial_attempts + lor_depth', data=df_logreg_multi).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_multi.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for logistic regression with prior_failed_catheters_this_enc as the predictor\n",
    "df_logreg_prior_failed = neuraxial_catheter_df.dropna(subset=['prior_failed_catheters_this_enc', 'failed_catheter'])\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model_attempts = smf.logit('failed_catheter ~ prior_failed_catheters_this_enc', data=df_logreg_prior_failed).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_attempts.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All univariate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def all_regressions_each_dummy(df, outcome_col='failed_catheter'):\n",
    "    \"\"\"\n",
    "    Fits a univariate logistic regression for each column in df (except outcome_col).\n",
    "    For numeric columns, you get a single slope term.\n",
    "    For categorical columns, you get one dummy variable per level (minus the reference).\n",
    "    Then plots x=coefficient, y=-log10(p-value) for *all* those dummy variables.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    results = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Skip the outcome column\n",
    "        if col == outcome_col:\n",
    "            continue\n",
    "\n",
    "        # Skip encounter_id\n",
    "        if col == \"anes_procedure_encounter_id_2273\" or col == \"unique_pt_id\":\n",
    "            continue\n",
    "        \n",
    "        # Skip datetime or other unsupported types\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            continue\n",
    "        \n",
    "        # Subset to non-null rows in outcome & predictor\n",
    "        temp_df = df[[outcome_col, col]].dropna()\n",
    "        \n",
    "        # Skip if not enough variation\n",
    "        if temp_df[col].nunique() < 2 or temp_df[col].count() < 5:\n",
    "            continue\n",
    "        \n",
    "        # Build formula\n",
    "        # Wrap in C() if categorical\n",
    "        if pd.api.types.is_numeric_dtype(temp_df[col]):\n",
    "            formula = f\"{outcome_col} ~ {col}\"\n",
    "        else:\n",
    "            formula = f\"{outcome_col} ~ C({col})\"\n",
    "        \n",
    "        # Fit the logistic model\n",
    "        try:\n",
    "            model = smf.logit(formula, data=temp_df).fit(disp=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping column '{col}' due to fitting error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # For each parameter (except the Intercept),\n",
    "        # capture the coefficient and p-value.\n",
    "        for param_name in model.params.index:\n",
    "            if param_name == 'Intercept':\n",
    "                continue\n",
    "            \n",
    "            coef = model.params.loc[param_name]\n",
    "            pval = model.pvalues.loc[param_name]\n",
    "            \n",
    "            # You might want to create a cleaner label for the parameter.\n",
    "            # For categorical variables, param_name will look like 'C(col)[T.level]'\n",
    "            # We'll store the raw param_name, but you can parse it if you like.\n",
    "\n",
    "            results.append({\n",
    "                'column': col,\n",
    "                'param_name': param_name,\n",
    "                'coef': coef,\n",
    "                'pval': pval\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(\"No valid predictors found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort by p-value (optional)\n",
    "    results_df = results_df.sort_values(by='pval')\n",
    "\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "results_df = all_regressions_each_dummy(neuraxial_catheter_df, 'failed_catheter')\n",
    "# This returns a DataFrame with columns: [column, param_name, coef, pval].\n",
    "# Each level of a categorical predictor will appear as a separate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def parse_param_name(param_name):\n",
    "    \"\"\"\n",
    "    Parses a statsmodels parameter name like:\n",
    "        'C(col)[T.value]'\n",
    "    and returns the level name 'value'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regex for the typical pattern: C(colName)[T.levelName]\n",
    "    pattern = r'.*\\[T\\.(.+)\\]'\n",
    "    match = re.match(pattern, param_name)\n",
    "    if match:\n",
    "        level_name = match.group(1)\n",
    "        return level_name\n",
    "    # If it doesn't match, assume it's some other type of parameter (e.g., numeric var)\n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['category_variable'] = results_df['param_name'].apply(parse_param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['pval'] < 0.05 / 59].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits from the graph annotations\n",
    "def remove_nums(string):\n",
    "    \"\"\"\n",
    "    Removes numbers from a string.\n",
    "    \"\"\"\n",
    "    return ''.join([i for i in string if not i.isdigit()])\n",
    "\n",
    "\n",
    "# Create plot: coefficient vs -log10(p-value)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "offset = 1e-300  # so we don't take log10(0)\n",
    "x_vals = results_df[results_df['pval'] < 0.9]['coef']\n",
    "y_vals = -np.log10(results_df[results_df['pval'] < 0.9]['pval'] + offset)\n",
    "\n",
    "sc = ax.scatter(x_vals, y_vals, color='blue')\n",
    "\n",
    "# Annotate each point\n",
    "for i, row in results_df[results_df['pval'] < 0.9].iterrows():\n",
    "    ax.text(\n",
    "        row['coef'],\n",
    "        -np.log10(row['pval'] + offset),\n",
    "        remove_nums(str(row['column'] + '__' + str(row['category_variable']))),\n",
    "        fontsize=8,\n",
    "        ha='left',\n",
    "        va='bottom'\n",
    "    )\n",
    "\n",
    "# Add a reference line for p=0.05\n",
    "ax.axhline(-np.log10(0.05), color='red', linestyle='--', label='p=0.05')\n",
    "\n",
    "ax.set_xlabel('Coefficient')\n",
    "ax.set_ylabel('-log10(p-value)')\n",
    "ax.set_title(f'Logistic Regressions for Catheter_Failure ~ Each Predictor (All Dummies)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume neuraxial_catheter_df is already defined.\n",
    "# For example:\n",
    "# neuraxial_catheter_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Identify categorical columns (assuming columns with dtype 'object' or 'category')\n",
    "categorical_cols = neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Create dummy variables for all identified categorical columns\n",
    "neuraxial_catheter_dummies = pd.get_dummies(neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1), columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Compute the correlation matrix using Pearson correlation by default\n",
    "correlation_matrix = neuraxial_catheter_dummies.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for neuraxial_catheter_df (with dummies)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete interesting colinear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_columns = ['current_anesthesiologist_catheter_count', # correlated with categorical experience variables\n",
    "                             'current_resident_catheter_count', # correlated with categorical experience variables\n",
    "                             'moderately_experienced_anesthesiologist', # correlated with categorical experience variables\n",
    "                             'gravidity_2047', # correlated witih parity\n",
    "                             'maternal_weight_end_pregnancy_2045', # correlated with BMI end pregnancy\n",
    "                              \"only_private_insurance\", # correlated with composite_SES_advantage\n",
    "                            \"maternal_language_english\", # correlated with composite_SES_advantage\n",
    "                            \"marital_status_married_or_partner\", # correlated with composite_SES_advantage\n",
    "                            \"country_of_origin_USA\", # correlated with composite_SES_advantage\n",
    "                            \"employment_status_fulltime\", # correlated with composite_SES_advantage\n",
    "                               'epidural_needle_type', # correlated with delivery location\n",
    "                               'maternal_ethnicity', # correlated with race\n",
    "                              \"delivery_site\", # correlated with delivery_site_bwh,\n",
    "                              \"fetal_presentation_position_2247\", # correlated with position_posterior_or_transverse\n",
    "                              \"fetal_presentation_category_2243\" # correlated with presentation_cephalic\n",
    "                    ]\n",
    "\n",
    "neuraxial_catheter_df = neuraxial_catheter_df.drop(columns=overlap_columns,axis=1)\n",
    "epidural_df = epidural_df.drop(columns=overlap_columns,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume neuraxial_catheter_df is already defined.\n",
    "# For example:\n",
    "# neuraxial_catheter_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Identify categorical columns (assuming columns with dtype 'object' or 'category')\n",
    "categorical_cols = neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Create dummy variables for all identified categorical columns\n",
    "neuraxial_catheter_dummies = pd.get_dummies(neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1), columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Compute the correlation matrix using Pearson correlation by default\n",
    "correlation_matrix = neuraxial_catheter_dummies.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for neuraxial_catheter_df (with dummies)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete non-predictive features (from neuraxial_catheter_df, not epidural_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_predictive_columns = ['maternal_race','has_scoliosis','composite_SES_advantage']\n",
    "neuraxial_catheter_df = neuraxial_catheter_df.drop(columns=non_predictive_columns,axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete unknowable columns to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_leakage_columns = ['rom_thru_delivery_hours','placement_to_delivery_hours']\n",
    "\n",
    "neuraxial_catheter_df = neuraxial_catheter_df.drop(data_leakage_columns, axis=1,errors='ignore')\n",
    "epidural_df = epidural_df.drop(data_leakage_columns, axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['bmi_end_pregnancy_2044', 'baby_weight_2196', 'gestational_age_weeks', 'maternal_age_years']:\n",
    "    neuraxial_catheter_df[col] = neuraxial_catheter_df[col] - neuraxial_catheter_df[col].median()\n",
    "    epidural_df[col] = epidural_df[col] - epidural_df[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuraxial_catheter_df['gestational_age_weeks'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random data for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = neuraxial_catheter_df.copy()\n",
    "failure_rate = test_dataset['failed_catheter'].mean()\n",
    "test_dataset['failed_catheter'] = np.random.binomial(n=1, p=failure_rate, size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = test_dataset\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "# If you do NOT want to use `anes_procedure_encounter_id_2273` as a feature:\n",
    "X = X.drop(columns=[\"unique_pt_id\"])  \n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train logistic regression with class weights\n",
    "logistic_model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', n_jobs=1)\n",
    "logistic_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = logistic_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"TEST RANDOM Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nTEST RANDOM Classification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real LOGIT regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Load and clean data\n",
    "data = neuraxial_catheter_df.copy()\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "# 2. Group-aware split\n",
    "groups = X['unique_pt_id']\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "\n",
    "# 3. Identify numeric vs. categorical\n",
    "numeric_features = X_train.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# 4. Impute numeric\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "X_train[numeric_features] = num_imputer.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = num_imputer.transform(X_test[numeric_features])\n",
    "\n",
    "# 5. Scale numeric\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# 6. Impute categorical\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_features] = cat_imputer.fit_transform(X_train[categorical_features])\n",
    "X_test[categorical_features] = cat_imputer.transform(X_test[categorical_features])\n",
    "\n",
    "# 7. One-hot encode categorical\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features, drop_first=True,dtype=int).astype(float)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_features, drop_first=True,dtype=int).astype(float)\n",
    "\n",
    "X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "# 8. Fit logistic regression with Statsmodels\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit(disp=0)\n",
    "\n",
    "# 9. Predict\n",
    "X_test_const = sm.add_constant(X_test, has_constant='add')\n",
    "y_pred_prob = result.predict(X_test_const)\n",
    "\n",
    "evaluation_metrics_by_threshold = []\n",
    "\n",
    "for i in range(0, 21):\n",
    "    prediction_threshold = i / 20\n",
    "    y_pred = (y_pred_prob >= prediction_threshold).astype(int)\n",
    "\n",
    "    # 10. Evaluate\n",
    "    evaluation_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred,zero_division=np.nan),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "        # \"classification_report\": classification_report(y_test, y_pred)\n",
    "    }\n",
    "    evaluation_metrics_by_threshold.append(evaluation_metrics)\n",
    "\n",
    "result_summ = result.summary(alpha = 0.001)\n",
    "\n",
    "# print(\"Model Evaluation:\")\n",
    "# for metric, value in evaluation_metrics.items():\n",
    "#     if metric == \"classification_report\":\n",
    "#         print(\"\\nClassification Report:\\n\", value)\n",
    "#     else:\n",
    "#         print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_by_threshold = pd.DataFrame(evaluation_metrics_by_threshold)\n",
    "evaluation_metrics_by_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_result_table = result_summ.tables[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table = pd.DataFrame(result_summ.tables[1].data)\n",
    "# 1. Set the first row as the new column headers.\n",
    "logit_predictor_table.columns = logit_predictor_table.iloc[0]\n",
    "\n",
    "# 2. Remove the first row (since it's now serving as header).\n",
    "logit_predictor_table = logit_predictor_table[1:]\n",
    "\n",
    "# 3. Set the first column (after the column headers update) as the index.\n",
    "# Here, `df.columns[0]` represents the name of the first column.\n",
    "logit_predictor_table = logit_predictor_table.set_index(logit_predictor_table.columns[0])\n",
    "\n",
    "# 4. Sort by the 'P>|z|' column in ascending order.\n",
    "logit_predictor_table = logit_predictor_table.sort_values('P>|z|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the odds ratio (OR) and the 95% confidence intervals\n",
    "logit_predictor_table['OR'] = np.exp(logit_predictor_table['coef'].astype(float))\n",
    "logit_predictor_table['OR_lower'] = np.exp(logit_predictor_table['[0.0005'].astype(float))\n",
    "logit_predictor_table['OR_upper'] = np.exp(logit_predictor_table['0.9995]'].astype(float))\n",
    "\n",
    "# Create the 'OR (95% CI)' column\n",
    "logit_predictor_table['OR (99.9% CI)'] = logit_predictor_table.apply(\n",
    "    lambda row: f\"{row['OR']:.2f} ({row['OR_lower']:.2f} - {row['OR_upper']:.2f})\", axis=1)\n",
    "\n",
    "logit_predictor_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_factors = ['delivery_site_bwh',\n",
    "       'parity_2048', 'gestational_age_weeks',\n",
    "       'presentation_cephalic',\n",
    "       'has_dorsalgia',\n",
    "       'maternal_age_years', 'has_back_problems',\n",
    "       'prior_failed_catheters_prev_enc',\n",
    "       'baby_weight_2196',\n",
    "       'composite_psychosocial_problems', 'prior_failed_catheters_this_enc',\n",
    "       'position_posterior_or_transverse', 'prior_pain_scores_max',\n",
    "       'bmi_end_pregnancy_2044', 'labor_induction']\n",
    "\n",
    "procedural_factors = ['true_procedure_type_incl_dpe_cse', \n",
    "       'true_procedure_type_incl_dpe_dpe',\n",
    "       'highly_experienced_anesthesiologist_none', 'lor_depth', 'number_of_neuraxial_attempts',\n",
    "       'paresthesias_present',\n",
    "       'highly_experienced_resident_no',\n",
    "       'highly_experienced_anesthesiologist_yes',\n",
    "       'highly_experienced_resident_yes',\n",
    "       'true_procedure_type_incl_dpe_intrathecal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = logit_predictor_table.loc[patient_factors].copy()\n",
    "procedural_df = logit_predictor_table.loc[procedural_factors].copy()\n",
    "\n",
    "rename_map = {\n",
    "    'gestational_age_weeks': 'Gestational Age (per week)',\n",
    "    'delivery_site_bwh': 'Delivery at flagship obstetric teaching hospital (vs other)',\n",
    "    'baby_weight_2196': 'Baby Weight (per kg)',\n",
    "    'bmi_end_pregnancy_2044': 'BMI (per kg/m^2)',\n",
    "    'parity_2048': 'Parity (per birth)',\n",
    "    'has_dorsalgia': 'Back pain (vs none)',\n",
    "    'has_back_problems': 'Scoliosis or other back problems (vs none)',\n",
    "    'prior_pain_scores_max': 'Max pain score prior to placement (per unit 0-10)',\n",
    "    'composite_psychosocial_problems': 'Psychosocial risk factors (vs none)',\n",
    "    'prior_failed_catheters_this_enc': 'Prior failed catheters in this encounter (per failure)',\n",
    "    'prior_failed_catheters_prev_enc': 'Prior failed catheters in prior encounters (per failure)',\n",
    "    'maternal_age_years': 'Maternal age (per year)',\n",
    "    'labor_induction': 'Induced labor (vs not)',\n",
    "    'position_posterior_or_transverse': 'Posterior or transverse fetal position (vs other)',\n",
    "    'presentation_cephalic': 'Cephalic fetal presentation (vs other)',\n",
    "    # procedural factors below\n",
    "    'lor_depth': 'Depth to loss of resistance (per cm)',\n",
    "    'highly_experienced_anesthesiologist_yes': 'Highly experienced attending anesthesiologist (vs less experienced)',\n",
    "    'highly_experienced_anesthesiologist_none': 'No attending anesthesiologist (vs less experienced attending)',\n",
    "    'highly_experienced_resident_yes': 'Highly experienced resident (vs no resident)',\n",
    "    'highly_experienced_resident_no': 'Less experienced resident (vs no resident)',\n",
    "    'paresthesias_present': 'Paresthesias present during placement (vs none)',\n",
    "    'number_of_neuraxial_attempts': 'Number of placement attempts (per attempt)',\n",
    "    'true_procedure_type_incl_dpe_intrathecal': 'Intrathecal catheter (vs conventional epidural)',\n",
    "    'true_procedure_type_incl_dpe_dpe': 'Dural puncture epidural (vs conventional epidural)',\n",
    "    'true_procedure_type_incl_dpe_cse': 'Combined spinal-epidural (vs conventional epidural)'\n",
    "}\n",
    "\n",
    "patient_df = patient_df.rename(index=rename_map)\n",
    "procedural_df = procedural_df.rename(index=rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "def plot_forest_colored_with_markers(\n",
    "    ax, \n",
    "    df, \n",
    "    title='Forest Plot', \n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5, \n",
    "    x_max=1.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a forest chart on 'ax' given a DataFrame 'df' with columns:\n",
    "      - 'OR'\n",
    "      - 'OR_lower'\n",
    "      - 'OR_upper'\n",
    "    \n",
    "    X-axis is restricted to [x_min, x_max].\n",
    "    \n",
    "    Rules:\n",
    "      - If the center OR is outside [x_min, x_max], skip plotting its dot.\n",
    "      - If the OR or any part of its CI is beyond [x_min, x_max], place '<' or '>' at that boundary.\n",
    "      - Print \"OR X.XX (L.LL, U.UU)\" above each data point in the same color.\n",
    "      - Color each factor's name on the y-axis to match that factor's color.\n",
    "    \n",
    "    Color scheme for significance:\n",
    "      - red if entire CI > 1\n",
    "      - blue if entire CI < 1\n",
    "      - black otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by OR if you want smaller/larger ORs in order\n",
    "    df = df.sort_values('OR')\n",
    "\n",
    "    # We'll manually set the y-ticks, one row per factor\n",
    "    y_positions = np.arange(len(df))\n",
    "\n",
    "    # We won't set the yticklabels yet; we'll do them manually to color each label.\n",
    "    ax.set_yticks(y_positions)\n",
    "    # Temporarily set them all to blank\n",
    "    ax.set_yticklabels([\"\"] * len(df))\n",
    "\n",
    "    # We'll collect the color for each row, so we can color the labels afterward\n",
    "    factor_colors = []\n",
    "\n",
    "    # Plot each factor individually\n",
    "    for y_pos, (idx, row) in zip(y_positions, df.iterrows()):\n",
    "        or_val = row['OR']\n",
    "        ci_low = row['OR_lower']\n",
    "        ci_high = row['OR_upper']\n",
    "\n",
    "        # Decide the color based on significance\n",
    "        if ci_low > 1:\n",
    "            c = 'red'    # entire CI above 1 => significant risk\n",
    "        elif ci_high < 1:\n",
    "            c = 'blue'   # entire CI below 1 => significant protective\n",
    "        else:\n",
    "            c = 'black'  # not significant\n",
    "\n",
    "        factor_colors.append(c)\n",
    "\n",
    "        # Check if OR or CI extends beyond the plot range\n",
    "        outside_left = (or_val < x_min) or (ci_low < x_min)\n",
    "        outside_right = (or_val > x_max) or (ci_high > x_max)\n",
    "\n",
    "        # If the center OR is out of range, skip the dot\n",
    "        center_outside = (or_val < x_min) or (or_val > x_max)\n",
    "        dot_fmt = 'none' if center_outside else 'o'\n",
    "\n",
    "        # Calculate the full error bar from the center\n",
    "        left_err = or_val - ci_low\n",
    "        right_err = ci_high - or_val\n",
    "\n",
    "        # Plot the error bar (may or may not include the dot)\n",
    "        ax.errorbar(\n",
    "            or_val,\n",
    "            y_pos,\n",
    "            xerr=[[left_err], [right_err]],\n",
    "            fmt=dot_fmt,   # skip the dot if center is outside\n",
    "            color=c,\n",
    "            ecolor=c,\n",
    "            capsize=4\n",
    "        )\n",
    "\n",
    "        # Place boundary markers if the OR or any part of CI is outside\n",
    "        if outside_left:\n",
    "            ax.text(\n",
    "                x_min, y_pos, '<', \n",
    "                va='center', ha='right', color=c, fontsize=14\n",
    "            )\n",
    "        if outside_right:\n",
    "            ax.text(\n",
    "                x_max, y_pos, '>', \n",
    "                va='center', ha='left', color=c, fontsize=14\n",
    "            )\n",
    "\n",
    "        # Prepare the label \"OR X.XX (L.LL - U.UU)\"\n",
    "        label_str = f\"OR {or_val:.2f} ({ci_low:.2f} - {ci_high:.2f})\"\n",
    "\n",
    "        # Place the label just above the data point (or boundary)\n",
    "        # We'll define a small offset in Y to shift text \"above\" the marker\n",
    "        label_offset = 0.2  # Adjust as needed\n",
    "        label_y = y_pos - label_offset  # axis is inverted => subtract to go \"up\"\n",
    "        ax.text(\n",
    "            1.08, label_y,\n",
    "            label_str,\n",
    "            va='bottom',   # text rises from the point\n",
    "            ha='center',\n",
    "            color=c,\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "    # Now color each factor name using the same color\n",
    "    # We already set blank y-ticklabels, so let's manually place them:\n",
    "    for y_pos, (idx, c) in zip(y_positions, zip(df.index, factor_colors)):\n",
    "        # We'll place the text a bit left of x_min so it doesn't collide with the plot\n",
    "        ax.text(\n",
    "            x_min - 0.05, y_pos,\n",
    "            idx,\n",
    "            va='center', ha='right',\n",
    "            color=c,\n",
    "            fontsize=12\n",
    "        )\n",
    "\n",
    "    # Draw a vertical line at OR=1\n",
    "    ax.axvline(x=1, color='gray', linestyle='--')\n",
    "\n",
    "    # Invert y-axis so the top row is at the top\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Limit the x-axis\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_xlabel(x_label, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Example usage with TWO subplots\n",
    "# ==========================\n",
    "\n",
    "# Suppose you have:\n",
    "#   patient_df\n",
    "#   procedural_df\n",
    "# Each with columns: ['OR', 'OR_lower', 'OR_upper']\n",
    "# and index = factor names.\n",
    "\n",
    "# Create the figure with two columns\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 8))\n",
    "\n",
    "plot_forest_colored_with_markers(\n",
    "    ax=ax1,\n",
    "    df=patient_df,\n",
    "    title='Patient Factors',\n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5,\n",
    "    x_max=1.5\n",
    ")\n",
    "\n",
    "plot_forest_colored_with_markers(\n",
    "    ax=ax2,\n",
    "    df=procedural_df,\n",
    "    title='Procedural Factors',\n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5,\n",
    "    x_max=1.5\n",
    ")\n",
    "\n",
    "# Create a manual legend for color interpretation:\n",
    "protect_marker = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
    "                               label='Significant protective factor')\n",
    "ns_marker = mlines.Line2D([], [], color='black', marker='o', linestyle='None',\n",
    "                          label='Not significant')\n",
    "risk_marker = mlines.Line2D([], [], color='red', marker='o', linestyle='None',\n",
    "                            label='Significant risk factor')\n",
    "\n",
    "fig.legend(\n",
    "    handles=[protect_marker, ns_marker, risk_marker],\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.05),\n",
    "    ncol=3,\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = neuraxial_catheter_df\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "\n",
    "## Drop delivery_site\n",
    "# X = X.drop(columns=[\"delivery_site\"],errors='ignore')\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "# If you do NOT want to use `anes_procedure_encounter_id_2273` as a feature:\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])  \n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train logistic regression with class weights\n",
    "logistic_model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', n_jobs=1)\n",
    "logistic_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = logistic_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nClassification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the feature names produced by the ColumnTransformer\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 2. Get the coefficients from the trained logistic model\n",
    "#    For binary classification, .coef_ is an array of shape (1, n_features)\n",
    "coefficients = logistic_model.coef_[0]\n",
    "\n",
    "\n",
    "# 3. Combine feature names with coefficients into a list of tuples\n",
    "coef_pairs = list(zip(feature_names, coefficients))\n",
    "\n",
    "# 4. Print the coefficients sorted by absolute magnitude\n",
    "print(\"Coefficients for each feature (sorted by absolute magnitude):\")\n",
    "for name, coef in sorted(coef_pairs, key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "# 5. Print the coefficients sorted alphabetically\n",
    "print('---------------------------------------------')\n",
    "print(\"Coefficients for each feature (sorted alphabetically):\")\n",
    "for name, coef in sorted(coef_pairs):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "# 6. Print the intercept\n",
    "print(f\"Intercept: {logistic_model.intercept_[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Imports\n",
    "##############################################################################\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Import XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "##############################################################################\n",
    "# 0. Load and prepare the dataset\n",
    "##############################################################################\n",
    "data = neuraxial_catheter_df\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "# # Drop delivery_site\n",
    "# X = X.drop(columns=[\"delivery_site\"],errors='ignore')\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])  # remove ID column from features\n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "##############################################################################\n",
    "# 3. Identify numeric and categorical columns\n",
    "##############################################################################\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "##############################################################################\n",
    "# 4. Create preprocessing pipelines\n",
    "##############################################################################\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "##############################################################################\n",
    "# 5. Preprocess the data\n",
    "##############################################################################\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "##############################################################################\n",
    "# 6. Train XGBoost Classifier\n",
    "##############################################################################\n",
    "# Instantiate the XGBClassifier\n",
    "# Note: You can tune parameters such as 'scale_pos_weight' if your data is imbalanced.\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=(y_train.shape[0] - y_train.sum()) / y_train.sum()\n",
    ")\n",
    "xgb_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "##############################################################################\n",
    "# 7. Make predictions\n",
    "##############################################################################\n",
    "y_pred = xgb_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = xgb_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "##############################################################################\n",
    "# 8. Evaluate the model\n",
    "##############################################################################\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "##############################################################################\n",
    "# 9. Print the evaluation metrics\n",
    "##############################################################################\n",
    "print(\"Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nClassification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring for DPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df['dpe'] = (df['true_procedure_type_incl_dpe'] == 'dpe').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'dpe'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring for CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df['cse'] = (df['true_procedure_type_incl_dpe'] == 'cse').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'cse'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity scoring for DPE vs CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df = df[df['true_procedure_type_incl_dpe'].isin(['cse', 'dpe'])]\n",
    "df['cse_not_dpe'] = (df['true_procedure_type_incl_dpe'] == 'cse').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'cse_not_dpe'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "XGBoost - hyperparameter tuning with Optuna\n",
    "\n",
    "Shapley values for interpretability\n",
    "\n",
    "Eliminate features that are both poorly predictive and have lots of missing data\n",
    "\n",
    "Abstract functions and separate them into different files\n",
    "\n",
    "Fewer features will improve interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
