{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import modules.data_analysis_utils as dau\n",
    "from importlib import reload\n",
    "\n",
    "# my_computer_fpath = \"C:\\\\Users\\\\dfber\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\"\n",
    "my_computer_fpath = \"C:\\\\Users\\\\User\\\\OneDrive - Mass General Brigham\\\\Epidural project\\\\Data\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(my_computer_fpath + 'processed_and_imputed_merlin_data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only epidural-only catheter procedures\n",
    "neuraxial_catheter_df = df\n",
    "epidural_df = df[(df['true_procedure_type_incl_dpe'] == 'epidural') | (df['true_procedure_type_incl_dpe'] == 'dpe')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_groups = {\n",
    "    'failure': ['failed_catheter','has_subsequent_neuraxial_catheter','has_subsequent_spinal','has_subsequent_airway'],\n",
    "    'timing': ['placement_to_delivery_hours','rom_thru_delivery_hours','rom_to_placement_hours'],\n",
    "    'maternal_age_gp': ['maternal_age_years','gravidity_2047','parity_2048'],\n",
    "    'multiple_gestation_and_labor_induction': ['multiple_gestation','labor_induction'],\n",
    "    'baby_size': ['gestational_age_weeks','baby_weight_2196'],\n",
    "    'maternal_size': ['bmi_end_pregnancy_2044', 'bmi_greater_than_40', 'maternal_weight_end_pregnancy_2045', 'bmi_before_pregnancy_2161'],\n",
    "    'team_composition': ['has_resident','has_anesthesiologist'],\n",
    "    'team_catheter_counts': ['current_anesthesiologist_catheter_count','current_resident_catheter_count','total_team_catheter_count'],\n",
    "    'bmi_and_experience': [\"high_bmi_and_highly_experienced_resident\",    \"high_bmi_and_lowly_experienced_resident\",    \"high_bmi_and_no_resident\",    \"high_bmi_and_highly_experienced_anesthesiologist\"],\n",
    "    'scoliosis_and_experience': [\"scoliosis_and_highly_experienced_resident\",    \"scoliosis_and_lowly_experienced_resident\",    \"scoliosis_and_no_resident\",    \"scoliosis_and_highly_experienced_anesthesiologist\"],\n",
    "    'back_group': ['high_bmi_and_scoliosis','has_scoliosis','has_dorsalgia','has_back_problems'],\n",
    "    'maternal_risk': ['prior_ob_cmi_scores_max','CS_hx','high_risk_current_pregnancy','high_risk_hx','iufd'],\n",
    "    'psychosocial_and_ses': ['composite_psychosocial_problems','only_private_insurance','maternal_language_english','marital_status_married_or_partner','country_of_origin_USA','employment_status_fulltime','composite_SES_advantage'],\n",
    "    'lor': ['lor_depth','predicted_lor_depth','unexpected_delta_lor','unexpected_delta_lor_squared'],\n",
    "    'pain_and_attempts': ['prior_pain_scores_max','paresthesias_present','number_of_neuraxial_attempts','number_of_spinal_attempts'],\n",
    "    'prior_catheters': ['prior_failed_catheters_this_enc','prior_failed_catheters_prev_enc','prior_all_catheters_all_enc']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_correlation_heatmap_with_related_groups(neuraxial_catheter_df, drop_columns=['anes_procedure_encounter_id_2273','unique_pt_id'],additional_groups=related_groups,draw_group_boxes=True,draw_group_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop any columns here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did formerly but no longer do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_correlation_heatmap_with_related_groups(neuraxial_catheter_df, drop_columns=['anes_procedure_encounter_id_2273','unique_pt_id'],additional_groups=related_groups,draw_group_boxes=True,draw_group_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(neuraxial_catheter_df['true_procedure_type_incl_dpe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='true_procedure_type_incl_dpe', value_col='failed_catheter',\n",
    "                           sort_by=None, legend_labels=['Successful', 'Failed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anesthesiologist Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='anesthesiologist_experience_category', value_col='failed_catheter',custom_order=['no_anesthesiologist','low','moderate','high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='resident_experience_category', value_col='failed_catheter',custom_order=['no_resident','low','high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='anesthesiologist_experience_category',index_col_2='resident_experience_category', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_violin_crosstab_anova(neuraxial_catheter_df, index_col='resident_experience_category', value_col='bmi_end_pregnancy_2044',custom_order=['no_resident','low','high'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivery Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='delivery_site', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='delivery_site', value_col='has_subsequent_neuraxial_catheter',\n",
    "                           sort_by=None, legend_labels=['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='delivery_site', value_col='has_subsequent_spinal',\n",
    "                           sort_by=None, legend_labels=['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='delivery_site', value_col='has_subsequent_airway',\n",
    "                           sort_by=None, legend_labels=['Yes', 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_violin_crosstab_anova(neuraxial_catheter_df, index_col='delivery_site', value_col='bmi_end_pregnancy_2044')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: create a pie chart of the fraction of DPE in epidural_df\n",
    "\n",
    "# Count DPE values, treating NaN and '' as \"no\"\n",
    "dpe_counts = epidural_df['true_procedure_type_incl_dpe'].value_counts()\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(dpe_counts, labels=dpe_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Fraction of DPE in Epidural Procedures')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='delivery_site', value_col='true_procedure_type_incl_dpe', sort_by='intrathecal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='true_procedure_type_incl_dpe', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='delivery_site', index_col_2='true_procedure_type_incl_dpe',value_col='failed_catheter',sort_by='no_sort', title='Failed Catheter Rate by Delivery Site and Procedure Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.crosstab([df['delivery_site'], df['true_procedure_type_incl_dpe']], df['failed_catheter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoliosis and back problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='has_scoliosis', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='has_back_problems', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='has_dorsalgia', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetal Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='fetal_presentation', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='fetal_position', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race and SES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_analyze = [\n",
    "    'maternal_race',\n",
    "    'composite_psychosocial_problems',\n",
    "    'only_private_insurance',\n",
    "    'maternal_language_english',\n",
    "    'marital_status_married_or_partner',\n",
    "    'country_of_origin_USA',\n",
    "    'employment_status_fulltime',\n",
    "    'composite_SES_advantage'\n",
    "]\n",
    "for col in columns_to_analyze:\n",
    "    dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col=col, value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='prior_pain_scores_max', value_col='failed_catheter', sort_by='no_sort', title='Failed Catheter Rate by Prior Pain Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gravidity and Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='gravidity_2047', value_col='failed_catheter', sort_by='no_sort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='parity_2048', value_col='failed_catheter',  sort_by='no_sort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maternal Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='maternal_age_years', y_axis='failed_catheter', bin_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMI / height / weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='bmi_end_pregnancy_2044', y_axis='failed_catheter', bin_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='bmi_end_pregnancy_2044', y_axis='failed_catheter', bin_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='maternal_weight_end_pregnancy_2045', y_axis='failed_catheter', bin_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needle Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='epidural_needle_type', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paresthesias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='paresthesias_present', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='number_of_neuraxial_attempts', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss of Resistance Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_histogram(neuraxial_catheter_df, col='lor_depth', bin_space=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the lor_depth by 0.5\n",
    "neuraxial_catheter_df['lor_depth_bin'] = (neuraxial_catheter_df['lor_depth'] // 0.5) * 0.5\n",
    "\n",
    "# Plot the stacked bar histogram\n",
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='lor_depth_bin', value_col='failed_catheter',sort_by='no_sort', title='Failed Catheter Rate by LOR Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Plot number of neuraxial attempts vs LOR depth on the x-axis. Add jiggle to both x and y axes\n",
    "\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'lor_depth'])\n",
    "\n",
    "# Add random jiggle to both x and y axes\n",
    "jiggle_x = np.random.normal(scale = 0.1, size=len(df_plot))\n",
    "jiggle_y = np.random.normal(scale = 0.1, size=len(df_plot))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_plot['lor_depth'] + jiggle_x, df_plot['number_of_neuraxial_attempts'] + jiggle_y, alpha=0.5)\n",
    "plt.xlabel('LOR Depth')\n",
    "plt.ylabel('Number of Neuraxial Attempts')\n",
    "plt.title('Number of Neuraxial Attempts vs. LOR Depth with Jiggle')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the data, dropping NaNs\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['lor_depth', 'number_of_neuraxial_attempts'])\n",
    "\n",
    "# Create a list of unique values in 'number_of_neuraxial_attempts'\n",
    "attempts = [1, 2, 3, 4]\n",
    "\n",
    "# Create histograms for each number_of_neuraxial_attempts\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, attempt in enumerate(attempts, start=1):\n",
    "    # Filter data for each attempt\n",
    "    subset = df_plot[df_plot['number_of_neuraxial_attempts'] == attempt]\n",
    "    \n",
    "    # Plot histogram for 'lor_depth'\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.hist(subset['lor_depth'], bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Histogram of LOR Depth for {attempt} Neuraxial Attempt(s)')\n",
    "    plt.xlabel('LOR Depth')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_violin_crosstab_anova(neuraxial_catheter_df, index_col='number_of_neuraxial_attempts', value_col='lor_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='number_of_neuraxial_attempts', y_axis='failed_catheter', bin_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='lor_depth', y_axis='failed_catheter', bin_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_scatter(neuraxial_catheter_df, x_axis='bmi_end_pregnancy_2044', y_axis='lor_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Extract the data, dropping NaNs\n",
    "df_plot = neuraxial_catheter_df.dropna(subset=['lor_depth', 'bmi_end_pregnancy_2044'])\n",
    "x = df_plot['bmi_end_pregnancy_2044'].values\n",
    "y = df_plot['lor_depth'].values\n",
    "\n",
    "# Perform kernel density estimation\n",
    "xy = np.vstack([x, y])\n",
    "kde = gaussian_kde(xy)\n",
    "\n",
    "# Define grid over data range\n",
    "xmin, xmax = x.min() - 1, x.max() + 1\n",
    "ymin, ymax = y.min() - 1, y.max() + 1\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "Z = np.reshape(kde(positions).T, X.shape)\n",
    "\n",
    "# Create the contour plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(X, Y, Z, levels=15, cmap='viridis')\n",
    "plt.colorbar(label='Density')\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('LOR Depth')\n",
    "plt.title('Contour Plot of LOR Depth vs. BMI (KDE)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with NaNs\n",
    "df_plot = neuraxial_catheter_df.dropna(\n",
    "    subset=['lor_depth', 'bmi_end_pregnancy_2044', 'failed_catheter']\n",
    ")\n",
    "\n",
    "# Separate the data by failed_catheter category\n",
    "df_0 = df_plot[df_plot['failed_catheter'] == 0]\n",
    "df_1 = df_plot[df_plot['failed_catheter'] == 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot for failed_catheter = 0\n",
    "plt.scatter(\n",
    "    df_0['bmi_end_pregnancy_2044'],\n",
    "    df_0['lor_depth'],\n",
    "    s=10, \n",
    "    alpha=0.7,\n",
    "    color='blue',\n",
    "    label='Failed Catheter = 0'\n",
    ")\n",
    "\n",
    "# Scatter plot for failed_catheter = 1\n",
    "plt.scatter(\n",
    "    df_1['bmi_end_pregnancy_2044'],\n",
    "    df_1['lor_depth'],\n",
    "    s=10, \n",
    "    alpha=0.7,\n",
    "    color='orange',\n",
    "    label='Failed Catheter = 1'\n",
    ")\n",
    "\n",
    "plt.scatter(neuraxial_catheter_df['bmi_end_pregnancy_2044'], neuraxial_catheter_df['predicted_lor_depth'], s=10, alpha=1, color='black', label='Predicted')\n",
    "\n",
    "# --- Calculate and plot regression line for failed_catheter = 0 ---\n",
    "p0 = np.polyfit(df_0['bmi_end_pregnancy_2044'], df_0['lor_depth'], deg=1)  # slope, intercept\n",
    "slope_0, intercept_0 = p0\n",
    "print(f\"For failed_catheter=0, slope = {slope_0:.2f}, intercept = {intercept_0:.2f}\")\n",
    "\n",
    "x_vals_0 = np.linspace(df_0['bmi_end_pregnancy_2044'].min(), df_0['bmi_end_pregnancy_2044'].max(), 100)\n",
    "y_vals_0 = np.polyval(p0, x_vals_0)\n",
    "plt.plot(x_vals_0, y_vals_0, color='blue', linewidth=2)\n",
    "\n",
    "# --- Calculate and plot regression line for failed_catheter = 1 ---\n",
    "p1 = np.polyfit(df_1['bmi_end_pregnancy_2044'], df_1['lor_depth'], deg=1)\n",
    "slope_1, intercept_1 = p1\n",
    "print(f\"For failed_catheter=1, slope = {slope_1:.2f}, intercept = {intercept_1:.2f}\")\n",
    "\n",
    "x_vals_1 = np.linspace(df_1['bmi_end_pregnancy_2044'].min(), df_1['bmi_end_pregnancy_2044'].max(), 100)\n",
    "y_vals_1 = np.polyval(p1, x_vals_1)\n",
    "plt.plot(x_vals_1, y_vals_1, color='orange', linewidth=2)\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('LOR Depth')\n",
    "plt.title('LOR Depth vs. BMI')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestational Age and Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_histogram(neuraxial_catheter_df, col='gestational_age_weeks', bin_space=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='gestational_age_weeks', y_axis='failed_catheter', bin_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_histogram(neuraxial_catheter_df, col='baby_weight_2196', bin_space=0.1, xtick_space=0.5, xlabel='Baby Weight (kg)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='baby_weight_2196', y_axis='failed_catheter', bin_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior failed catheters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_stacked_bar_histogram(neuraxial_catheter_df, index_col='prior_failed_catheters_this_enc', value_col='failed_catheter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='prior_failed_catheters_this_enc', y_axis='failed_catheter', bin_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placement to Delivery Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.plot_binned_errorbar(neuraxial_catheter_df, x_axis='placement_to_delivery_hours', y_axis='failed_catheter', bin_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some individually interesting regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = neuraxial_catheter_df.dropna(subset=['lor_depth', 'number_of_neuraxial_attempts'])\n",
    "\n",
    "# Fit the model using the formula\n",
    "model = smf.ols('number_of_neuraxial_attempts ~ lor_depth', data=df_corr).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical variables like DPE and failed_catheter\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "dpe_crosstab = pd.crosstab((epidural_df['true_procedure_type_incl_dpe'] == 'dpe').astype(int), epidural_df['failed_catheter'])\n",
    "chi2, p, _, _ = chi2_contingency(dpe_crosstab)\n",
    "\n",
    "print(dpe_crosstab.div(dpe_crosstab.sum(axis=1), axis=0) * 100)\n",
    "print(\"Chi-squared statistic:\", chi2)\n",
    "print(\"P-value:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Do univariate logistic regression separately using number of attempts and loss of resistance depth to predict failure\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Prepare the data for logistic regression with number of attempts as the predictor\n",
    "df_logreg_attempts = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'failed_catheter'])\n",
    "# Fit the logistic regression model\n",
    "model_attempts = smf.logit('failed_catheter ~ number_of_neuraxial_attempts', data=df_logreg_attempts).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_attempts.summary())\n",
    "\n",
    "\n",
    "# Prepare the data for logistic regression with loss of resistance depth as the predictor\n",
    "df_logreg_lor = neuraxial_catheter_df.dropna(subset=['lor_depth', 'failed_catheter'])\n",
    "# Fit the logistic regression model\n",
    "model_lor = smf.logit('failed_catheter ~ lor_depth', data=df_logreg_lor).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_lor.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Now do multivariate analysis using the same two predictors\n",
    "\n",
    "# Prepare the data for logistic regression with both predictors\n",
    "df_logreg_multi = neuraxial_catheter_df.dropna(subset=['number_of_neuraxial_attempts', 'lor_depth', 'failed_catheter'])\n",
    "\n",
    "# Fit the logistic regression model with both predictors\n",
    "model_multi = smf.logit('failed_catheter ~ number_of_neuraxial_attempts + lor_depth', data=df_logreg_multi).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_multi.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for logistic regression with prior_failed_catheters_this_enc as the predictor\n",
    "df_logreg_prior_failed = neuraxial_catheter_df.dropna(subset=['prior_failed_catheters_this_enc', 'failed_catheter'])\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model_attempts = smf.logit('failed_catheter ~ prior_failed_catheters_this_enc', data=df_logreg_prior_failed).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_attempts.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All univariate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def all_regressions_each_dummy(df, outcome_col='failed_catheter'):\n",
    "    \"\"\"\n",
    "    Fits a univariate logistic regression for each column in df (except outcome_col).\n",
    "    For numeric columns, you get a single slope term.\n",
    "    For categorical columns, you get one dummy variable per level (minus the reference).\n",
    "    Then plots x=coefficient, y=-log10(p-value) for *all* those dummy variables.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    results = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Skip the outcome column\n",
    "        if col == outcome_col:\n",
    "            continue\n",
    "\n",
    "        # Skip encounter_id\n",
    "        if col == \"anes_procedure_encounter_id_2273\" or col == \"unique_pt_id\":\n",
    "            continue\n",
    "        \n",
    "        # Skip datetime or other unsupported types\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            continue\n",
    "        \n",
    "        # Subset to non-null rows in outcome & predictor\n",
    "        temp_df = df[[outcome_col, col]].dropna()\n",
    "        \n",
    "        # Skip if not enough variation\n",
    "        if temp_df[col].nunique() < 2 or temp_df[col].count() < 5:\n",
    "            continue\n",
    "        \n",
    "        # Build formula\n",
    "        # Wrap in C() if categorical\n",
    "        if pd.api.types.is_numeric_dtype(temp_df[col]):\n",
    "            formula = f\"{outcome_col} ~ {col}\"\n",
    "        else:\n",
    "            formula = f\"{outcome_col} ~ C({col})\"\n",
    "        \n",
    "        # Fit the logistic model\n",
    "        try:\n",
    "            model = smf.logit(formula, data=temp_df).fit(disp=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping column '{col}' due to fitting error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # For each parameter (except the Intercept),\n",
    "        # capture the coefficient and p-value.\n",
    "        for param_name in model.params.index:\n",
    "            if param_name == 'Intercept':\n",
    "                continue\n",
    "            \n",
    "            coef = model.params.loc[param_name]\n",
    "            pval = model.pvalues.loc[param_name]\n",
    "            \n",
    "            # You might want to create a cleaner label for the parameter.\n",
    "            # For categorical variables, param_name will look like 'C(col)[T.level]'\n",
    "            # We'll store the raw param_name, but you can parse it if you like.\n",
    "\n",
    "            results.append({\n",
    "                'column': col,\n",
    "                'param_name': param_name,\n",
    "                'coef': coef,\n",
    "                'pval': pval\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(\"No valid predictors found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort by p-value (optional)\n",
    "    results_df = results_df.sort_values(by='pval')\n",
    "\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "results_df = all_regressions_each_dummy(neuraxial_catheter_df, 'failed_catheter')\n",
    "# This returns a DataFrame with columns: [column, param_name, coef, pval].\n",
    "# Each level of a categorical predictor will appear as a separate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def parse_param_name(param_name):\n",
    "    \"\"\"\n",
    "    Parses a statsmodels parameter name like:\n",
    "        'C(col)[T.value]'\n",
    "    and returns the level name 'value'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regex for the typical pattern: C(colName)[T.levelName]\n",
    "    pattern = r'.*\\[T\\.(.+)\\]'\n",
    "    match = re.match(pattern, param_name)\n",
    "    if match:\n",
    "        level_name = match.group(1)\n",
    "        return level_name\n",
    "    # If it doesn't match, assume it's some other type of parameter (e.g., numeric var)\n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['category_variable'] = results_df['param_name'].apply(parse_param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['pval'] < 0.05 / 59].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits from the graph annotations\n",
    "def remove_nums(string):\n",
    "    \"\"\"\n",
    "    Removes numbers from a string.\n",
    "    \"\"\"\n",
    "    return ''.join([i for i in string if not i.isdigit()])\n",
    "\n",
    "\n",
    "# Create plot: coefficient vs -log10(p-value)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "offset = 1e-300  # so we don't take log10(0)\n",
    "x_vals = results_df[results_df['pval'] < 0.9]['coef']\n",
    "y_vals = -np.log10(results_df[results_df['pval'] < 0.9]['pval'] + offset)\n",
    "\n",
    "sc = ax.scatter(x_vals, y_vals, color='blue')\n",
    "\n",
    "# Annotate each point\n",
    "for i, row in results_df[results_df['pval'] < 0.9].iterrows():\n",
    "    ax.text(\n",
    "        row['coef'],\n",
    "        -np.log10(row['pval'] + offset),\n",
    "        remove_nums(str(row['column'] + '__' + str(row['category_variable']))),\n",
    "        fontsize=8,\n",
    "        ha='left',\n",
    "        va='bottom'\n",
    "    )\n",
    "\n",
    "# Add a reference line for p=0.05\n",
    "ax.axhline(-np.log10(0.05), color='red', linestyle='--', label='p=0.05')\n",
    "\n",
    "ax.set_xlabel('Coefficient')\n",
    "ax.set_ylabel('-log10(p-value)')\n",
    "ax.set_title(f'Logistic Regressions for Catheter_Failure ~ Each Predictor (All Dummies)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume neuraxial_catheter_df is already defined.\n",
    "# For example:\n",
    "# neuraxial_catheter_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Identify categorical columns (assuming columns with dtype 'object' or 'category')\n",
    "categorical_cols = neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Create dummy variables for all identified categorical columns\n",
    "neuraxial_catheter_dummies = pd.get_dummies(neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1), columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Compute the correlation matrix using Pearson correlation by default\n",
    "correlation_matrix = neuraxial_catheter_dummies.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for neuraxial_catheter_df (with dummies)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete interesting colinear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_cols = ['current_anesthesiologist_catheter_count', # correlated with categorical experience variables\n",
    "                             'current_resident_catheter_count', # correlated with categorical experience variables\n",
    "                             'moderately_experienced_anesthesiologist', # correlated with categorical experience variables\n",
    "                             'gravidity_2047', # correlated witih parity\n",
    "                             'maternal_weight_end_pregnancy_2045', # correlated with BMI end pregnancy\n",
    "                              \"only_private_insurance\", # correlated with composite_SES_advantage\n",
    "                            \"maternal_language_english\", # correlated with composite_SES_advantage\n",
    "                            \"marital_status_married_or_partner\", # correlated with composite_SES_advantage\n",
    "                            \"country_of_origin_USA\", # correlated with composite_SES_advantage\n",
    "                            \"employment_status_fulltime\", # correlated with composite_SES_advantage\n",
    "                               'epidural_needle_type', # correlated with delivery location\n",
    "                               'maternal_ethnicity', # correlated with race\n",
    "                              \"delivery_site\", # correlated with delivery_site_bwh,\n",
    "                              \"fetal_presentation_position_2247\", # correlated with position_posterior_or_transverse\n",
    "                              \"fetal_presentation_category_2243\" # correlated with presentation_cephalic\n",
    "                    ]\n",
    "\n",
    "neuraxial_catheter_df = neuraxial_catheter_df.drop(columns=correlated_cols,axis=1)\n",
    "epidural_df = epidural_df.drop(columns=correlated_cols,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume neuraxial_catheter_df is already defined.\n",
    "# For example:\n",
    "# neuraxial_catheter_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Identify categorical columns (assuming columns with dtype 'object' or 'category')\n",
    "categorical_cols = neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Create dummy variables for all identified categorical columns\n",
    "neuraxial_catheter_dummies = pd.get_dummies(neuraxial_catheter_df.drop(columns=['anes_procedure_encounter_id_2273','unique_pt_id'],axis=1), columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Compute the correlation matrix using Pearson correlation by default\n",
    "correlation_matrix = neuraxial_catheter_dummies.corr()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for neuraxial_catheter_df (with dummies)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete non-predictive features (from neuraxial_catheter_df, not epidural_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_predictive_columns = ['maternal_race','has_scoliosis','composite_SES_advantage']\n",
    "neuraxial_catheter_df = neuraxial_catheter_df.drop(columns=non_predictive_columns,axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete unknowable columns to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_leakage_columns = ['rom_thru_delivery_hours','placement_to_delivery_hours']\n",
    "\n",
    "neuraxial_catheter_df = neuraxial_catheter_df.drop(data_leakage_columns, axis=1,errors='ignore')\n",
    "epidural_df = epidural_df.drop(data_leakage_columns, axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['bmi_end_pregnancy_2044', 'baby_weight_2196', 'gestational_age_weeks', 'maternal_age_years']:\n",
    "    neuraxial_catheter_df[col] = neuraxial_catheter_df[col] - neuraxial_catheter_df[col].median()\n",
    "    epidural_df[col] = epidural_df[col] - epidural_df[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuraxial_catheter_df['gestational_age_weeks'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepent '#' to most common so it will be the one dropped when dummies are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dau.prepend_char_to_most_common(df, '#', cols_to_ignore=['anes_procedure_encounter_id_2273','unique_pt_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random data for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = neuraxial_catheter_df.copy()\n",
    "failure_rate = test_dataset['failed_catheter'].mean()\n",
    "test_dataset['failed_catheter'] = np.random.binomial(n=1, p=failure_rate, size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = test_dataset\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "# If you do NOT want to use `anes_procedure_encounter_id_2273` as a feature:\n",
    "X = X.drop(columns=[\"unique_pt_id\"])  \n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train logistic regression with class weights\n",
    "logistic_model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', n_jobs=1)\n",
    "logistic_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = logistic_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"TEST RANDOM Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nTEST RANDOM Classification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real LOGIT regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Load and clean data\n",
    "data = neuraxial_catheter_df.copy()\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "# 2. Group-aware split\n",
    "groups = X['unique_pt_id']\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "\n",
    "# 3. Identify numeric vs. categorical\n",
    "numeric_features = X_train.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# 4. Impute numeric\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "X_train[numeric_features] = num_imputer.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = num_imputer.transform(X_test[numeric_features])\n",
    "\n",
    "# 5. Scale numeric\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# 6. Impute categorical\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_features] = cat_imputer.fit_transform(X_train[categorical_features])\n",
    "X_test[categorical_features] = cat_imputer.transform(X_test[categorical_features])\n",
    "\n",
    "# 7. One-hot encode categorical\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features, drop_first=True,dtype=int).astype(float)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_features, drop_first=True,dtype=int).astype(float)\n",
    "\n",
    "X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "# 8. Fit logistic regression with Statsmodels\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit(disp=0)\n",
    "\n",
    "# 9. Predict\n",
    "X_test_const = sm.add_constant(X_test, has_constant='add')\n",
    "y_pred_prob = result.predict(X_test_const)\n",
    "\n",
    "evaluation_metrics_by_threshold = []\n",
    "\n",
    "for i in range(0, 21):\n",
    "    prediction_threshold = i / 20\n",
    "    y_pred = (y_pred_prob >= prediction_threshold).astype(int)\n",
    "\n",
    "    # 10. Evaluate\n",
    "    evaluation_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred,zero_division=np.nan),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "        # \"classification_report\": classification_report(y_test, y_pred)\n",
    "    }\n",
    "    evaluation_metrics_by_threshold.append(evaluation_metrics)\n",
    "\n",
    "result_summ = result.summary(alpha = 0.001)\n",
    "\n",
    "# print(\"Model Evaluation:\")\n",
    "# for metric, value in evaluation_metrics.items():\n",
    "#     if metric == \"classification_report\":\n",
    "#         print(\"\\nClassification Report:\\n\", value)\n",
    "#     else:\n",
    "#         print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_by_threshold = pd.DataFrame(evaluation_metrics_by_threshold)\n",
    "evaluation_metrics_by_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_result_table = result_summ.tables[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table = pd.DataFrame(result_summ.tables[1].data)\n",
    "# 1. Set the first row as the new column headers.\n",
    "logit_predictor_table.columns = logit_predictor_table.iloc[0]\n",
    "\n",
    "# 2. Remove the first row (since it's now serving as header).\n",
    "logit_predictor_table = logit_predictor_table[1:]\n",
    "\n",
    "# 3. Set the first column (after the column headers update) as the index.\n",
    "# Here, `df.columns[0]` represents the name of the first column.\n",
    "logit_predictor_table = logit_predictor_table.set_index(logit_predictor_table.columns[0])\n",
    "\n",
    "# 4. Sort by the 'P>|z|' column in ascending order.\n",
    "logit_predictor_table = logit_predictor_table.sort_values('P>|z|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the odds ratio (OR) and the 95% confidence intervals\n",
    "logit_predictor_table['OR'] = np.exp(logit_predictor_table['coef'].astype(float))\n",
    "logit_predictor_table['OR_lower'] = np.exp(logit_predictor_table['[0.0005'].astype(float))\n",
    "logit_predictor_table['OR_upper'] = np.exp(logit_predictor_table['0.9995]'].astype(float))\n",
    "\n",
    "# Create the 'OR (95% CI)' column\n",
    "logit_predictor_table['OR (99.9% CI)'] = logit_predictor_table.apply(\n",
    "    lambda row: f\"{row['OR']:.2f} ({row['OR_lower']:.2f} - {row['OR_upper']:.2f})\", axis=1)\n",
    "\n",
    "logit_predictor_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predictor_table.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_factors = ['delivery_site_bwh',\n",
    "       'parity_2048', 'gestational_age_weeks',\n",
    "       'presentation_cephalic',\n",
    "       'has_dorsalgia',\n",
    "       'maternal_age_years', 'has_back_problems',\n",
    "       'prior_failed_catheters_prev_enc',\n",
    "       'baby_weight_2196',\n",
    "       'composite_psychosocial_problems', 'prior_failed_catheters_this_enc',\n",
    "       'position_posterior_or_transverse', 'prior_pain_scores_max',\n",
    "       'bmi_end_pregnancy_2044', 'labor_induction']\n",
    "\n",
    "procedural_factors = ['true_procedure_type_incl_dpe_cse', \n",
    "       'true_procedure_type_incl_dpe_dpe',\n",
    "       'highly_experienced_anesthesiologist_none', 'lor_depth', 'number_of_neuraxial_attempts',\n",
    "       'paresthesias_present',\n",
    "       'highly_experienced_resident_no',\n",
    "       'highly_experienced_anesthesiologist_yes',\n",
    "       'highly_experienced_resident_yes',\n",
    "       'true_procedure_type_incl_dpe_intrathecal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = logit_predictor_table.loc[patient_factors].copy()\n",
    "procedural_df = logit_predictor_table.loc[procedural_factors].copy()\n",
    "\n",
    "rename_map = {\n",
    "    'gestational_age_weeks': 'Gestational Age (per week)',\n",
    "    'delivery_site_bwh': 'Delivery at our obstetric teaching hospital (vs other)',\n",
    "    'baby_weight_2196': 'Baby Weight (per kg)',\n",
    "    'bmi_end_pregnancy_2044': 'BMI (per kg/m^2)',\n",
    "    'parity_2048': 'Parity (per birth)',\n",
    "    'has_dorsalgia': 'Back pain (vs none)',\n",
    "    'has_back_problems': 'Scoliosis or other back problems (vs none)',\n",
    "    'prior_pain_scores_max': 'Max pain score prior to placement (per unit 0-10)',\n",
    "    'composite_psychosocial_problems': 'Psychosocial risk factors (vs none)',\n",
    "    'prior_failed_catheters_this_enc': 'Prior failed catheters in this encounter (per failure)',\n",
    "    'prior_failed_catheters_prev_enc': 'Prior failed catheters in prior encounters (per failure)',\n",
    "    'maternal_age_years': 'Maternal age (per year)',\n",
    "    'labor_induction': 'Induced labor (vs not)',\n",
    "    'position_posterior_or_transverse': 'Posterior or transverse fetal position (vs other)',\n",
    "    'presentation_cephalic': 'Cephalic fetal presentation (vs other)',\n",
    "    # procedural factors below\n",
    "    'lor_depth': 'Depth to loss of resistance (per cm)',\n",
    "    'highly_experienced_anesthesiologist_yes': 'Highly experienced attending anesthesiologist (vs less experienced)',\n",
    "    'highly_experienced_anesthesiologist_none': 'No attending anesthesiologist (vs less experienced attending)',\n",
    "    'highly_experienced_resident_yes': 'Highly experienced resident (vs no resident)',\n",
    "    'highly_experienced_resident_no': 'Less experienced resident (vs no resident)',\n",
    "    'paresthesias_present': 'Paresthesias present during placement (vs none)',\n",
    "    'number_of_neuraxial_attempts': 'Number of placement attempts (per attempt)',\n",
    "    'true_procedure_type_incl_dpe_intrathecal': 'Intrathecal catheter (vs conventional epidural)',\n",
    "    'true_procedure_type_incl_dpe_dpe': 'Dural puncture epidural (vs conventional epidural)',\n",
    "    'true_procedure_type_incl_dpe_cse': 'Combined spinal-epidural (vs conventional epidural)'\n",
    "}\n",
    "\n",
    "patient_df = patient_df.rename(index=rename_map)\n",
    "procedural_df = procedural_df.rename(index=rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "def plot_forest_colored_with_markers(\n",
    "    ax, \n",
    "    df, \n",
    "    title='Forest Plot', \n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5, \n",
    "    x_max=1.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a forest chart on 'ax' given a DataFrame 'df' with columns:\n",
    "      - 'OR'\n",
    "      - 'OR_lower'\n",
    "      - 'OR_upper'\n",
    "    \n",
    "    X-axis is restricted to [x_min, x_max].\n",
    "    \n",
    "    Rules:\n",
    "      - If the center OR is outside [x_min, x_max], skip plotting its dot.\n",
    "      - If the OR or any part of its CI is beyond [x_min, x_max], place '<' or '>' at that boundary.\n",
    "      - Print \"OR X.XX (L.LL, U.UU)\" above each data point in the same color.\n",
    "      - Color each factor's name on the y-axis to match that factor's color.\n",
    "    \n",
    "    Color scheme for significance:\n",
    "      - red if entire CI > 1\n",
    "      - blue if entire CI < 1\n",
    "      - black otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by OR if you want smaller/larger ORs in order\n",
    "    df = df.sort_values('OR')\n",
    "\n",
    "    # We'll manually set the y-ticks, one row per factor\n",
    "    y_positions = np.arange(len(df))\n",
    "\n",
    "    # We won't set the yticklabels yet; we'll do them manually to color each label.\n",
    "    ax.set_yticks(y_positions)\n",
    "    # Temporarily set them all to blank\n",
    "    ax.set_yticklabels([\"\"] * len(df))\n",
    "\n",
    "    # We'll collect the color for each row, so we can color the labels afterward\n",
    "    factor_colors = []\n",
    "\n",
    "    # Plot each factor individually\n",
    "    for y_pos, (idx, row) in zip(y_positions, df.iterrows()):\n",
    "        or_val = row['OR']\n",
    "        ci_low = row['OR_lower']\n",
    "        ci_high = row['OR_upper']\n",
    "\n",
    "        # Decide the color based on significance\n",
    "        if ci_low > 1:\n",
    "            c = 'red'    # entire CI above 1 => significant risk\n",
    "        elif ci_high < 1:\n",
    "            c = 'blue'   # entire CI below 1 => significant protective\n",
    "        else:\n",
    "            c = 'black'  # not significant\n",
    "\n",
    "        factor_colors.append(c)\n",
    "\n",
    "        # Check if OR or CI extends beyond the plot range\n",
    "        outside_left = (or_val < x_min) or (ci_low < x_min)\n",
    "        outside_right = (or_val > x_max) or (ci_high > x_max)\n",
    "\n",
    "        # If the center OR is out of range, skip the dot\n",
    "        center_outside = (or_val < x_min) or (or_val > x_max)\n",
    "        dot_fmt = 'none' if center_outside else 'o'\n",
    "\n",
    "        # Calculate the full error bar from the center\n",
    "        left_err = or_val - ci_low\n",
    "        right_err = ci_high - or_val\n",
    "\n",
    "        # Plot the error bar (may or may not include the dot)\n",
    "        ax.errorbar(\n",
    "            or_val,\n",
    "            y_pos,\n",
    "            xerr=[[left_err], [right_err]],\n",
    "            fmt=dot_fmt,   # skip the dot if center is outside\n",
    "            color=c,\n",
    "            ecolor=c,\n",
    "            capsize=4\n",
    "        )\n",
    "\n",
    "        # Place boundary markers if the OR or any part of CI is outside\n",
    "        if outside_left:\n",
    "            ax.text(\n",
    "                x_min, y_pos, '<', \n",
    "                va='center', ha='right', color=c, fontsize=14\n",
    "            )\n",
    "        if outside_right:\n",
    "            ax.text(\n",
    "                x_max, y_pos, '>', \n",
    "                va='center', ha='left', color=c, fontsize=14\n",
    "            )\n",
    "\n",
    "        # Prepare the label \"OR X.XX (L.LL - U.UU)\"\n",
    "        label_str = f\"OR {or_val:.2f} ({ci_low:.2f} - {ci_high:.2f})\"\n",
    "\n",
    "        # Place the label just above the data point (or boundary)\n",
    "        # We'll define a small offset in Y to shift text \"above\" the marker\n",
    "        label_offset = 0.2  # Adjust as needed\n",
    "        label_y = y_pos - label_offset  # axis is inverted => subtract to go \"up\"\n",
    "        ax.text(\n",
    "            1.06, label_y,\n",
    "            label_str,\n",
    "            va='bottom',   # text rises from the point\n",
    "            ha='center',\n",
    "            color=c,\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "    # Now color each factor name using the same color\n",
    "    # We already set blank y-ticklabels, so let's manually place them:\n",
    "    for y_pos, (idx, c) in zip(y_positions, zip(df.index, factor_colors)):\n",
    "        # We'll place the text a bit left of x_min so it doesn't collide with the plot\n",
    "        ax.text(\n",
    "            x_min - 0.05, y_pos,\n",
    "            idx,\n",
    "            va='center', ha='right',\n",
    "            color=c,\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "    # Draw a vertical line at OR=1\n",
    "    ax.axvline(x=1, color='gray', linestyle='--')\n",
    "\n",
    "    # Invert y-axis so the top row is at the top\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Limit the x-axis\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_xlabel(x_label, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Example usage with TWO subplots\n",
    "# ==========================\n",
    "\n",
    "# Suppose you have:\n",
    "#   patient_df\n",
    "#   procedural_df\n",
    "# Each with columns: ['OR', 'OR_lower', 'OR_upper']\n",
    "# and index = factor names.\n",
    "\n",
    "# Create the figure with two columns\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 8))\n",
    "\n",
    "plot_forest_colored_with_markers(\n",
    "    ax=ax1,\n",
    "    df=patient_df,\n",
    "    title='Patient Factors',\n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5,\n",
    "    x_max=1.5\n",
    ")\n",
    "\n",
    "plot_forest_colored_with_markers(\n",
    "    ax=ax2,\n",
    "    df=procedural_df,\n",
    "    title='Procedural Factors',\n",
    "    x_label='Odds Ratio (99.9% CI)',\n",
    "    x_min=0.5,\n",
    "    x_max=1.5\n",
    ")\n",
    "\n",
    "# Create a manual legend for color interpretation:\n",
    "protect_marker = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
    "                               label='Significant protective factor')\n",
    "ns_marker = mlines.Line2D([], [], color='black', marker='o', linestyle='None',\n",
    "                          label='Not significant')\n",
    "risk_marker = mlines.Line2D([], [], color='red', marker='o', linestyle='None',\n",
    "                            label='Significant risk factor')\n",
    "\n",
    "fig.legend(\n",
    "    handles=[protect_marker, ns_marker, risk_marker],\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.05),\n",
    "    ncol=3,\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = neuraxial_catheter_df\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "\n",
    "## Drop delivery_site\n",
    "# X = X.drop(columns=[\"delivery_site\"],errors='ignore')\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "# If you do NOT want to use `anes_procedure_encounter_id_2273` as a feature:\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])  \n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train logistic regression with class weights\n",
    "logistic_model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', n_jobs=1)\n",
    "logistic_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = logistic_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nClassification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the feature names produced by the ColumnTransformer\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 2. Get the coefficients from the trained logistic model\n",
    "#    For binary classification, .coef_ is an array of shape (1, n_features)\n",
    "coefficients = logistic_model.coef_[0]\n",
    "\n",
    "\n",
    "# 3. Combine feature names with coefficients into a list of tuples\n",
    "coef_pairs = list(zip(feature_names, coefficients))\n",
    "\n",
    "# 4. Print the coefficients sorted by absolute magnitude\n",
    "print(\"Coefficients for each feature (sorted by absolute magnitude):\")\n",
    "for name, coef in sorted(coef_pairs, key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "# 5. Print the coefficients sorted alphabetically\n",
    "print('---------------------------------------------')\n",
    "print(\"Coefficients for each feature (sorted alphabetically):\")\n",
    "for name, coef in sorted(coef_pairs):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "# 6. Print the intercept\n",
    "print(f\"Intercept: {logistic_model.intercept_[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Imports\n",
    "##############################################################################\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "\n",
    "# Import XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "##############################################################################\n",
    "# 0. Load and prepare the dataset\n",
    "##############################################################################\n",
    "data = neuraxial_catheter_df\n",
    "\n",
    "# Drop columns with more than 80% missing values\n",
    "threshold = len(data) * 0.2\n",
    "data_cleaned = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "data_cleaned = data_cleaned.dropna(subset=[\"failed_catheter\"])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=[\"failed_catheter\"], errors='ignore')\n",
    "y = data_cleaned[\"failed_catheter\"]\n",
    "\n",
    "# # Drop delivery_site\n",
    "# X = X.drop(columns=[\"delivery_site\"],errors='ignore')\n",
    "\n",
    "##############################################################################\n",
    "# 1. Extract the group labels and remove them from X if it's just an ID column\n",
    "##############################################################################\n",
    "groups = X['unique_pt_id']  # Save group labels\n",
    "X = X.drop(columns=[\"anes_procedure_encounter_id_2273\",\"unique_pt_id\"])  # remove ID column from features\n",
    "\n",
    "##############################################################################\n",
    "# 2. Split using GroupShuffleSplit instead of train_test_split\n",
    "##############################################################################\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "##############################################################################\n",
    "# 3. Identify numeric and categorical columns\n",
    "##############################################################################\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "##############################################################################\n",
    "# 4. Create preprocessing pipelines\n",
    "##############################################################################\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "##############################################################################\n",
    "# 5. Preprocess the data\n",
    "##############################################################################\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "##############################################################################\n",
    "# 6. Train XGBoost Classifier\n",
    "##############################################################################\n",
    "# Instantiate the XGBClassifier\n",
    "# Note: You can tune parameters such as 'scale_pos_weight' if your data is imbalanced.\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=(y_train.shape[0] - y_train.sum()) / y_train.sum()\n",
    ")\n",
    "xgb_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "##############################################################################\n",
    "# 7. Make predictions\n",
    "##############################################################################\n",
    "y_pred = xgb_model.predict(X_test_preprocessed)\n",
    "y_pred_prob = xgb_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "##############################################################################\n",
    "# 8. Evaluate the model\n",
    "##############################################################################\n",
    "evaluation_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "    \"classification_report\": classification_report(y_test, y_pred)\n",
    "}\n",
    "\n",
    "##############################################################################\n",
    "# 9. Print the evaluation metrics\n",
    "##############################################################################\n",
    "print(\"Model Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    if metric == \"classification_report\":\n",
    "        print(\"\\nClassification Report:\\n\", value)\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring for DPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df['dpe'] = (df['true_procedure_type_incl_dpe'] == 'dpe').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'dpe'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring for CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df['cse'] = (df['true_procedure_type_incl_dpe'] == 'cse').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'cse'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity scoring for DPE vs CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For logistic regression and nearest neighbor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# For imputation and scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For statistical inference (CIs, p-values)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Copy your original dataframe\n",
    "# ------------------------------------------------------------------------------\n",
    "df = neuraxial_catheter_df.copy()\n",
    "df = df[df['true_procedure_type_incl_dpe'].isin(['cse', 'dpe'])]\n",
    "df['cse_not_dpe'] = (df['true_procedure_type_incl_dpe'] == 'cse').astype(int)\n",
    "df.drop(columns=['true_procedure_type_incl_dpe'], inplace=True)\n",
    "\n",
    "# Columns for the treatment and outcome\n",
    "treatment_col = 'cse_not_dpe'\n",
    "outcome_col   = 'failed_catheter'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Identify numeric vs. categorical columns (excluding treatment & outcome)\n",
    "# ------------------------------------------------------------------------------\n",
    "# If 'dpe' or 'failed_catheter' happen to be numeric, we still exclude them from imputation.\n",
    "numeric_cols = [\n",
    "    col for col in df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in [treatment_col, outcome_col]\n",
    "]\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in numeric_cols and col not in [treatment_col, outcome_col]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Impute missing data\n",
    "#    - Median for numeric\n",
    "#    - Most frequent for categorical\n",
    "# ------------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit/transform numeric columns\n",
    "df_num = pd.DataFrame(\n",
    "    num_imputer.fit_transform(df[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Fit/transform categorical columns\n",
    "df_cat = pd.DataFrame(\n",
    "    cat_imputer.fit_transform(df[categorical_cols]),\n",
    "    columns=categorical_cols\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. One-hot encode (dummy) the categorical columns\n",
    "# ------------------------------------------------------------------------------\n",
    "df_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Combine imputed numeric + encoded categorical with original treatment/outcome\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reattach treatment/outcome columns to the front, for convenience\n",
    "df_imputed = pd.concat(\n",
    "    [\n",
    "        df[[treatment_col, outcome_col]].reset_index(drop=True),\n",
    "        df_num.reset_index(drop=True),\n",
    "        df_cat_encoded.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Standardize numeric features (optional but often recommended)\n",
    "#    Identify which columns in df_num still exist in df_imputed\n",
    "# ------------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_imputed[numeric_cols]),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Now replace the unscaled numeric columns in df_imputed\n",
    "for col in numeric_cols:\n",
    "    df_imputed[col] = df_num_scaled[col]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Fit the propensity model (LogisticRegression) on all columns except\n",
    "#    the treatment and outcome columns.\n",
    "# ------------------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_imputed.columns if c not in [treatment_col, outcome_col]]\n",
    "\n",
    "X = df_imputed[feature_cols].values  # all imputed & encoded features\n",
    "y = df_imputed[treatment_col].values # the treatment indicator (dpe)\n",
    "\n",
    "propensity_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "propensity_model.fit(X, y)\n",
    "\n",
    "# Probability of dpe=1\n",
    "propensity_scores = propensity_model.predict_proba(X)[:, 1]\n",
    "df_imputed['propensity_score'] = propensity_scores\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8. Separate treated vs. control and do nearest-neighbor matching\n",
    "# ------------------------------------------------------------------------------\n",
    "treated = df_imputed[df_imputed[treatment_col] == 1].copy()\n",
    "control = df_imputed[df_imputed[treatment_col] == 0].copy()\n",
    "\n",
    "treated_scores = treated[['propensity_score']].values\n",
    "control_scores = control[['propensity_score']].values\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "nn.fit(control_scores)\n",
    "\n",
    "distances, indices = nn.kneighbors(treated_scores)\n",
    "distances = distances.flatten()\n",
    "indices = indices.flatten()\n",
    "\n",
    "matched_treated = treated.copy()\n",
    "matched_control = control.iloc[indices].copy()\n",
    "\n",
    "# Combine matched sample\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9. Fit an outcome model on the matched sample\n",
    "#    We'll use statsmodels for confidence intervals and p-values.\n",
    "# ------------------------------------------------------------------------------\n",
    "matched_data['intercept'] = 1.0\n",
    "\n",
    "# We'll just use dpe (and intercept) in the outcome model here\n",
    "X_outcome = matched_data[['intercept', treatment_col]]\n",
    "y_outcome = matched_data[outcome_col]\n",
    "\n",
    "logit_sm = sm.Logit(y_outcome, X_outcome)\n",
    "result_sm = logit_sm.fit(disp=0)  # disp=0 hides optimization output\n",
    "\n",
    "print(result_sm.summary())\n",
    "\n",
    "# Extract OR & 95% CI\n",
    "params = result_sm.params\n",
    "conf = result_sm.conf_int()\n",
    "odds_ratios = np.exp(params)\n",
    "conf_odds = np.exp(conf)\n",
    "\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)\n",
    "print(\"\\n95% Confidence Intervals:\\n\", conf_odds)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "XGBoost - hyperparameter tuning with Optuna\n",
    "\n",
    "Shapley values for interpretability\n",
    "\n",
    "Eliminate features that are both poorly predictive and have lots of missing data\n",
    "\n",
    "Abstract functions and separate them into different files\n",
    "\n",
    "Fewer features will improve interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
